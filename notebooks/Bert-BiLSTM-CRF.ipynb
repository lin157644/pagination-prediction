{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "import re\n",
    "import sys\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "from urllib.parse import urlparse, urlsplit, parse_qs, parse_qsl\n",
    "\n",
    "import numpy as np\n",
    "import parsel\n",
    "from sklearn_crfsuite.metrics import flat_classification_report, sequence_accuracy_score\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "from autopager.storage import Storage\n",
    "from autopager.htmlutils import (get_link_text, get_text_around_selector_list,\n",
    "                                 get_link_href, get_selector_root)\n",
    "from autopager.utils import (\n",
    "    get_domain, normalize_whitespaces, normalize, ngrams, tokenize, ngrams_wb, replace_digits\n",
    ")\n",
    "from autopager.model import _num_tokens_feature, _elem_attr\n",
    "from autopager import AUTOPAGER_LIMITS\n",
    "from autopager.parserutils import (TagParser, MyHTMLParser, draw_scaled_page, position_check, compare_tag, get_first_tag)\n",
    "parser = MyHTMLParser()\n",
    "tagParser = TagParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import random\n",
    "tf.random.set_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current test file:  ['en', 'zh', 'ko', 'ja', 'de', 'ru']\n"
     ]
    }
   ],
   "source": [
    "storage = Storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpus)!=0:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPUs visible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contain position: True\n",
      "Finish: Get Page 1 (Encoding: UTF-8)records ... (len: 303)\n",
      "Finish: Get Page 2 (Encoding: UTF-8)records ... (len: 243)\n",
      "Finish: Get Page 3 (Encoding: UTF-8)records ... (len: 119)\n",
      "Finish: Get Page 4 (Encoding: UTF-8)records ... (len: 944)\n",
      "Finish: Get Page 5 (Encoding: UTF-8)records ... (len: 93)\n",
      "Finish: Get Page 6 (Encoding: UTF-8)records ... (len: 994)\n",
      "Finish: Get Page 7 (Encoding: UTF-8)records ... (len: 1014)\n",
      "Finish: Get Page 8 (Encoding: UTF-8)records ... (len: 7)\n",
      "Finish: Get Page 21 (Encoding: UTF-8)records ... (len: 158)\n",
      "Finish: Get Page 22 (Encoding: UTF-8)records ... (len: 171)\n",
      "Finish: Get Page 23 (Encoding: UTF-8)records ... (len: 181)\n",
      "Finish: Get Page 24 (Encoding: UTF-8)records ... (len: 10)\n",
      "Finish: Get Page 25 (Encoding: UTF-8)records ... (len: 165)\n",
      "Finish: Get Page 26 (Encoding: UTF-8)records ... (len: 147)\n",
      "Finish: Get Page 28 (Encoding: UTF-8)records ... (len: 268)\n",
      "Finish: Get Page 33 (Encoding: UTF-8)records ... (len: 108)\n",
      "Finish: Get Page 34 (Encoding: UTF-8)records ... (len: 109)\n",
      "Finish: Get Page 35 (Encoding: UTF-8)records ... (len: 84)\n",
      "Finish: Get Page 36 (Encoding: UTF-8)records ... (len: 718)\n",
      "Finish: Get Page 37 (Encoding: UTF-8)records ... (len: 723)\n",
      "Finish: Get Page 38 (Encoding: UTF-8)records ... (len: 703)\n",
      "Finish: Get Page 46 (Encoding: UTF-8)records ... (len: 82)\n",
      "Finish: Get Page 47 (Encoding: UTF-8)records ... (len: 88)\n",
      "Finish: Get Page 48 (Encoding: UTF-8)records ... (len: 34)\n",
      "Finish: Get Page 49 (Encoding: UTF-8)records ... (len: 62)\n",
      "Finish: Get Page 50 (Encoding: UTF-8)records ... (len: 15)\n",
      "Finish: Get Page 56 (Encoding: cp1252)records ... (len: 520)\n",
      "Finish: Get Page 57 (Encoding: cp1252)records ... (len: 463)\n",
      "Finish: Get Page 62 (Encoding: UTF-8)records ... (len: 55)\n",
      "Finish: Get Page 63 (Encoding: UTF-8)records ... (len: 84)\n",
      "Finish: Get Page 64 (Encoding: UTF-8)records ... (len: 95)\n",
      "Finish: Get Page 68 (Encoding: UTF-8)records ... (len: 37)\n",
      "Finish: Get Page 69 (Encoding: UTF-8)records ... (len: 312)\n",
      "Finish: Get Page 71 (Encoding: UTF-8)records ... (len: 104)\n",
      "Finish: Get Page 72 (Encoding: UTF-8)records ... (len: 92)\n",
      "Finish: Get Page 74 (Encoding: UTF-8)records ... (len: 171)\n",
      "Finish: Get Page 75 (Encoding: UTF-8)records ... (len: 386)\n",
      "Finish: Get Page 76 (Encoding: UTF-8)records ... (len: 319)\n",
      "Finish: Get Page 77 (Encoding: UTF-8)records ... (len: 114)\n",
      "Finish: Get Page 78 (Encoding: UTF-8)records ... (len: 118)\n",
      "Finish: Get Page 79 (Encoding: UTF-8)records ... (len: 98)\n",
      "Finish: Get Page 84 (Encoding: UTF-8)records ... (len: 94)\n",
      "Finish: Get Page 85 (Encoding: UTF-8)records ... (len: 98)\n",
      "Finish: Get Page 91 (Encoding: UTF-8)records ... (len: 2389)\n",
      "Finish: Get Page 92 (Encoding: UTF-8)records ... (len: 2379)\n",
      "Finish: Get Page 93 (Encoding: UTF-8)records ... (len: 160)\n",
      "Finish: Get Page 94 (Encoding: UTF-8)records ... (len: 74)\n",
      "Finish: Get Page 95 (Encoding: UTF-8)records ... (len: 143)\n",
      "Finish: Get Page 96 (Encoding: UTF-8)records ... (len: 133)\n",
      "Finish: Get Page 97 (Encoding: UTF-8)records ... (len: 163)\n",
      "Finish: Get Page 98 (Encoding: UTF-8)records ... (len: 378)\n",
      "Finish: Get Page 99 (Encoding: UTF-8)records ... (len: 120)\n",
      "Finish: Get Page 100 (Encoding: UTF-8)records ... (len: 124)\n",
      "Finish: Get Page 101 (Encoding: UTF-8)records ... (len: 122)\n",
      "Finish: Get Page 108 (Encoding: UTF-8)records ... (len: 233)\n",
      "Finish: Get Page 109 (Encoding: cp1252)records ... (len: 155)\n",
      "Finish: Get Page 110 (Encoding: cp1252)records ... (len: 161)\n",
      "Finish: Get Page 111 (Encoding: cp1252)records ... (len: 131)\n",
      "Finish: Get Page 112 (Encoding: cp1252)records ... (len: 131)\n",
      "Finish: Get Page 113 (Encoding: UTF-8)records ... (len: 72)\n",
      "Finish: Get Page 114 (Encoding: UTF-8)records ... (len: 70)\n",
      "Finish: Get Page 115 (Encoding: UTF-8)records ... (len: 126)\n",
      "Finish: Get Page 116 (Encoding: UTF-8)records ... (len: 90)\n",
      "Finish: Get Page 117 (Encoding: UTF-8)records ... (len: 89)\n",
      "Finish: Get Page 118 (Encoding: UTF-8)records ... (len: 79)\n",
      "Finish: Get Page 119 (Encoding: UTF-8)records ... (len: 133)\n",
      "Finish: Get Page 120 (Encoding: UTF-8)records ... (len: 89)\n",
      "Finish: Get Page 121 (Encoding: UTF-8)records ... (len: 81)\n",
      "Finish: Get Page 125 (Encoding: UTF-8)records ... (len: 16)\n",
      "Finish: Get Page 127 (Encoding: UTF-8)records ... (len: 154)\n",
      "Finish: Get Page 128 (Encoding: UTF-8)records ... (len: 51)\n",
      "Finish: Get Page 129 (Encoding: UTF-8)records ... (len: 88)\n",
      "Finish: Get Page 130 (Encoding: UTF-8)records ... (len: 88)\n",
      "Finish: Get Page 132 (Encoding: UTF-8)records ... (len: 306)\n",
      "Finish: Get Page 133 (Encoding: UTF-8)records ... (len: 309)\n",
      "Finish: Get Page 134 (Encoding: UTF-8)records ... (len: 103)\n",
      "Finish: Get Page 135 (Encoding: UTF-8)records ... (len: 88)\n",
      "Finish: Get Page 136 (Encoding: UTF-8)records ... (len: 159)\n",
      "Finish: Get Page 137 (Encoding: UTF-8)records ... (len: 35)\n",
      "Finish: Get Page 138 (Encoding: UTF-8)records ... (len: 112)\n",
      "Finish: Get Page 139 (Encoding: UTF-8)records ... (len: 117)\n",
      "Finish: Get Page 140 (Encoding: UTF-8)records ... (len: 142)\n",
      "Finish: Get Page 141 (Encoding: UTF-8)records ... (len: 116)\n",
      "Finish: Get Page 143 (Encoding: cp1252)records ... (len: 84)\n",
      "Finish: Get Page 144 (Encoding: cp1252)records ... (len: 134)\n",
      "Finish: Get Page 145 (Encoding: cp1252)records ... (len: 139)\n",
      "Finish: Get Page 146 (Encoding: cp1252)records ... (len: 95)\n",
      "Finish: Get Page 147 (Encoding: cp1252)records ... (len: 130)\n",
      "Finish: Get Page 148 (Encoding: UTF-8)records ... (len: 478)\n",
      "Finish: Get Page 149 (Encoding: UTF-8)records ... (len: 365)\n",
      "Finish: Get Page 150 (Encoding: UTF-8)records ... (len: 368)\n",
      "Finish: Get Page 151 (Encoding: UTF-8)records ... (len: 369)\n",
      "Finish: Get Page 152 (Encoding: cp1252)records ... (len: 294)\n",
      "Finish: Get Page 153 (Encoding: UTF-8)records ... (len: 271)\n",
      "Finish: Get Page 162 (Encoding: UTF-8)records ... (len: 308)\n",
      "Finish: Get Page 163 (Encoding: UTF-8)records ... (len: 298)\n",
      "Finish: Get Page 164 (Encoding: UTF-8)records ... (len: 285)\n",
      "Finish: Get Page 165 (Encoding: UTF-8)records ... (len: 221)\n",
      "Finish: Get Page 203 (Encoding: UTF-8)records ... (len: 361)\n",
      "Finish: Get Page 204 (Encoding: UTF-8)records ... (len: 356)\n",
      "Finish: Get Page 205 (Encoding: UTF-8)records ... (len: 360)\n",
      "Finish: Get Page 206 (Encoding: UTF-8)records ... (len: 331)\n",
      "Finish: Get Page 207 (Encoding: UTF-8)records ... (len: 498)\n",
      "Finish: Get Page 208 (Encoding: UTF-8)records ... (len: 499)\n",
      "Finish: Get Page 209 (Encoding: UTF-8)records ... (len: 497)\n",
      "Finish: Get Page 211 (Encoding: UTF-8)records ... (len: 145)\n",
      "Finish: Get Page 212 (Encoding: UTF-8)records ... (len: 147)\n",
      "Finish: Get Page 213 (Encoding: UTF-8)records ... (len: 111)\n",
      "Finish: Get Page 218 (Encoding: UTF-8)records ... (len: 83)\n",
      "Finish: Get Page 219 (Encoding: UTF-8)records ... (len: 84)\n",
      "Finish: Get Page 220 (Encoding: UTF-8)records ... (len: 72)\n",
      "Finish: Get Page 221 (Encoding: UTF-8)records ... (len: 207)\n",
      "Finish: Get Page 222 (Encoding: UTF-8)records ... (len: 202)\n",
      "Finish: Get Page 223 (Encoding: UTF-8)records ... (len: 103)\n",
      "Finish: Get Page 224 (Encoding: cp1252)records ... (len: 59)\n",
      "Finish: Get Page 225 (Encoding: UTF-8)records ... (len: 58)\n",
      "Finish: Get Page 226 (Encoding: UTF-8)records ... (len: 346)\n",
      "Finish: Get Page 227 (Encoding: UTF-8)records ... (len: 281)\n",
      "Finish: Get Page 228 (Encoding: UTF-8)records ... (len: 164)\n",
      "Finish: Get Page 232 (Encoding: cp1252)records ... (len: 74)\n",
      "Finish: Get Page 233 (Encoding: UTF-8)records ... (len: 61)\n",
      "Finish: Get Page 234 (Encoding: UTF-8)records ... (len: 258)\n",
      "Finish: Get Page 237 (Encoding: UTF-8)records ... (len: 286)\n",
      "Finish: Get Page 238 (Encoding: UTF-8)records ... (len: 190)\n",
      "Finish: Get Page 239 (Encoding: cp1252)records ... (len: 256)\n",
      "Finish: Get Page 240 (Encoding: cp1252)records ... (len: 148)\n",
      "Finish: Get Page 241 (Encoding: UTF-8)records ... (len: 98)\n",
      "Finish: Get Page 242 (Encoding: UTF-8)records ... (len: 164)\n",
      "Finish: Get Page 243 (Encoding: UTF-8)records ... (len: 169)\n",
      "Finish: Get Page 244 (Encoding: UTF-8)records ... (len: 139)\n",
      "Finish: Get Page 245 (Encoding: UTF-8)records ... (len: 131)\n",
      "Finish: Get Page 246 (Encoding: cp1252)records ... (len: 528)\n",
      "Finish: Get Page 247 (Encoding: UTF-8)records ... (len: 217)\n",
      "Finish: Get Page 249 (Encoding: UTF-8)records ... (len: 61)\n",
      "Finish: Get Page 261 (Encoding: UTF-8)records ... (len: 306)\n",
      "Finish: Get Page 262 (Encoding: UTF-8)records ... (len: 129)\n",
      "Finish: Get Page 263 (Encoding: UTF-8)records ... (len: 171)\n",
      "Finish: Get Page 264 (Encoding: UTF-8)records ... (len: 53)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish: Get Page 265 (Encoding: UTF-8)records ... (len: 199)\n",
      "Finish: Get Page 266 (Encoding: UTF-8)records ... (len: 86)\n",
      "Finish: Get Page 267 (Encoding: UTF-8)records ... (len: 131)\n",
      "Finish: Get Page 284 (Encoding: cp1252)records ... (len: 130)\n",
      "Finish: Get Page 287 (Encoding: UTF-8)records ... (len: 82)\n",
      "Finish: Get Page 288 (Encoding: UTF-8)records ... (len: 140)\n",
      "Finish: Get Page 289 (Encoding: UTF-8)records ... (len: 44)\n",
      "Finish: Get Page 293 (Encoding: UTF-8)records ... (len: 74)\n",
      "Finish: Get Page 294 (Encoding: UTF-8)records ... (len: 63)\n",
      "Finish: Get Page 295 (Encoding: UTF-8)records ... (len: 65)\n",
      "Finish: Get Page 296 (Encoding: UTF-8)records ... (len: 20)\n",
      "Finish: Get Page 299 (Encoding: UTF-8)records ... (len: 361)\n",
      "Finish: Get Page 300 (Encoding: UTF-8)records ... (len: 361)\n",
      "Finish: Get Page 301 (Encoding: UTF-8)records ... (len: 364)\n",
      "Finish: Get Page 302 (Encoding: UTF-8)records ... (len: 170)\n",
      "Finish: Get Page 303 (Encoding: UTF-8)records ... (len: 154)\n",
      "Finish: Get Page 304 (Encoding: cp1252)records ... (len: 117)\n",
      "Finish: Get Page 305 (Encoding: UTF-8)records ... (len: 1987)\n",
      "Finish: Get Page 312 (Encoding: cp1252)records ... (len: 136)\n",
      "Finish: Get Page 313 (Encoding: UTF-8)records ... (len: 383)\n",
      "Finish: Get Page 314 (Encoding: UTF-8)records ... (len: 317)\n",
      "Finish: Get Page 315 (Encoding: cp1252)records ... (len: 314)\n",
      "Finish: Get Page 316 (Encoding: cp1252)records ... (len: 357)\n",
      "Finish: Get Page 317 (Encoding: cp1252)records ... (len: 370)\n",
      "Finish: Get Page 323 (Encoding: UTF-8)records ... (len: 389)\n",
      "Finish: Get Page 324 (Encoding: UTF-8)records ... (len: 330)\n",
      "pages: 164  domains: 55\n",
      "CPU times: user 4.16 s, sys: 69.5 ms, total: 4.23 s\n",
      "Wall time: 4.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "urls = [rec['Page URL'] for rec in storage.iter_records(language='en',contain_button = True, file_type='T')]\n",
    "X_raw, y, page_positions = storage.get_Xy(language='en',contain_button = True,  contain_position=True,file_type='T', scaled_page='normal')\n",
    "print(\"pages: {}  domains: {}\".format(len(urls), len({get_domain(url) for url in urls})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_page_seq = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slice data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_data(x, y, p):\n",
    "    new_tmp_x_array = []\n",
    "    new_tmp_y_array = []\n",
    "    new_tmp_p_array = []\n",
    "    for tmp_x, tmp_y, tmp_p in zip(x, y, p):\n",
    "        new_tmp_x_array.extend(chunks(tmp_x, max_page_seq))\n",
    "        new_tmp_y_array.extend(chunks(tmp_y, max_page_seq))\n",
    "        new_tmp_p_array.extend(chunks(tmp_p, max_page_seq))\n",
    "    return new_tmp_x_array, new_tmp_y_array, new_tmp_p_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_x, chunks_y, chunk_positions = get_chunks_data(X_raw, y, page_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained Bert model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import bert\n",
    "from bert import tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "from BertModel import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_short_model = BertModel(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_long_model = BertModel(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbert = bert_short_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained Fastext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "from FastTextModel import FastTextModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dimension:  100\n"
     ]
    }
   ],
   "source": [
    "ft = FastTextModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 µs, sys: 4 µs, total: 15 µs\n",
      "Wall time: 21 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# XXX: these functions should be copy-pasted from autopager/model.py\n",
    "\n",
    "def _as_list(generator, limit=None):\n",
    "    \"\"\"\n",
    "    >>> _as_list(ngrams_wb(\"text\", 2, 2), 0)\n",
    "    []\n",
    "    >>> _as_list(ngrams_wb(\"text\", 2, 2), 2)\n",
    "    ['te', 'ex']\n",
    "    >>> _as_list(ngrams_wb(\"text\", 2, 2))\n",
    "    ['te', 'ex', 'xt']\n",
    "    \"\"\"\n",
    "    return list(generator if limit is None else islice(generator, 0, limit))\n",
    "\n",
    "def feat_to_tokens(feat, tokenizer):\n",
    "    if type(feat) == type([]):\n",
    "        feat = ' '.join(feat)\n",
    "    tokens = tokenizer.tokenize(feat)\n",
    "    return tokens\n",
    "\n",
    "def link_to_features(link):\n",
    "    text = normalize(get_link_text(link))\n",
    "    href = get_link_href(link)\n",
    "    if href is None:\n",
    "        href = \"\"\n",
    "    p = urlsplit(href)\n",
    "    parent = link.xpath('..').extract()\n",
    "    parent = get_first_tag(parser, parent[0])\n",
    "    query_parsed = parse_qsl(p.query) #parse query string from path\n",
    "    query_param_names = [k.lower() for k, v in query_parsed]\n",
    "    query_param_names_ngrams = _as_list(ngrams_wb(\n",
    "        \" \".join([normalize(name) for name in query_param_names]), 3, 5, True\n",
    "    ))\n",
    "\n",
    "    # Classes of link itself and all its children.\n",
    "    # It is common to have e.g. span elements with fontawesome\n",
    "    # arrow icon classes inside <a> links.\n",
    "    self_and_children_classes = ' '.join(_as_list(link.xpath(\".//@class\").extract(), 5))\n",
    "#     print(\"self and children: \",_as_list(link.xpath(\".//@class\").extract(), 5))\n",
    "    parent_classes = ' '.join(_as_list(link.xpath('../@class').extract(), 5))\n",
    "#     print(\"parent: \",_as_list(link.xpath('../@class').extract(), 5))\n",
    "    css_classes = normalize(parent_classes + ' ' + self_and_children_classes)\n",
    "#     print(css_classes)\n",
    "    token_feature = {\n",
    "        'text-before': '',\n",
    "        'text-exact': replace_digits(text.strip()[:40].strip()),\n",
    "        'text-after': '',\n",
    "        'class': css_classes,\n",
    "        'query': _as_list(query_param_names, 10),\n",
    "        'parent-tag': parent,\n",
    "    }\n",
    "    tag_feature = {\n",
    "        'isdigit': 1 if text.isdigit() is True else 0,\n",
    "        'isalpha': 1 if text.isalpha() is True else 0,\n",
    "        'has-href': 0 if href is \"\" else 1,\n",
    "        'path-has-page': 1 if 'page' in p.path.lower() else 0,\n",
    "        'path-has-pageXX': 1 if re.search(r'[/-](?:p|page\\w?)/?\\d+', p.path.lower()) is not None else 0,\n",
    "        'path-has-number': 1 if any(part.isdigit() for part in p.path.split('/')) else 0,\n",
    "        'href-has-year': 1 if re.search('20\\d\\d', href) is not None else 0,\n",
    "        'class-has-disabled': 1 if 'disabled' in css_classes else 0,\n",
    "        'num-tokens': _num_tokens_feature(text),\n",
    "    }\n",
    "    tag_feature = [v for k,v in tag_feature.items()]\n",
    "#     attribute_feature = elem_rel + elem_target\n",
    "    non_token_feature = tag_feature #+ attribute_feature\n",
    "    return [token_feature, non_token_feature]\n",
    "\n",
    "\n",
    "def page_to_features(xseq):\n",
    "    feat_list = [link_to_features(a) for a in xseq]\n",
    "    around = get_text_around_selector_list(xseq, max_length=15)\n",
    "    \n",
    "    # weight is less than 1 because there is a lot of duplicate information\n",
    "    # in these ngrams and so we want to regularize them stronger\n",
    "    # (as if they are a single feature, not many features)\n",
    "    k = 0.2\n",
    "    for feat, (before, after) in zip(feat_list, around):\n",
    "        feat[0]['text-before'] = normalize(before)\n",
    "        feat[0]['text-after'] = normalize(after)\n",
    "        \n",
    "    return feat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_tag_features_from_chunks(chunks):\n",
    "    token_features = []\n",
    "    tag_features = []\n",
    "    for idx, page in enumerate(chunks):\n",
    "        try:\n",
    "            feat_list = page_to_features(page)\n",
    "            token_features.append([node[0] for node in feat_list])\n",
    "            tag_features.append([node[1] for node in feat_list])\n",
    "        except:\n",
    "            raise Exception(f\"Error occured on {idx}\")\n",
    "    return token_features, tag_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tokens_from_token_features(token_features, tokenizer):\n",
    "    train_tag_feature_token_list = []\n",
    "    for page in token_features:\n",
    "        tmp_page_list = []\n",
    "        for node in page: \n",
    "            tmp_list = ['[CLS]']\n",
    "            start = False\n",
    "            for k, v in node.items():\n",
    "                if k == 'class':\n",
    "                    start = True\n",
    "                if start is True:\n",
    "                    tokens = feat_to_tokens(v, tokenizer)\n",
    "                    tmp_list = tmp_list + tokens + ['[SEP]']\n",
    "            tmp_page_list.append(tmp_list)\n",
    "        train_tag_feature_token_list.append(tmp_page_list)\n",
    "    return train_tag_feature_token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_to_two_bert_embeddings(token_features, tokenizer):\n",
    "    text_first_segs = []\n",
    "    text_second_segs = []\n",
    "    for page in token_features:\n",
    "        page_one_features = []\n",
    "        page_two_features = []\n",
    "        for node in page:\n",
    "            text_before = tokenizer.tokenize(node[\"text-before\"])\n",
    "            text_exact = tokenizer.tokenize(node[\"text-exact\"])\n",
    "            text_after = tokenizer.tokenize(node[\"text-after\"])\n",
    "            page_one_features.append([\"[CLS]\"]+text_before+[\"[SEP]\"]+text_exact+[\"[SEP]\"])\n",
    "            page_two_features.append([\"[CLS]\"]+text_exact+[\"[SEP]\"]+text_after+[\"[SEP]\"])\n",
    "        text_first_segs.append(page_one_features)\n",
    "        text_second_segs.append(page_two_features)\n",
    "    print(\"Start encode first seg embeddings\")\n",
    "    first_emb = pbert.page_list_to_bert_embedding_list(text_first_segs, Token=True)\n",
    "    print(\"Start encode second seg embeddings\")\n",
    "    second_emb = pbert.page_list_to_bert_embedding_list(text_second_segs, Token=True)\n",
    "    full_text_emb = [np.concatenate([first_emb[page], second_emb[page]], axis = 1) for page in range(len(token_features))]\n",
    "    return first_emb, second_emb, full_text_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_vector(ft, word_list):\n",
    "    if type(word_list) == type([]):\n",
    "        if len(word_list) == 0:\n",
    "            return np.zeros(ft.getModel().get_dimension())\n",
    "        else:\n",
    "            vectors_array = []\n",
    "            for word in word_list:\n",
    "                vector = ft.getWordVector(word)\n",
    "                vectors_array.append(vector)\n",
    "            mean_vector = np.mean(vectors_array, axis = 0)\n",
    "            return mean_vector\n",
    "    else:\n",
    "        return ft.getWordVector(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pages_to_word_vector(ft, token_features):\n",
    "    pages_vector = []\n",
    "    for page in token_features:\n",
    "        page_vectors = []\n",
    "        for node in page:\n",
    "            classes = word_to_vector(ft, node['class'])\n",
    "            query = word_to_vector(ft, node['query'])\n",
    "            p_tag = word_to_vector(ft, node['parent-tag'])\n",
    "            full_vector = np.concatenate([classes, query, p_tag], axis = 0)\n",
    "            page_vectors.append(full_vector)\n",
    "        pages_vector.append(np.array(page_vectors))\n",
    "    return pages_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pages_to_word_vector_from_keylist(ft, token_features, word_to_vec_list = token_feature_list):\n",
    "    print(f\"Transform key {word_to_vec_list} to word_vector ... \")\n",
    "    pages_vector = []\n",
    "    for page in token_features:\n",
    "        page_vectors = []\n",
    "        for node in page:\n",
    "            full_vector_list = []\n",
    "            for k,v in node.items():\n",
    "                if k in word_to_vec_list:\n",
    "                    full_vector_list.append(word_to_vector(ft, v))\n",
    "            full_vector = np.concatenate(full_vector_list, axis=0)\n",
    "            page_vectors.append(full_vector)\n",
    "        pages_vector.append(np.array(page_vectors))\n",
    "    return pages_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_features, tag_features = get_token_tag_features_from_chunks(chunks_x)\n",
    "# train_tag_feature_token_list = extract_tokens_from_token_features(token_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text-before': '',\n",
       " 'text-exact': 'community',\n",
       " 'text-after': '',\n",
       " 'class': 'page active',\n",
       " 'query': [],\n",
       " 'parent-tag': 'li',\n",
       " 'text': ['co',\n",
       "  'om',\n",
       "  'mm',\n",
       "  'mu',\n",
       "  'un',\n",
       "  'ni',\n",
       "  'it',\n",
       "  'ty',\n",
       "  'com',\n",
       "  'omm',\n",
       "  'mmu',\n",
       "  'mun',\n",
       "  'uni',\n",
       "  'nit',\n",
       "  'ity',\n",
       "  'comm',\n",
       "  'ommu',\n",
       "  'mmun',\n",
       "  'muni',\n",
       "  'unit',\n",
       "  'nity',\n",
       "  'commu',\n",
       "  'ommun',\n",
       "  'mmuni',\n",
       "  'munit',\n",
       "  'unity']}"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_features[0][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_feature_list = list(token_features[0][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform key ['text-before', 'text-exact', 'text-after', 'class', 'query', 'parent-tag'] to word_vector ... \n"
     ]
    }
   ],
   "source": [
    "# Use ft to encode all token_features\n",
    "ft_full_tokens_emb = pages_to_word_vector_from_keylist(ft, token_features, token_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 600)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_full_tokens_emb[0].shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for page in range(len(token_features)):\n",
    "    for itx, ity in zip(token_features[page],chunks_y[page]):\n",
    "        if ity != 'O' and len(itx['query'])!= 0:\n",
    "            print(itx['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_tokens_emb = pages_to_word_vector(ft, token_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 300)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_tokens_emb[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start encode first seg embeddings\n",
      "Use custom Token: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d0b20c45b244a88863673676da0392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=183)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start encode second seg embeddings\n",
      "Use custom Token: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475ca365da69455a85cba0cd6c17eb37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=183)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "first_emb, second_emb, full_text_emb = page_to_two_bert_embeddings(token_features, pbert.get_tokenizer())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tag_feature_tokens = extract_tokens_from_token_features(token_features, pbert.get_tokenizer())\n",
    "tag_feature_bert_embedding = bert_long_model.page_list_to_bert_embedding_list(tag_feature_tokens,Token=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.save('embedding/train/first.npy', first_emb)\n",
    "np.save('embedding/train/second.npy', second_emb)\n",
    "np.save('embedding/train/full_text.npy', full_text_emb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "first_emb = np.load('embedding/train/first.npy', allow_pickle=True)\n",
    "second_emb = np.load('embedding/train/second.npy', allow_pickle=True)\n",
    "full_text_emb = np.load('embedding/train/full_text.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First emb:(303, 768)\n",
      "Second emb:(303, 768)\n",
      "Full_text emb:(303, 1536)\n"
     ]
    }
   ],
   "source": [
    "print(f\"First emb:{first_emb[0].shape}\")\n",
    "print(f\"Second emb:{second_emb[0].shape}\")\n",
    "print(f\"Full_text emb:{full_text_emb[0].shape}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "max_node = -1\n",
    "page_sum = 0\n",
    "for page in tag_feature_tokens:\n",
    "    sum = 0\n",
    "    for node in page:\n",
    "        sum+=len(node)\n",
    "        if len(node) > max_node:\n",
    "            max_node = len(node)\n",
    "    page_sum+=sum/len(page)\n",
    "print(\"Average: \", page_sum/len(tag_feature_tokens))\n",
    "print(\"Max_node: \",max_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag_info_list = tag_features #features which only have tag true/false information"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Extract text-exact feature in token_features\n",
    "train_text_list = [[ data['text-exact'] for data in x] for x in token_features]\n",
    "train_text_before_list = [[ data['text-before'] for data in x] for x in token_features]\n",
    "train_text_after_list = [[ data['text-after'] for data in x] for x in token_features]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def concat_text(before, mid, after):\n",
    "    res = \"\"\n",
    "    if before != \"\":\n",
    "        res+=before + \",\"\n",
    "    if mid != \"\":\n",
    "        res+=mid + \",\"\n",
    "    if after == \"\":\n",
    "        res = res[:-1]\n",
    "    else:\n",
    "        res+=after\n",
    "    return res\n",
    "# Extract text-exact feature in token_features\n",
    "train_full_text_list = [[ concat_text(data['text-before'],data['text-exact'],data['text-after']) for data in x] for x in token_features]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pbert.max_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature List\n",
    "    * train_tag_feature_token_list => Tag Attributes tokens\n",
    "    * train_tag_info_list => Tag information\n",
    "    * train_text_emb => Only Text node => Bert Text embedding\n",
    "    * train_tag_emb => Text-before Text Text-after [SEP] Other Attributes => Bert Text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks_text_emb = train_text_emb # text\n",
    "# chunks_text_emb = train_tag_emb # tag\n",
    "chunks_text_emb = full_text_emb # full text embedding (two bert)\n",
    "# chunks_text_emb = second_emb\n",
    "\n",
    "chunks_tag_infos = train_tag_info_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding to fixed size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_tokens(page_tokens, tokenizer, max_len):\n",
    "    pages_class = []\n",
    "    pages_query = []\n",
    "    pages_parent_tag = []\n",
    "#     print(len(page_tokens))\n",
    "    for page in page_tokens:\n",
    "        class_page = []\n",
    "        query_page = []\n",
    "        parent_tag_page = []\n",
    "        for node in page:\n",
    "            #class\n",
    "            class_tokens = tokenizer.tokenize(node['class'])\n",
    "            class_ids = tokenizer.convert_tokens_to_ids(class_tokens)\n",
    "            class_ids = class_ids + [0] * (max_len-len(class_ids))\n",
    "            class_page.append(class_ids[:256])\n",
    "            #query\n",
    "            query_tokens = tokenizer.tokenize(' '.join(node['query']))\n",
    "            query_ids = tokenizer.convert_tokens_to_ids(query_tokens)\n",
    "            query_ids = query_ids + [0] * (max_len-len(query_ids))\n",
    "            query_page.append(query_ids[:256])\n",
    "            #parent tag\n",
    "            parent_tag_tokens = tokenizer.tokenize(node['parent-tag'])\n",
    "            parent_tag_ids = tokenizer.convert_tokens_to_ids(parent_tag_tokens)\n",
    "            parent_tag_ids = parent_tag_ids + [0] * (max_len-len(parent_tag_ids))\n",
    "            parent_tag_page.append(parent_tag_ids[:256])\n",
    "        pages_class.append(class_page)\n",
    "        pages_query.append(query_page)\n",
    "        pages_parent_tag.append(parent_tag_page)\n",
    "    return pages_class, pages_query, pages_parent_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_pad_to_npdata(embedding):\n",
    "    dataset = Dataset.from_generator(lambda: iter(embedding), tf.float32)\n",
    "    dataset = dataset.padded_batch(1, padded_shapes= (max_page_seq, len(embedding[0][0])), padding_values=-1.,drop_remainder=False)\n",
    "    after_pad = np.array([ data[0] for data in list(dataset.as_numpy_iterator())])\n",
    "    return after_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_pad_to_npdata(embedding):\n",
    "    dataset = Dataset.from_generator(lambda: iter(embedding), tf.int32)\n",
    "    dataset = dataset.padded_batch(1, padded_shapes= (max_page_seq, len(embedding[0][0])), padding_values=0,drop_remainder=False)\n",
    "    after_pad = np.array([ data[0] for data in list(dataset.as_numpy_iterator())])\n",
    "    return after_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 256"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pages_class, pages_query, pages_parent_tag = prepare_input_tokens(token_features, pbert.get_tokenizer(), max_len)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_class = token_pad_to_npdata(pages_class)\n",
    "train_query = token_pad_to_npdata(pages_query)\n",
    "train_parent_tag = token_pad_to_npdata(pages_parent_tag)\n",
    "print(\"Current Shape:\")\n",
    "print(f\"train_class: {train_class.shape}\")\n",
    "print(f\"train_query: {train_query.shape}\")\n",
    "print(f\"train_parent_tag: {train_parent_tag.shape}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_tag_feature = feature_pad_to_npdata(tag_feature_bert_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tag_token = feature_pad_to_npdata(chunks_tag_tokens)\n",
    "train_text_emb_x = feature_pad_to_npdata(chunks_text_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag_x = feature_pad_to_npdata(chunks_tag_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attr_x = feature_pad_to_npdata(ft_tokens_emb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_positions_x = feature_pad_to_npdata(chunk_positions)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_tag_x = np.concatenate([train_tag_x, train_positions_x], axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info_x = np.concatenate([train_text_emb_x, train_tag_x], axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"O\", \"PREV\", \"PAGE\", \"NEXT\", \"[PAD]\"]\n",
    "tag2idx = { label:idx for idx,label in enumerate(labels)}\n",
    "idx2tag = { idx:label for idx,label in enumerate(labels)}\n",
    "num_tags = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pad_sequences([[tag2idx.get(l) for l in lab] for lab in chunks_y], maxlen=max_page_seq, padding=\"post\", truncating=\"post\", value=tag2idx[\"[PAD]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Shape:\n",
      "train_text_emb_x: (183, 512, 1536)\n",
      "train_tag_x: (183, 512, 8)\n",
      "train_ft_x: (183, 512, 300)\n",
      "train_y: (183, 512)\n"
     ]
    }
   ],
   "source": [
    "print(\"Current Shape:\")\n",
    "print(f\"train_text_emb_x: {train_text_emb_x.shape}\")\n",
    "print(f\"train_tag_x: {train_tag_x.shape}\")\n",
    "# print(f\"train_info_x: {train_info_x.shape}\")\n",
    "print(f\"train_ft_x: {train_attr_x.shape}\")\n",
    "print(f\"train_y: {train_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x = train_text_emb_x\n",
    "train_x = train_info_x\n",
    "# train_x = train_tag_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(356, 512, 1544)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_composite_input = [train_text_emb_x, train_tag_x]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_composite_input = [train_text_emb_x, train_tag_feature, train_tag_x]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_composite_with_token = [train_text_emb_x, train_tag_x, train_class, train_query, train_parent_tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_composite_input = [train_text_emb_x, train_attr_x, train_tag_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(183, 512, 1536)\n",
      "(183, 512, 300)\n",
      "(183, 512, 8)\n"
     ]
    }
   ],
   "source": [
    "for inputs in train_composite_input:\n",
    "    print(inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input: Bert text + Ft tag + tag information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tag_token = feature_pad_to_npdata(chunks_tag_tokens)\n",
    "train_text_emb_x = feature_pad_to_npdata(chunks_text_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag_x = feature_pad_to_npdata(chunks_tag_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attr_x = feature_pad_to_npdata(ft_tokens_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_composite_input = [train_text_emb_x, train_attr_x, train_tag_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(183, 512, 1536)\n",
      "(183, 512, 300)\n",
      "(183, 512, 8)\n"
     ]
    }
   ],
   "source": [
    "for inputs in train_composite_input:\n",
    "    print(inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input: FastText All + Tag information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_emb_x = feature_pad_to_npdata(ft_full_tokens_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag_x = feature_pad_to_npdata(train_tag_info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_composite_input = [train_text_emb_x, train_tag_x] #Ft, taginfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(183, 512, 600)\n",
      "(183, 512, 8)\n"
     ]
    }
   ],
   "source": [
    "for inputs in train_composite_input:\n",
    "    print(inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build BERT-BiLSTM-CRF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_addons.layers.crf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (Dense, Input, Bidirectional, LSTM, Embedding, Masking, Concatenate,\n",
    "                                    AveragePooling2D, GlobalAveragePooling2D, Reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestWeightCallback(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.best_weights = None\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.best = np.Inf\n",
    "        self.best_epoch = np.Inf\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"val_loss\")\n",
    "        epoch = epoch + 1\n",
    "        if np.less(current, self.best):\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            self.best = current\n",
    "            self.best_epoch = epoch\n",
    "    def on_train_end(self, logs=None):\n",
    "        print(f\"Training Finish, Best epoch: {self.best_epoch}, Best Val_loss: {self.best}\")\n",
    "        self.model.set_weights(self.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME_STAMP: 512\n",
      "HIDDEN_UNITS: 200\n",
      "DROPOUT_RATE: 0.1\n",
      "NUM_CLASS: 5\n"
     ]
    }
   ],
   "source": [
    "TIME_STAMPS = max_page_seq\n",
    "HIDDEN_UNITS = 200\n",
    "DROPOUT_RATE = 0.1\n",
    "# NUM_CLASS = 5\n",
    "NUM_CLASS = num_tags\n",
    "print(f\"TIME_STAMP: {TIME_STAMPS}\")\n",
    "print(f\"HIDDEN_UNITS: {HIDDEN_UNITS}\")\n",
    "print(f\"DROPOUT_RATE: {DROPOUT_RATE}\")\n",
    "print(f\"NUM_CLASS: {NUM_CLASS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwCallback = BestWeightCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#For custom embedding\n",
    "embbed_output_shape = 128\n",
    "page_embbed_shape = (512, 256)\n",
    "pool_size = (1, embbed_output_shape)\n",
    "tag_emb_shape = train_class[0].shape\n",
    "vocab_size = len(pbert.get_tokenizer().vocab)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_custom_emb_model(use_crf = True):\n",
    "    #BERT-BiLSTM-SoftMAX with Custom Embeddings\n",
    "    input_bert_embeddings = Input(shape=(bert_shape), name=\"input_bert_embeddings\")\n",
    "    input_tag_information = Input(shape=(tag_info_shape), name=\"input_tag_information\")\n",
    "    input_class = Input(shape=(tag_emb_shape), name=\"input_class\")\n",
    "    input_query = Input(shape=(tag_emb_shape), name=\"input_query\")\n",
    "    input_parent_tag = Input(shape=(tag_emb_shape), name=\"input_parent_tag\")\n",
    "\n",
    "    #Embedding layers\n",
    "    ## input_class\n",
    "    class_emb = Embedding(input_dim = vocab_size, output_dim = embbed_output_shape, input_length=max_page_seq, mask_zero = True)(input_class)\n",
    "    class_emb = AveragePooling2D(1, pool_size, data_format = 'channels_first')(class_emb)\n",
    "    class_emb = Reshape(page_embbed_shape, name=\"class_emb_out\")(class_emb)\n",
    "    ## input_query\n",
    "    query_emb = Embedding(input_dim = vocab_size, output_dim = embbed_output_shape, input_length=max_page_seq, mask_zero = True)(input_query)\n",
    "    query_emb = AveragePooling2D(1, pool_size, data_format = 'channels_first')(query_emb)\n",
    "    query_emb = Reshape(page_embbed_shape, name=\"query_emb_out\")(query_emb)\n",
    "    ## input_parent_tag\n",
    "    parent_tag_emb = Embedding(input_dim = vocab_size, output_dim = embbed_output_shape, input_length=max_page_seq, mask_zero = True)(input_parent_tag)\n",
    "    parent_tag_emb = AveragePooling2D(1, pool_size, data_format = 'channels_first')(parent_tag_emb)\n",
    "    parent_tag_emb = Reshape(page_embbed_shape, name=\"parent_tag_emb_out\")(parent_tag_emb)\n",
    "\n",
    "    # input_tags = Concatenate()([class_emb, query_emb, parent_tag_emb])\n",
    "    input_tags = Concatenate()([class_emb, query_emb, parent_tag_emb, input_tag_information])\n",
    "    input_tags_FFN = Dense(units = 324, activation = 'relu')(input_tags)\n",
    "    input_tags_FFN = Dense(units = 162, activation = 'relu', name=\"input_tag_FFN_out\")(input_tags_FFN)\n",
    "\n",
    "    bert_FFN = Masking(mask_value=-1.)(input_bert_embeddings)\n",
    "    bert_FFN = Dense(units = 768, activation = 'relu')(input_bert_embeddings)\n",
    "    bert_FFN = Dense(units = 324, activation = 'relu')(bert_FFN)\n",
    "    bert_FFN = Dense(units = 162, activation = 'relu', name=\"bert_FFN_out\")(bert_FFN)\n",
    "\n",
    "    # merged = Concatenate()([bert_FFN, input_tags_FFN])\n",
    "    merged = Concatenate()([bert_FFN, input_tags_FFN, input_tag_information])\n",
    "    model = Bidirectional(LSTM(units=100, return_sequences=True))(merged)\n",
    "    if use_crf:\n",
    "        crf=CRF(5,name='crf_layer')\n",
    "        out =crf(model)\n",
    "    else:\n",
    "        out = Dense(units = 5, activation='softmax')(model)\n",
    "    model = Model([input_bert_embeddings, input_tag_information, input_class, input_query, input_parent_tag], out)\n",
    "    if use_crf:\n",
    "        model.compile('adam',loss={'crf_layer': crf.get_loss})\n",
    "    else:\n",
    "        model.compile('adam',loss=tf.keras.losses.SparseCategoricalCrossentropy())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For ft_bert_bilstm\n",
    "bert_shape = train_text_emb_x[0].shape\n",
    "tag_info_shape = train_tag_x[0].shape\n",
    "ft_emb_shape = train_attr_x[0].shape\n",
    "# tag_emb_shape = train_tag_feature[0].shape\n",
    "def get_ft_bert_bilstm_model_simple(use_crf = True):\n",
    "    #BERT-BiLSTM-SoftMAX with Custom Embeddings\n",
    "    input_bert_embeddings = Input(shape=(bert_shape), name=\"input_bert_embeddings\")\n",
    "    input_tag_information = Input(shape=(tag_info_shape), name=\"input_tag_information\")\n",
    "    input_ft_embeddings = Input(shape=(ft_emb_shape), name=\"input_ft_embeddings\")\n",
    "    \n",
    "    input_tags_FFN = Concatenate()([input_bert_embeddings, input_ft_embeddings, input_tag_information])\n",
    "    if not use_crf:\n",
    "        input_tags_FFN = Masking(mask_value=-1.)(input_tags_FFN)\n",
    "    input_tags_FFN = Dense(units = 768, activation = 'relu')(input_tags_FFN)\n",
    "    input_tags_FFN = Dense(units = 324, activation = 'relu', name=\"tags_FFN_out\")(input_tags_FFN)\n",
    "    \n",
    "    model = Bidirectional(LSTM(units=HIDDEN_UNITS//2, return_sequences=True))(input_tags_FFN)\n",
    "    if use_crf:\n",
    "        crf=CRF(NUM_CLASS,name='crf_layer')\n",
    "        out =crf(model)\n",
    "    else:\n",
    "        out = Dense(units = NUM_CLASS, activation='softmax')(model)\n",
    "    model = Model([input_bert_embeddings, input_ft_embeddings, input_tag_information], out)\n",
    "    if use_crf:\n",
    "        model.compile('adam',loss={'crf_layer': crf.get_loss})\n",
    "    else:\n",
    "        model.compile('adam',loss=tf.keras.losses.SparseCategoricalCrossentropy())\n",
    "    return model\n",
    "def get_ft_bert_bilstm_model(use_crf = True):\n",
    "    #BERT-BiLSTM-SoftMAX with Custom Embeddings\n",
    "    input_bert_embeddings = Input(shape=(bert_shape), name=\"input_bert_embeddings\")\n",
    "    input_tag_information = Input(shape=(tag_info_shape), name=\"input_tag_information\")\n",
    "    input_ft_embeddings = Input(shape=(ft_emb_shape), name=\"input_ft_embeddings\")\n",
    "    \n",
    "#     input_tags_FFN = Masking(mask_value=-1.)(input_tags_info)\n",
    "    input_tags_FFN = Dense(units = 150, activation = 'relu')(input_ft_embeddings)\n",
    "    input_tags_FFN = Dense(units = 75, activation = 'relu', name=\"tags_FFN_out\")(input_tags_FFN)\n",
    "    \n",
    "#     bert_FFN = Masking(mask_value=-1.)(input_bert_embeddings)\n",
    "    bert_FFN = Dense(units = 768, activation = 'relu')(input_bert_embeddings)\n",
    "    bert_FFN = Dense(units = 324, activation = 'relu')(bert_FFN)\n",
    "    bert_FFN = Dense(units = 162, activation = 'relu', name=\"bert_FFN_out\")(bert_FFN)\n",
    "\n",
    "    # merged = Concatenate()([bert_FFN, input_tags_FFN])\n",
    "    merged = Concatenate()([bert_FFN, input_tags_FFN, input_tag_information])\n",
    "    model = Bidirectional(LSTM(units=HIDDEN_UNITS//2, return_sequences=True))(merged)\n",
    "    if use_crf:\n",
    "        crf=CRF(NUM_CLASS,name='crf_layer')\n",
    "        out =crf(model)\n",
    "    else:\n",
    "        out = Dense(units = NUM_CLASS, activation='softmax')(model)\n",
    "    model = Model([input_bert_embeddings, input_ft_embeddings, input_tag_information], out)\n",
    "    if use_crf:\n",
    "        model.compile('adam',loss={'crf_layer': crf.get_loss})\n",
    "    else:\n",
    "        model.compile('adam',loss=tf.keras.losses.SparseCategoricalCrossentropy())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get ft-bert models and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_ft_bert_bilstm_model(use_crf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_ft_bert_bilstm_model_simple(use_crf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_22\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_bert_embeddings (InputLay [(None, 512, 1536)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_59 (Dense)                (None, 512, 768)     1180416     input_bert_embeddings[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_ft_embeddings (InputLayer [(None, 512, 300)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_60 (Dense)                (None, 512, 324)     249156      dense_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_58 (Dense)                (None, 512, 150)     45150       input_ft_embeddings[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bert_FFN_out (Dense)            (None, 512, 162)     52650       dense_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tags_FFN_out (Dense)            (None, 512, 75)      11325       dense_58[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_tag_information (InputLay [(None, 512, 8)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 512, 245)     0           bert_FFN_out[0][0]               \n",
      "                                                                 tags_FFN_out[0][0]               \n",
      "                                                                 input_tag_information[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_20 (Bidirectional (None, 512, 100)     118400      concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_61 (Dense)                (None, 512, 5)       505         bidirectional_20[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 1,657,602\n",
      "Trainable params: 1,657,602\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 1s 318ms/step - loss: 0.9116 - val_loss: 0.1477\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.0915 - val_loss: 0.1014\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.0741 - val_loss: 0.1001\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.0730 - val_loss: 0.0973\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.0699 - val_loss: 0.0942\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.0680 - val_loss: 0.0933\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.0665 - val_loss: 0.0922\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.0652 - val_loss: 0.0919\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.0641 - val_loss: 0.0908\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 0.0625 - val_loss: 0.0893\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 0.0618 - val_loss: 0.0886\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.0609 - val_loss: 0.0881\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.0595 - val_loss: 0.0866\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.0577 - val_loss: 0.0847\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.0554 - val_loss: 0.0847\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.0547 - val_loss: 0.0811\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.0517 - val_loss: 0.0780\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.0510 - val_loss: 0.0728\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.0480 - val_loss: 0.0691\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 0.0455 - val_loss: 0.0659\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.0436 - val_loss: 0.0643\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.0425 - val_loss: 0.0617\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 0.0401 - val_loss: 0.0591\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 0.0383 - val_loss: 0.0592\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.0358 - val_loss: 0.0571\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 141ms/step - loss: 0.0353 - val_loss: 0.0566\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.0329 - val_loss: 0.0535\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.0323 - val_loss: 0.0532\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.0310 - val_loss: 0.0520\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.0297 - val_loss: 0.0515\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 0.0290 - val_loss: 0.0518\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.0280 - val_loss: 0.0505\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 0.0269 - val_loss: 0.0584\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.0275 - val_loss: 0.0483\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0266 - val_loss: 0.0513\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 0.0261 - val_loss: 0.0504\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 0.0259 - val_loss: 0.0479\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.0255 - val_loss: 0.0500\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.0238 - val_loss: 0.0459\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0243 - val_loss: 0.0533\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 0.0238 - val_loss: 0.0474\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 0.0227 - val_loss: 0.0520\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 0.0226 - val_loss: 0.0470\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 0.0216 - val_loss: 0.0482\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.0212 - val_loss: 0.0458\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 0.0207 - val_loss: 0.0482\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 0.0205 - val_loss: 0.0466\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.0199 - val_loss: 0.0460\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.0201 - val_loss: 0.0478\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0193 - val_loss: 0.0442\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 0.0191 - val_loss: 0.0482\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.0193 - val_loss: 0.0470\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 0.0187 - val_loss: 0.0475\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 0.0189 - val_loss: 0.0469\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.0182 - val_loss: 0.0450\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.0178 - val_loss: 0.0469\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.0174 - val_loss: 0.0465\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.0169 - val_loss: 0.0447\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.0166 - val_loss: 0.0457\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 0.0164 - val_loss: 0.0447\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.0163 - val_loss: 0.0448\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0167 - val_loss: 0.0510\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 0.0165 - val_loss: 0.0463\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.0164 - val_loss: 0.0484\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 0.0150 - val_loss: 0.0465\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 0.0150 - val_loss: 0.0483\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 0.0144 - val_loss: 0.0473\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.0140 - val_loss: 0.0487\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.0137 - val_loss: 0.0485\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0135 - val_loss: 0.0502\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.0129 - val_loss: 0.0500\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.0128 - val_loss: 0.0512\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.0124 - val_loss: 0.0520\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 0.0119 - val_loss: 0.0518\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.0118 - val_loss: 0.0531\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 0.0116 - val_loss: 0.0543\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.0113 - val_loss: 0.0556\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.0110 - val_loss: 0.0583\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 0.0109 - val_loss: 0.0564\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 0.0104 - val_loss: 0.0581\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.0101 - val_loss: 0.0593\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.0106 - val_loss: 0.0642\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 127ms/step - loss: 0.0120 - val_loss: 0.0616\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0110 - val_loss: 0.0622\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0116 - val_loss: 0.0615\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 0.0103 - val_loss: 0.0611\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 0.0101 - val_loss: 0.0626\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 0.0098 - val_loss: 0.0636\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.0096 - val_loss: 0.0648\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.0094 - val_loss: 0.0637\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 0.0088 - val_loss: 0.0628\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.0083 - val_loss: 0.0620\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 101ms/step - loss: 0.0081 - val_loss: 0.0629\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 0.0079 - val_loss: 0.0643\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 0.0078 - val_loss: 0.0653\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 0.0076 - val_loss: 0.0656\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 0.0075 - val_loss: 0.0663\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 0.0073 - val_loss: 0.0663\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.0072 - val_loss: 0.0662\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 0.0072 - val_loss: 0.0657\n",
      "Training Finish, Best epoch: 50, Best Val_loss: 0.04418157413601875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1a3e8b17b8>"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_composite_input, train_y, batch_size=64, epochs=100, validation_split=0.1, verbose=1, callbacks=[bwCallback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get full ft model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME_STAMP: 512\n",
      "HIDDEN_UNITS: 100\n",
      "DROPOUT_RATE: 0.1\n",
      "NUM_CLASS: 5\n"
     ]
    }
   ],
   "source": [
    "TIME_STAMPS = max_page_seq\n",
    "HIDDEN_UNITS = 100\n",
    "DROPOUT_RATE = 0.1\n",
    "# NUM_CLASS = 5\n",
    "NUM_CLASS = num_tags\n",
    "print(f\"TIME_STAMP: {TIME_STAMPS}\")\n",
    "print(f\"HIDDEN_UNITS: {HIDDEN_UNITS}\")\n",
    "print(f\"DROPOUT_RATE: {DROPOUT_RATE}\")\n",
    "print(f\"NUM_CLASS: {NUM_CLASS}\")\n",
    "tag_info_shape = train_tag_x[0].shape\n",
    "ft_emb_shape = train_text_emb_x[0].shape\n",
    "def get_ft_bilstm_model(use_crf = True):\n",
    "    input_tag_information = Input(shape=(tag_info_shape), name=\"input_tag_information\")\n",
    "    input_ft_embeddings = Input(shape=(ft_emb_shape), name=\"input_ft_embeddings\")\n",
    "    \n",
    "    input_tags_FFN = Dense(units = 300, activation = 'relu')(input_ft_embeddings)\n",
    "    input_tags_FFN = Dense(units = 150, activation = 'relu', name=\"tags_FFN_out\")(input_tags_FFN)\n",
    "\n",
    "    merged = Concatenate()([input_tag_information, input_tags_FFN])\n",
    "    model = Bidirectional(LSTM(units=HIDDEN_UNITS//2, return_sequences=True))(merged)\n",
    "    if use_crf:\n",
    "        crf=CRF(NUM_CLASS,name='crf_layer')\n",
    "        out =crf(model)\n",
    "    else:\n",
    "        out = Dense(units = NUM_CLASS, activation='softmax')(model)\n",
    "    model = Model([input_ft_embeddings, input_tag_information], out)\n",
    "    if use_crf:\n",
    "        model.compile('adam',loss={'crf_layer': crf.get_loss})\n",
    "    else:\n",
    "        model.compile('adam',loss=tf.keras.losses.SparseCategoricalCrossentropy())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_ft_bilstm_model(use_crf = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ft_embeddings (InputLayer [(None, 512, 600)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_50 (Dense)                (None, 512, 300)     180300      input_ft_embeddings[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "input_tag_information (InputLay [(None, 512, 8)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tags_FFN_out (Dense)            (None, 512, 150)     45150       dense_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 512, 158)     0           input_tag_information[0][0]      \n",
      "                                                                 tags_FFN_out[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_17 (Bidirectional (None, 512, 100)     83600       concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_51 (Dense)                (None, 512, 5)       505         bidirectional_17[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 309,555\n",
      "Trainable params: 309,555\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3/3 [==============================] - 1s 274ms/step - loss: 1.2681 - val_loss: 0.6623\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.3728 - val_loss: 0.3175\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.1678 - val_loss: 0.1565\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.0969 - val_loss: 0.1127\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 0.0816 - val_loss: 0.1028\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.0767 - val_loss: 0.0956\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.0717 - val_loss: 0.0875\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.0651 - val_loss: 0.0808\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.0600 - val_loss: 0.0776\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 0.0559 - val_loss: 0.0735\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.0511 - val_loss: 0.0644\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.0442 - val_loss: 0.0562\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 0.0395 - val_loss: 0.0514\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 0.0358 - val_loss: 0.0499\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 0.0332 - val_loss: 0.0511\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.0316 - val_loss: 0.0523\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 0.0303 - val_loss: 0.0522\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 0.0294 - val_loss: 0.0520\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 0.0287 - val_loss: 0.0510\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.0279 - val_loss: 0.0517\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.0271 - val_loss: 0.0517\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.0263 - val_loss: 0.0512\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.0257 - val_loss: 0.0502\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.0251 - val_loss: 0.0503\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.0245 - val_loss: 0.0504\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 0.0240 - val_loss: 0.0508\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.0234 - val_loss: 0.0518\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 0.0228 - val_loss: 0.0522\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.0223 - val_loss: 0.0519\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 0.0216 - val_loss: 0.0529\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 0.0211 - val_loss: 0.0530\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.0206 - val_loss: 0.0535\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 0.0200 - val_loss: 0.0564\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.0195 - val_loss: 0.0585\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.0190 - val_loss: 0.0572\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 0.0185 - val_loss: 0.0581\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.0181 - val_loss: 0.0599\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.0176 - val_loss: 0.0586\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.0172 - val_loss: 0.0571\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.0168 - val_loss: 0.0580\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.0164 - val_loss: 0.0626\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.0161 - val_loss: 0.0606\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.0156 - val_loss: 0.0597\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.0152 - val_loss: 0.0627\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.0149 - val_loss: 0.0645\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0146 - val_loss: 0.0624\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0142 - val_loss: 0.0616\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.0139 - val_loss: 0.0641\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 0.0136 - val_loss: 0.0644\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.0133 - val_loss: 0.0616\n",
      "Training Finish, Best epoch: 14, Best Val_loss: 0.049879852682352066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1a669e4550>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_composite_input, train_y, batch_size=64, epochs=50, validation_split=0.1, verbose=1, callbacks=[bwCallback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get bert and train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "def get_bert_embeddings_bilstm_softmax():\n",
    "    # BiLSTM-Softmax bert FT + bert Tag\n",
    "    input_bert_embeddings = Input(shape=(bert_shape), dtype=tf.float32,\n",
    "                                                   name=\"input_bert_embeddings\")\n",
    "    input_tag_information = Input(shape=(tag_info_shape), dtype=tf.float32,\n",
    "                                               name=\"input_tag_information\")\n",
    "    input_tag_embeddings = Input(shape=(tag_emb_shape), dtype=tf.float32,\n",
    "                                                   name=\"input_tag_embeddings\")\n",
    "\n",
    "    input_tags = Masking(mask_value=-1.)(input_tag_embeddings)\n",
    "    input_tags_FFN = Concatenate()([input_tags, input_tag_information])\n",
    "    input_tags_FFN = Dense(units = 324, activation = 'relu')(input_tags_FFN)\n",
    "    input_tags_FFN = Dense(units = 162, activation = 'relu', name=\"input_tag_FFN_out\")(input_tags_FFN)\n",
    "\n",
    "    bert_FFN = Masking(mask_value=-1.)(input_bert_embeddings)\n",
    "    bert_FFN = Dense(units = 768, activation = 'relu')(bert_FFN)\n",
    "    bert_FFN = Dense(units = 324, activation = 'relu', name=\"bert_FFN_out\")(bert_FFN)\n",
    "\n",
    "\n",
    "    merged = Concatenate()([bert_FFN, input_tags_FFN, input_tag_information])\n",
    "    model = Bidirectional(LSTM(units=100, return_sequences=True))(merged)\n",
    "    out = Dense(units = 5, activation='softmax')(model)\n",
    "    model = Model([input_bert_embeddings, input_tag_embeddings, input_tag_information], out)\n",
    "    model.compile('adam',loss=tf.keras.losses.SparseCategoricalCrossentropy())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.fit(train_composite_input, train_y, batch_size=32, epochs=1000, validation_split=0.1, verbose=1, callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_BERT_BILSTM_CRF(SHAPE, numtags):\n",
    "    print(f\"SHAPE: {SHAPE}\")\n",
    "    model=tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=SHAPE))\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=HIDDEN_UNITS, return_sequences=True)))\n",
    "    crf=CRF(numtags,name='crf_layer')\n",
    "    model.add(crf)\n",
    "    model.compile('adam',loss={'crf_layer': crf.get_loss})\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_BERT_FFN(SHAPE, numtags):\n",
    "    print(f\"SHAPE: {SHAPE}\")\n",
    "    model=tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=SHAPE))\n",
    "    model.add(tf.keras.layers.Dense(units = 768, activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(units = 324, activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(units = 162, activation = 'relu'))\n",
    "#     model.add(tf.keras.layers.Dense(units = 81, activation = 'relu'))\n",
    "#     crf=CRF(numtags,name='crf_layer')\n",
    "    model.add(tf.keras.layers.Dense(units = numtags, activation='softmax'))\n",
    "#     model.add(crf)\n",
    "#     model.compile('adam',loss={'crf_layer': crf.get_loss})\n",
    "    model.compile('adam',loss=tf.keras.losses.SparseCategoricalCrossentropy())\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "def get_BERT_BILSTM_SOFTMAX(SHAPE, numtags):\n",
    "    print(f\"SHAPE: {SHAPE}\")\n",
    "    model=tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=SHAPE))\n",
    "    model.add(tf.keras.layers.Masking(input_shape=SHAPE, mask_value=-1.))\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=HIDDEN_UNITS, return_sequences=True)))\n",
    "    model.add(tf.keras.layers.Dense(units = numtags, activation='softmax'))\n",
    "    model.compile('adam',loss=tf.keras.losses.SparseCategoricalCrossentropy())\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "FFN = get_BERT_FFN(train_x.shape[1:], num_tags)\n",
    "\n",
    "CRF_model = get_BERT_BILSTM_CRF(train_x.shape[1:], num_tags)\n",
    "\n",
    "Softmax_model = get_BERT_BILSTM_SOFTMAX(train_x.shape[1:], num_tags)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "FFN.fit(train_x, train_y, batch_size=128, epochs=1000, validation_split=0.1, verbose=1, callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "CRF_model.fit(train_x, train_y, batch_size=128, epochs=1000, validation_split=0.1, verbose=1, callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Softmax_model.fit(train_x, train_y, batch_size=128, epochs=1000, validation_split=0.1, verbose=1, callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_MODELS = [FFN_CRF, CRF_model, Softmax_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_MODELS = [FFN_CRF, CRF_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_MODELS = [Softmax_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(type=None, scaled_page='normal'):\n",
    "    if type is None:\n",
    "        print(\"Please assign type of test_data\")\n",
    "        return\n",
    "    if type != 'EVENT_SOURCE':\n",
    "        storage.test_file = 'NORMAL'\n",
    "        test_urls = [rec['Page URL'] for rec in storage.iter_test_records()]\n",
    "        test_X_one, test_y_one, test_page_positions_one = storage.get_test_Xy(validate=False, scaled_page=scaled_page, contain_position = True)\n",
    "        print(\"pages: {}  domains: {}\".format(len(test_urls), len({get_domain(url) for url in test_urls})))\n",
    "        if type == 'NORMAL':\n",
    "            return test_X_one, test_y_one, test_page_positions_one\n",
    "    if type != 'NORMAL':\n",
    "        storage.test_file = 'EVENT_SOURCE'\n",
    "        test_urls = [rec['Page URL'] for rec in storage.iter_test_records()]\n",
    "        test_X_two, test_y_two, test_page_positions_two = storage.get_test_Xy(validate=False, scaled_page=scaled_page, contain_position = True)\n",
    "        print(\"pages: {}  domains: {}\".format(len(test_urls), len({get_domain(url) for url in test_urls})))\n",
    "        if type == 'EVENT_SOURCE':\n",
    "            return test_X_two, test_y_two, test_page_positions_two\n",
    "    test_X_raw = test_X_one + test_X_two\n",
    "    test_y = test_y_one + test_y_two\n",
    "    test_positions = test_page_positions_one + test_page_positions_two\n",
    "    return test_X_raw, test_y, test_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_distribution_to_label(predict_y):\n",
    "    if len(predict_y.shape) != 3:\n",
    "        return predict_y\n",
    "    label_y = list()\n",
    "    for page in predict_y:\n",
    "        tmp = list()\n",
    "        for lab in page:\n",
    "            lab = lab.tolist()\n",
    "            tmp.append(lab.index(max(lab)))\n",
    "        label_y.append(tmp)\n",
    "    return label_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_predict_and_evaluate(models, x_test, y_test, evaluate_labels):\n",
    "    for idx, model in enumerate(models):\n",
    "        print(f\"Start predict model {idx}\")\n",
    "#         print(model.summary())\n",
    "        print(\"--------------------------\")\n",
    "        predict_y = model.predict(x_test)\n",
    "        predict_y = label_distribution_to_label(predict_y)\n",
    "        predict_y = np.asarray([[idx2tag.get(lab) for lab in page] for page in predict_y])\n",
    "        print(flat_classification_report(y_test, predict_y, labels=evaluate_labels, digits=3))\n",
    "        print(\"--------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contain position: True\n",
      "pages: 100  domains: 58\n"
     ]
    }
   ],
   "source": [
    "# test_X_raw, test_y, test_page_positions = get_test_data('EVENT_SOURCE')\n",
    "test_X_raw, test_y, test_page_positions = get_test_data('NORMAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_data_wo_position(x, y):\n",
    "    new_tmp_x_array = []\n",
    "    new_tmp_y_array = []\n",
    "    for tmp_x, tmp_y in zip(x, y):\n",
    "        new_tmp_x_array.extend(chunks(tmp_x, max_page_seq))\n",
    "        new_tmp_y_array.extend(chunks(tmp_y, max_page_seq))\n",
    "    return new_tmp_x_array, new_tmp_y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_testing(test_X_raw, test_y_raw): #ft-bert\n",
    "    chunks_test_x, chunks_test_y = get_chunks_data_wo_position(test_X_raw, test_y_raw)\n",
    "    test_token_features, test_tag_features = get_token_tag_features_from_chunks(chunks_test_x)\n",
    "    _, _, test_full_text_emb = page_to_two_bert_embeddings(test_token_features, pbert.get_tokenizer())\n",
    "    test_tag_info_list = test_tag_features\n",
    "    test_ft_emb = pages_to_word_vector(ft, test_token_features)\n",
    "    ## X_test_input\n",
    "    test_text_emb_x = feature_pad_to_npdata(test_full_text_emb)\n",
    "    test_ft_emb_x = feature_pad_to_npdata(test_ft_emb)\n",
    "    test_tag_x = feature_pad_to_npdata(test_tag_info_list)\n",
    "    test_composite_input = [test_text_emb_x, test_ft_emb_x, test_tag_x]\n",
    "    \n",
    "    ## y_test_input\n",
    "    y_test = pad_sequences([[tag2idx.get(l) for l in lab] for lab in chunks_test_y], maxlen=max_page_seq, padding=\"post\", truncating=\"post\", value=tag2idx[\"[PAD]\"])\n",
    "    y_test = [[idx2tag.get(lab) for lab in page] for page in y_test]\n",
    "    y_test = np.asarray(y_test)\n",
    "    \n",
    "    return test_composite_input, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_urls = [rec['Page URL'] for rec in storage.iter_test_records()]\n",
    "test_groups = set([get_domain(url) for url in test_urls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_test_x, chunks_test_y, chunks_test_positions = get_chunks_data(test_X_raw, test_y, test_page_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token_features, test_tag_features = get_token_tag_features_from_chunks(chunks_test_x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_tag_feature_tokens = extract_tokens_from_token_features(test_token_features, pbert.get_tokenizer())\n",
    "test_tag_feature_bert_embedding = bert_long_model.page_list_to_bert_embedding_list(test_tag_feature_tokens,Token=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "test_first_emb, test_second_emb, test_full_text_emb = page_to_two_bert_embeddings(test_token_features, pbert.get_tokenizer())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.save('embedding/test/first.npy', test_first_emb)\n",
    "np.save('embedding/test/second.npy', test_second_emb)\n",
    "np.save('embedding/test/full_text.npy', test_full_text_emb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_first_emb = np.load('embedding/test/first.npy', allow_pickle=True)\n",
    "test_second_emb = np.load('embedding/test/second.npy', allow_pickle=True)\n",
    "test_full_text_emb = np.load('embedding/test/full_text.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(f\"First emb:{test_first_emb[0].shape}\")\n",
    "print(f\"Second emb:{test_second_emb[0].shape}\")\n",
    "print(f\"Full_text emb:{test_full_text_emb[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tag_info_list = test_tag_features"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "max_node = -1\n",
    "page_sum = 0\n",
    "for page in test_tag_tokens:\n",
    "    sum = 0\n",
    "    for node in page:\n",
    "        sum+=len(node)\n",
    "        if len(node) > max_node:\n",
    "            max_node = len(node)\n",
    "    page_sum+=sum/len(page)\n",
    "print(\"Average: \", page_sum/len(test_tag_tokens))\n",
    "print(\"Max_node: \",max_node)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_pages_class, test_pages_query, test_pages_parent_tag = prepare_input_tokens(test_token_features, pbert.get_tokenizer(), max_len)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_class = token_pad_to_npdata(test_pages_class)\n",
    "test_query = token_pad_to_npdata(test_pages_query)\n",
    "test_parent_tag = token_pad_to_npdata(test_pages_parent_tag)\n",
    "print(\"Current Shape:\")\n",
    "print(f\"test_class: {test_class.shape}\")\n",
    "print(f\"test_query: {test_query.shape}\")\n",
    "print(f\"test_parent_tag: {test_parent_tag.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_emb_x = feature_pad_to_npdata(test_full_text_emb) # full text emb / two-bert emb\n",
    "# test_text_emb_x = feature_pad_to_npdata(test_second_emb)\n",
    "# test_text_emb_x = feature_pad_to_npdata(test_text_emb) # text emb"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_tag_emb = feature_pad_to_npdata(test_tag_feature_bert_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tag_x = feature_pad_to_npdata(test_tag_info_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_positions_x = feature_pad_to_npdata(chunks_test_positions)\n",
    "test_tag_x = np.concatenate([test_tag_x, test_positions_x], axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_info_x = np.concatenate([test_text_emb_x, test_tag_x], axis = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input for full ft_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_testing_full_ft(test_X_raw, test_y_raw): #ft-full\n",
    "    chunks_test_x, chunks_test_y = get_chunks_data_wo_position(test_X_raw, test_y_raw)\n",
    "    test_token_features, test_tag_features = get_token_tag_features_from_chunks(chunks_test_x)\n",
    "    \n",
    "    test_tag_info_list = test_tag_features\n",
    "    test_ft_full_emb = pages_to_word_vector_from_keylist(ft, test_token_features)\n",
    "    ## X_test_input\n",
    "    test_ft_emb_x = feature_pad_to_npdata(test_ft_full_emb)\n",
    "    test_tag_x = feature_pad_to_npdata(test_tag_info_list)\n",
    "    test_composite_input = [test_ft_emb_x, test_tag_x]\n",
    "    \n",
    "    ## y_test_input\n",
    "    y_test = pad_sequences([[tag2idx.get(l) for l in lab] for lab in chunks_test_y], maxlen=max_page_seq, padding=\"post\", truncating=\"post\", value=tag2idx[\"[PAD]\"])\n",
    "    y_test = [[idx2tag.get(lab) for lab in page] for page in y_test]\n",
    "    y_test = np.asarray(y_test)\n",
    "    \n",
    "    return test_composite_input, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform key ['text-before', 'text-exact', 'text-after', 'class', 'query', 'parent-tag'] to word_vector ... \n"
     ]
    }
   ],
   "source": [
    "ft_full_emb = pages_to_word_vector_from_keylist(ft, test_token_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full_text_x = feature_pad_to_npdata(ft_full_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tag_x = feature_pad_to_npdata(test_tag_info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_composite_input = [test_full_text_x, test_tag_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68, 512, 600)\n",
      "(68, 512, 8)\n"
     ]
    }
   ],
   "source": [
    "for inputs in test_composite_input:\n",
    "    print(inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready to testing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_composite_input = [test_text_emb_x, test_tag_emb, test_tag_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_composite_with_token = [test_text_emb_x, test_tag_x, test_class, test_query, test_parent_tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pad_sequences([[tag2idx.get(l) for l in lab] for lab in chunks_test_y], maxlen=max_page_seq, padding=\"post\", truncating=\"post\", value=tag2idx[\"[PAD]\"])\n",
    "y_test = [[idx2tag.get(lab) for lab in page] for page in y_test]\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_labels = ['PREV', 'PAGE', 'NEXT', '[PAD]', 'O']\n",
    "evaluate_labels = ['PAGE', 'NEXT']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "recursive_predict_and_evaluate([FFN], x_test, y_test, evaluate_labels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "recursive_predict_and_evaluate([model], test_composite_input, y_test, evaluate_labels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def test_model(iter_train_x, iter_train_y, iter_test_x, iter_test_y, iters = 5):\n",
    "    for it in range(iters):\n",
    "        print(f\"iteration: {it}\")\n",
    "        model = get_bert_embeddings_bilstm_softmax()\n",
    "        model.fit(iter_train_x, iter_train_y, batch_size=32, epochs=1000, validation_split=0.1, verbose=False, callbacks=[earlyStopping])\n",
    "        recursive_predict_and_evaluate([model], iter_test_x, iter_test_y, ['PAGE','NEXT'])\n",
    "test_model(train_composite_input, train_y, test_composite_input, y_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "recursive_predict_and_evaluate([model], test_composite_token_input, y_test, evaluate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n",
      "Start predict model 0\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/DL/lib/python3.6/site-packages/sklearn/utils/validation.py:71: FutureWarning: Pass labels=['PAGE', 'NEXT'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PAGE      0.665     0.860     0.750       279\n",
      "        NEXT      0.786     0.449     0.571        49\n",
      "\n",
      "   micro avg      0.674     0.799     0.731       328\n",
      "   macro avg      0.725     0.655     0.661       328\n",
      "weighted avg      0.683     0.799     0.723       328\n",
      "\n",
      "--------------------------\n",
      "iteration: 1\n",
      "Start predict model 0\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/DL/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PAGE      0.624     0.839     0.716       279\n",
      "        NEXT      0.000     0.000     0.000        49\n",
      "\n",
      "   micro avg      0.624     0.713     0.666       328\n",
      "   macro avg      0.312     0.419     0.358       328\n",
      "weighted avg      0.531     0.713     0.609       328\n",
      "\n",
      "--------------------------\n",
      "iteration: 2\n",
      "Start predict model 0\n",
      "--------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PAGE      0.682     0.878     0.768       279\n",
      "        NEXT      0.000     0.000     0.000        49\n",
      "\n",
      "   micro avg      0.682     0.747     0.713       328\n",
      "   macro avg      0.341     0.439     0.384       328\n",
      "weighted avg      0.580     0.747     0.653       328\n",
      "\n",
      "--------------------------\n",
      "iteration: 3\n",
      "Start predict model 0\n",
      "--------------------------\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": " Dst tensor is not initialized.\n\t [[{{node model_6/embedding_14/embedding_lookup/_30}}]] [Op:__inference_predict_function_84639]\n\nFunction call stack:\npredict_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-dd27344c22b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_train_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_train_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearlyStopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mrecursive_predict_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_test_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_test_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'PAGE'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'NEXT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtest_custom_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_composite_with_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_composite_with_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-70-dd27344c22b9>\u001b[0m in \u001b[0;36mtest_custom_model\u001b[0;34m(iter_train_x, iter_train_y, iter_test_x, iter_test_y, iters)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_custom_emb_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_crf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_train_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_train_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearlyStopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mrecursive_predict_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_test_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_test_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'PAGE'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'NEXT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtest_custom_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_composite_with_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_composite_with_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-75908b9168e4>\u001b[0m in \u001b[0;36mrecursive_predict_and_evaluate\u001b[0;34m(models, x_test, y_test, evaluate_labels)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#         print(model.summary())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mpredict_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mpredict_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_distribution_to_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mpredict_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx2tag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlab\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredict_y\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DL/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m~/DL/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1266\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m             \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m             \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DL/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DL/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    648\u001b[0m               *args, **kwds)\n\u001b[1;32m    649\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_stateful_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DL/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DL/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DL/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/DL/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m:  Dst tensor is not initialized.\n\t [[{{node model_6/embedding_14/embedding_lookup/_30}}]] [Op:__inference_predict_function_84639]\n\nFunction call stack:\npredict_function\n"
     ]
    }
   ],
   "source": [
    "def test_custom_model(iter_train_x, iter_train_y, iter_test_x, iter_test_y, iters = 10):\n",
    "    for it in range(iters):\n",
    "        print(f\"iteration: {it}\")\n",
    "        model = get_custom_emb_model(use_crf=False)\n",
    "        model.fit(iter_train_x, iter_train_y, batch_size=8, epochs=1000, validation_split=0.1, verbose=False, callbacks=[earlyStopping])\n",
    "        recursive_predict_and_evaluate([model], iter_test_x, iter_test_y, ['PAGE','NEXT'])\n",
    "test_custom_model(train_composite_with_token, train_y, test_composite_with_token, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_languages = storage.get_all_test_languages()\n",
    "# test_languages = [\"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing language:  en\n",
      "pages: 49  domains: 34\n",
      "Start encode first seg embeddings\n",
      "Use custom Token: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d565a661ebe44a8bcf992f20ef8a033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=68)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start encode second seg embeddings\n",
      "Use custom Token: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3414b891d304cef8538d49c77e71c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=68)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predict model 0\n",
      "--------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PAGE      0.801     0.865     0.832       126\n",
      "        NEXT      0.000     0.000     0.000        29\n",
      "\n",
      "   micro avg      0.801     0.703     0.749       155\n",
      "   macro avg      0.401     0.433     0.416       155\n",
      "weighted avg      0.652     0.703     0.676       155\n",
      "\n",
      "--------------------------\n",
      "===================================\n",
      "Testing language:  zh\n",
      "pages: 44  domains: 19\n",
      "Start encode first seg embeddings\n",
      "Use custom Token: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ab4d425ad341458caa478b00ccba76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=48)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start encode second seg embeddings\n",
      "Use custom Token: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdffae311ea7449f8cf6e041c36328c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=48)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predict model 0\n",
      "--------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PAGE      0.714     0.650     0.681       277\n",
      "        NEXT      0.000     0.000     0.000        24\n",
      "\n",
      "   micro avg      0.714     0.598     0.651       301\n",
      "   macro avg      0.357     0.325     0.340       301\n",
      "weighted avg      0.657     0.598     0.626       301\n",
      "\n",
      "--------------------------\n",
      "===================================\n",
      "Testing language:  ko\n",
      "pages: 24  domains: 13\n",
      "Start encode first seg embeddings\n",
      "Use custom Token: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2321cc9e01e487184abc06ae632f93e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=36)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start encode second seg embeddings\n",
      "Use custom Token: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a02c685e1cc4b949c76df115b5eea3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=36)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predict model 0\n",
      "--------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PAGE      0.644     0.933     0.762        60\n",
      "        NEXT      0.000     0.000     0.000         5\n",
      "\n",
      "   micro avg      0.644     0.862     0.737        65\n",
      "   macro avg      0.322     0.467     0.381        65\n",
      "weighted avg      0.594     0.862     0.703        65\n",
      "\n",
      "--------------------------\n",
      "===================================\n",
      "Testing language:  ja\n",
      "pages: 23  domains: 9\n",
      "Start encode first seg embeddings\n",
      "Use custom Token: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e930fa3c6c4583b378718d18371b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=23)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start encode second seg embeddings\n",
      "Use custom Token: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df6358c79964476a5cf2b40f7a414e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=23)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predict model 0\n",
      "--------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PAGE      0.644     0.592     0.617        49\n",
      "        NEXT      0.000     0.000     0.000        11\n",
      "\n",
      "   micro avg      0.644     0.483     0.552        60\n",
      "   macro avg      0.322     0.296     0.309        60\n",
      "weighted avg      0.526     0.483     0.504        60\n",
      "\n",
      "--------------------------\n",
      "===================================\n",
      "Testing language:  de\n",
      "pages: 20  domains: 7\n",
      "Start encode first seg embeddings\n",
      "Use custom Token: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e66733079f4e77967e8239546de952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=25)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start encode second seg embeddings\n",
      "Use custom Token: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c27775f567f41bba7a75e1df3c6fec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=25)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predict model 0\n",
      "--------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PAGE      0.892     0.770     0.826       139\n",
      "        NEXT      0.000     0.000     0.000        22\n",
      "\n",
      "   micro avg      0.892     0.665     0.762       161\n",
      "   macro avg      0.446     0.385     0.413       161\n",
      "weighted avg      0.770     0.665     0.713       161\n",
      "\n",
      "--------------------------\n",
      "===================================\n",
      "Testing language:  ru\n",
      "pages: 21  domains: 14\n",
      "Start encode first seg embeddings\n",
      "Use custom Token: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca67feec7454b81b9a9e1c18d87cb85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=20)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start encode second seg embeddings\n",
      "Use custom Token: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101b7366062c41a59447edfe8c2fc8bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=20)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predict model 0\n",
      "--------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PAGE      0.692     0.730     0.711        37\n",
      "        NEXT      0.000     0.000     0.000         7\n",
      "\n",
      "   micro avg      0.692     0.614     0.651        44\n",
      "   macro avg      0.346     0.365     0.355        44\n",
      "weighted avg      0.582     0.614     0.597        44\n",
      "\n",
      "--------------------------\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "TEST_MODELS = [model]\n",
    "test_languages = storage.get_all_test_languages()\n",
    "for language in test_languages:\n",
    "    print(\"Testing language: \", language)\n",
    "    test_urls = [rec['Page URL'] for rec in storage.iter_test_records_by_language(language=language)]\n",
    "    test_X_raw, test_y = storage.get_test_Xy_by_language(language=language)\n",
    "    print(\"pages: {}  domains: {}\".format(len(test_urls), len({get_domain(url) for url in test_urls})))\n",
    "    _test_x, _test_y = prepare_for_testing(test_X_raw, test_y)\n",
    "    recursive_predict_and_evaluate(TEST_MODELS, _test_x, _test_y, ['PAGE','NEXT'])\n",
    "    print(\"===================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing language:  en\n",
      "pages: 49  domains: 34\n",
      "Transform key ['text-before', 'text-exact', 'text-after', 'class', 'query', 'parent-tag'] to word_vector ... \n",
      "Start predict model 0\n",
      "--------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PAGE      0.843     0.556     0.670       126\n",
      "        NEXT      0.000     0.000     0.000        29\n",
      "\n",
      "   micro avg      0.843     0.452     0.588       155\n",
      "   macro avg      0.422     0.278     0.335       155\n",
      "weighted avg      0.686     0.452     0.545       155\n",
      "\n",
      "--------------------------\n",
      "===================================\n",
      "Testing language:  zh\n",
      "pages: 44  domains: 19\n",
      "Transform key ['text-before', 'text-exact', 'text-after', 'class', 'query', 'parent-tag'] to word_vector ... \n",
      "Start predict model 0\n",
      "--------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PAGE      0.783     0.534     0.635       277\n",
      "        NEXT      0.000     0.000     0.000        24\n",
      "\n",
      "   micro avg      0.783     0.492     0.604       301\n",
      "   macro avg      0.392     0.267     0.318       301\n",
      "weighted avg      0.721     0.492     0.585       301\n",
      "\n",
      "--------------------------\n",
      "===================================\n",
      "Testing language:  ko\n",
      "pages: 24  domains: 13\n",
      "Transform key ['text-before', 'text-exact', 'text-after', 'class', 'query', 'parent-tag'] to word_vector ... \n",
      "Start predict model 0\n",
      "--------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PAGE      0.679     0.917     0.780        60\n",
      "        NEXT      0.000     0.000     0.000         5\n",
      "\n",
      "   micro avg      0.679     0.846     0.753        65\n",
      "   macro avg      0.340     0.458     0.390        65\n",
      "weighted avg      0.627     0.846     0.720        65\n",
      "\n",
      "--------------------------\n",
      "===================================\n",
      "Testing language:  ja\n",
      "pages: 23  domains: 9\n",
      "Transform key ['text-before', 'text-exact', 'text-after', 'class', 'query', 'parent-tag'] to word_vector ... \n",
      "Start predict model 0\n",
      "--------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PAGE      0.465     0.408     0.435        49\n",
      "        NEXT      0.000     0.000     0.000        11\n",
      "\n",
      "   micro avg      0.465     0.333     0.388        60\n",
      "   macro avg      0.233     0.204     0.217        60\n",
      "weighted avg      0.380     0.333     0.355        60\n",
      "\n",
      "--------------------------\n",
      "===================================\n",
      "Testing language:  de\n",
      "pages: 20  domains: 7\n",
      "Transform key ['text-before', 'text-exact', 'text-after', 'class', 'query', 'parent-tag'] to word_vector ... \n",
      "Start predict model 0\n",
      "--------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PAGE      0.764     0.489     0.596       139\n",
      "        NEXT      0.000     0.000     0.000        22\n",
      "\n",
      "   micro avg      0.764     0.422     0.544       161\n",
      "   macro avg      0.382     0.245     0.298       161\n",
      "weighted avg      0.660     0.422     0.515       161\n",
      "\n",
      "--------------------------\n",
      "===================================\n",
      "Testing language:  ru\n",
      "pages: 21  domains: 14\n",
      "Transform key ['text-before', 'text-exact', 'text-after', 'class', 'query', 'parent-tag'] to word_vector ... \n",
      "Start predict model 0\n",
      "--------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PAGE      0.704     0.514     0.594        37\n",
      "        NEXT      0.000     0.000     0.000         7\n",
      "\n",
      "   micro avg      0.704     0.432     0.535        44\n",
      "   macro avg      0.352     0.257     0.297        44\n",
      "weighted avg      0.592     0.432     0.499        44\n",
      "\n",
      "--------------------------\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "test_languages = storage.get_all_test_languages()\n",
    "#Test for full ft emb\n",
    "TEST_MODELS = [model]\n",
    "for language in test_languages:\n",
    "    print(\"Testing language: \", language)\n",
    "    test_urls = [rec['Page URL'] for rec in storage.iter_test_records_by_language(language=language)]\n",
    "    test_X_raw, test_y = storage.get_test_Xy_by_language(language=language)\n",
    "    print(\"pages: {}  domains: {}\".format(len(test_urls), len({get_domain(url) for url in test_urls})))\n",
    "    _test_x, _test_y = prepare_for_testing_full_ft(test_X_raw, test_y)\n",
    "    recursive_predict_and_evaluate(TEST_MODELS, _test_x, _test_y, ['PAGE','NEXT'])\n",
    "    print(\"===================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
