{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import bert\n",
    "from bert import tokenization\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.data import Dataset\n",
    "import tensorflow as tf\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if len(gpus)!=0:\n",
    "#   # Restrict TensorFlow to only use the first GPU\n",
    "#     try:\n",
    "#         tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "#         for gpu in gpus:\n",
    "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#         logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "#         print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "#     except RuntimeError as e:\n",
    "#     # Visible devices must be set before GPUs have been initialized\n",
    "#         print(e)\n",
    "# else:\n",
    "#     print(\"No GPUs visible\")\n",
    "\n",
    "class BertModel:\n",
    "    def __init__(self, max_seq_length = None):\n",
    "        if max_seq_length == None:\n",
    "            print(\"Need to assign max_seq_length\")\n",
    "            return\n",
    "        else:\n",
    "            self.max_seq_length = max_seq_length  \n",
    "        self.get_multilingual_bert()\n",
    "        \n",
    "    def get_multilingual_bert(self):\n",
    "        max_seq_length = self.max_seq_length\n",
    "        input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                               name=\"input_word_ids\")\n",
    "        input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                           name=\"input_mask\")\n",
    "        segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                            name=\"segment_ids\")\n",
    "        bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/1\",\n",
    "                                    trainable=False)\n",
    "        vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "        do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "        self.tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "        pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "        self.model = tf.keras.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output, sequence_output])\n",
    "        \n",
    "    def get_tokenizer(self):\n",
    "        return self.tokenizer\n",
    "    \n",
    "    def get_bert_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def get_masks(self, tokens):\n",
    "        \"\"\"Mask for padding\"\"\"\n",
    "        if len(tokens) > self.max_seq_length:\n",
    "            raise IndexError(f\"Token length more than max seq length! {len(tokens)} > {self.max_seq_length}\")\n",
    "        return [1]*len(tokens) + [0] * (self.max_seq_length - len(tokens))\n",
    "\n",
    "    def get_segments(self, tokens):\n",
    "        \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "        if len(tokens) > self.max_seq_length:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        segments = []\n",
    "        current_segment_id = 0\n",
    "        for token in tokens:\n",
    "            segments.append(current_segment_id)\n",
    "            if token == \"[SEP]\":\n",
    "                current_segment_id += 1\n",
    "        return segments + [0] * (self.max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "    def get_ids(self, tokens):\n",
    "        \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_ids = token_ids + [0] * (self.max_seq_length-len(token_ids))\n",
    "        return input_ids\n",
    "\n",
    "    def get_bert_inputs_from_sequences(self, seqs, Token):\n",
    "        if type(seqs) != type([]):\n",
    "            print(\"seqs must be list of seq\")\n",
    "            return\n",
    "        if Token is False:\n",
    "            tokens_list = [self.tokenizer.tokenize(seq) for seq in seqs]\n",
    "        else:\n",
    "            tokens_list = seqs\n",
    "        ids = [ np.array(self.get_ids(tokens)) for tokens in tokens_list ]\n",
    "        masks = [ np.array(self.get_masks(tokens))  for tokens in tokens_list ]\n",
    "        segments = [ np.array(self.get_segments(tokens))  for tokens in tokens_list ]\n",
    "        return np.array(ids), np.array(masks), np.array(segments)\n",
    "    def page_list_to_bert_embedding_list(self, page_list, Token = None):\n",
    "        '''\n",
    "        Args:\n",
    "            page_list - Input pages\n",
    "            model - pre-trained emb model\n",
    "            tokenizer - text tokenizer\n",
    "            max_seq_length - max seq length per node\n",
    "            Token - Input is Tokenized list or not (raw input data)\n",
    "        '''\n",
    "        if Token is None:\n",
    "            print(\"Please assign Token argument\")\n",
    "            return\n",
    "        print(f\"Use custom Token: {Token}\")\n",
    "        p = IntProgress(max=len(page_list))\n",
    "        p.description = '(Init)'\n",
    "        p.value = 0\n",
    "        display(p)\n",
    "        seq_list = []\n",
    "        for idx, page in enumerate(page_list):\n",
    "            p.description = f\"Task: {idx+1}\"\n",
    "            p.value = idx+1\n",
    "            page_idx, page_mask, page_seg = self.get_bert_inputs_from_sequences(page, Token)\n",
    "            pooled_emb, _ = self.model.predict([ page_idx, page_mask, page_seg ])\n",
    "            seq_list.append(pooled_emb)\n",
    "        p.description = '(Done)'\n",
    "        return seq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use custom Token: False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2a195fc0724f09ac4ea27583a7f277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test emb shape: (3, 768)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    bertModel = BertModel(128)\n",
    "    test_pages = [[\"hello world\",\"hello world\",\"hello world\"],[\"hello world\",\"hello world\"]]\n",
    "    test_emb = bertModel.page_list_to_bert_embedding_list(test_pages, False)\n",
    "    print(f\"test emb shape: {test_emb[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
