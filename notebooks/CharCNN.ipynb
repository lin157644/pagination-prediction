{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\"\n",
    "import re\n",
    "import sys\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "from urllib.parse import urlparse, urlsplit, parse_qs, parse_qsl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import parsel\n",
    "from sklearn_crfsuite.metrics import flat_classification_report, sequence_accuracy_score\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "from autopager.storage import Storage\n",
    "from autopager.htmlutils import (get_link_text, get_text_around_selector_list,\n",
    "                                 get_link_href, get_selector_root)\n",
    "from autopager.utils import (\n",
    "    get_domain, normalize_whitespaces, normalize, ngrams, tokenize, ngrams_wb, replace_digits\n",
    ")\n",
    "from autopager.model import _num_tokens_feature, _elem_attr\n",
    "from autopager import AUTOPAGER_LIMITS\n",
    "from autopager.parserutils import (TagParser, MyHTMLParser, draw_scaled_page, position_check, compare_tag, get_first_tag)\n",
    "parser = MyHTMLParser()\n",
    "tagParser = TagParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import random\n",
    "tf.random.set_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_addons.layers.crf import CRF\n",
    "from tensorflow.keras.layers import (Dense, Input, Bidirectional, LSTM, Embedding, Masking, Concatenate,\n",
    "                                    AveragePooling2D, MaxPooling2D, Reshape, Attention, GlobalAveragePooling1D\n",
    "                                    , Activation, Conv1D, Conv2D, Flatten, Dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.data import Dataset\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "commands = sys.argv\n",
    "if len(commands) < 2:\n",
    "    print(\"python CharCNN.py MODE: Normal|Test \")\n",
    "    sys.exit(0)\n",
    "if commands[1].lower() == 'normal':\n",
    "    train_epoch = 25\n",
    "    target = 'all'\n",
    "elif commands[1].lower() == 'test':\n",
    "    train_epoch = 1\n",
    "    target = 'ko'\n",
    "else:\n",
    "    print(\"Mode only contains: Normal | Test\")\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mode: \",commands[1].lower())\n",
    "print(\"Train_epoch: \",train_epoch)\n",
    "print(\"Test_target: \",target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting GPU ... \")\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpus:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "print(\"Setting GPU Finish.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_empty(x, y):\n",
    "    res_x = [page for page in x if len(x)!= 0]\n",
    "    res_y = [page for page in y if len(y)!= 0]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = Storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_page_seq = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [rec['Page URL'] for rec in storage.iter_records(language='en',contain_button = True, file_type='T')]\n",
    "X_raw, y, page_positions = storage.get_Xy(language='en',contain_button = True,  contain_position=True,file_type='T', scaled_page='normal')\n",
    "print(\"pages: {}  domains: {}\".format(len(urls), len({get_domain(url) for url in urls})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_x, chunks_y, chunk_positions = X_raw, y, page_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_x, chunks_y = filter_empty(chunks_x, chunks_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LaserSentenceModel import LaserSentenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "laser = LaserSentenceModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseAttribute(html):\n",
    "    close_index = html.find('>')\n",
    "    open_text = html[:close_index]\n",
    "    open_text = open_text.replace('<a ','')\n",
    "    open_text = open_text.replace('<button','')\n",
    "    return normalize(open_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\n",
      "Wall time: 9.3 µs\n"
     ]
    }
   ],
   "source": [
    "def _as_list(generator, limit=None):\n",
    "    \"\"\"\n",
    "    >>> _as_list(ngrams_wb(\"text\", 2, 2), 0)\n",
    "    []\n",
    "    >>> _as_list(ngrams_wb(\"text\", 2, 2), 2)\n",
    "    ['te', 'ex']\n",
    "    >>> _as_list(ngrams_wb(\"text\", 2, 2))\n",
    "    ['te', 'ex', 'xt']\n",
    "    \"\"\"\n",
    "    return list(generator if limit is None else islice(generator, 0, limit))\n",
    "\n",
    "def feat_to_tokens(feat, tokenizer):\n",
    "    if type(feat) == type([]):\n",
    "        feat = ' '.join(feat)\n",
    "    tokens = tokenizer.tokenize(feat)\n",
    "    return tokens\n",
    "\n",
    "def num_token_feature_to_class(number):\n",
    "    if number == '=0':\n",
    "        return [1, 0, 0, 0]\n",
    "    elif number == '=1':\n",
    "        return [0, 1, 0, 0]\n",
    "    elif number == '=2':\n",
    "        return [0, 0, 1, 0]\n",
    "    else:\n",
    "        return [0, 0, 0, 1]\n",
    "\n",
    "def link_to_features(link):\n",
    "    text = normalize(get_link_text(link))\n",
    "    href = get_link_href(link)\n",
    "    if href is None:\n",
    "        href = \"\"\n",
    "    p = urlsplit(href)\n",
    "    parent = link.xpath('..').extract()\n",
    "    parent = get_first_tag(parser, parent[0])\n",
    "    query_parsed = parse_qsl(p.query) #parse query string from path\n",
    "    query_param_names = [k.lower() for k, v in query_parsed]\n",
    "    query_param_names_ngrams = _as_list(ngrams_wb(\n",
    "        \" \".join([normalize(name) for name in query_param_names]), 3, 5, True\n",
    "    ))\n",
    "    attribute_text = parseAttribute(link.extract())\n",
    "    # Classes of link itself and all its children.\n",
    "    # It is common to have e.g. span elements with fontawesome\n",
    "    # arrow icon classes inside <a> links.\n",
    "    self_and_children_classes = ' '.join(link.xpath(\".//@class\").extract())\n",
    "    parent_classes = ' '.join(link.xpath('../@class').extract())\n",
    "    css_classes = normalize(parent_classes + ' ' + self_and_children_classes)\n",
    "    \n",
    "    token_feature = {\n",
    "        'text-exact': replace_digits(text.strip()[:100].strip()),\n",
    "#         'query': query_param_names,\n",
    "        'query': query_param_names_ngrams,\n",
    "        'parent-tag': parent,\n",
    "#         'class': css_classes.split()[:AUTOPAGER_LIMITS.max_css_features],\n",
    "        'class':_as_list(ngrams_wb(css_classes, 4, 5),\n",
    "                          AUTOPAGER_LIMITS.max_css_features),\n",
    "        'text': _as_list(ngrams_wb(replace_digits(text), 2, 5),\n",
    "                         AUTOPAGER_LIMITS.max_text_features),\n",
    "        'attribute_text': attribute_text,\n",
    "    }\n",
    "    tag_feature = {\n",
    "        'isdigit': 1 if text.isdigit() is True else 0,\n",
    "        'isalpha': 1 if text.isalpha() is True else 0,\n",
    "        'has-href': 0 if href is \"\" else 1,\n",
    "        'path-has-page': 1 if 'page' in p.path.lower() else 0,\n",
    "        'path-has-pageXX': 1 if re.search(r'[/-](?:p|page\\w?)/?\\d+', p.path.lower()) is not None else 0,\n",
    "        'path-has-number': 1 if any(part.isdigit() for part in p.path.split('/')) else 0,\n",
    "        'href-has-year': 1 if re.search('20\\d\\d', href) is not None else 0,\n",
    "        'class-has-disabled': 1 if 'disabled' in css_classes else 0,\n",
    "#         'num-tokens': num_token_feature_to_class(_num_tokens_feature(text)),\n",
    "    }\n",
    "    non_token_feature = []\n",
    "    for k,v in tag_feature.items():\n",
    "        if type(v) == type([]):\n",
    "            non_token_feature.extend(v)\n",
    "        else:\n",
    "            non_token_feature.append(v)\n",
    "    return [token_feature, non_token_feature]\n",
    "\n",
    "\n",
    "def page_to_features(xseq):\n",
    "    feat_list = [link_to_features(a) for a in xseq]\n",
    "    around = get_text_around_selector_list(xseq, max_length=15)\n",
    "    return feat_list\n",
    "\n",
    "def get_token_tag_features_from_chunks(chunks):\n",
    "    token_features = []\n",
    "    tag_features = []\n",
    "    for idx, page in enumerate(chunks):\n",
    "        try:\n",
    "            feat_list = page_to_features(page)\n",
    "            token_features.append([node[0] for node in feat_list])\n",
    "            tag_features.append(np.array([node[1] for node in feat_list]))\n",
    "        except:\n",
    "            raise Exception(f\"Error occured on {idx}\")\n",
    "    return token_features, tag_features\n",
    "\n",
    "def word_to_vector(word_list, word_vector_method = None):\n",
    "    if word_vector_method is None:\n",
    "        print(\"Need to specified a method.\")\n",
    "        return\n",
    "    elif word_vector_method == 'FastText':\n",
    "        if type(word_list) == type([]):\n",
    "            if len(word_list) == 0:\n",
    "                return np.zeros(ft.getModel().get_dimension())\n",
    "            else:\n",
    "                vectors_array = []\n",
    "                for word in word_list:\n",
    "                    vector = ft.getWordVector(word)\n",
    "                    vectors_array.append(vector)\n",
    "                mean_vector = np.mean(vectors_array, axis = 0)\n",
    "                return mean_vector\n",
    "        else:\n",
    "            return ft.getWordVector(word_list)\n",
    "    elif word_vector_method == 'Laser':\n",
    "        return laser.getSentenceVector(word_list)\n",
    "\n",
    "def pages_to_word_vector(ft, token_features):\n",
    "    pages_vector = []\n",
    "    for page in token_features:\n",
    "        page_vectors = []\n",
    "        for node in page:\n",
    "            classes = word_to_vector(ft, node['class'])\n",
    "            query = word_to_vector(ft, node['query'])\n",
    "            p_tag = word_to_vector(ft, node['parent-tag'])\n",
    "            full_vector = np.concatenate([classes, query, p_tag], axis = 0)\n",
    "            page_vectors.append(full_vector)\n",
    "        pages_vector.append(np.array(page_vectors))\n",
    "    return pages_vector\n",
    "    \n",
    "def list_to_dataSet(data, dataType):\n",
    "    dataset = Dataset.from_generator(lambda: iter(data), dataType)\n",
    "    return dataset\n",
    "\n",
    "def zip_dataSet(data):\n",
    "    data_tuple = tuple(data)\n",
    "    dataset = Dataset.zip(data_tuple)\n",
    "    return dataset\n",
    "\n",
    "def describe_dataset(dataset):\n",
    "    print(train_dataset.element_spec)\n",
    "    \n",
    "def composite_splite_to_train_val(composite_x, y, number):\n",
    "    x_train = [ data[:-number] for data in composite_x]\n",
    "    y_train = y[:-number]\n",
    "    x_val = [ data[-number:] for data in composite_x]\n",
    "    y_val = y[-number:]\n",
    "    return x_train, y_train, x_val, y_val\n",
    "\n",
    "def composite_cut_data(composite_x, y, percent):\n",
    "    number = round(len(y) * percent)\n",
    "    new_composite_x = [ data[:number] for data in composite_x]\n",
    "    new_y = y[:number]\n",
    "    return new_composite_x, new_y\n",
    "\n",
    "def data_list_to_dataset(x, y, isValidation = False, batch_size = 1):\n",
    "    all_data = None\n",
    "    for data in x:\n",
    "        dataset = list_to_dataSet(data, tf.float32)\n",
    "        if all_data == None:\n",
    "            all_data = dataset\n",
    "        else:\n",
    "            all_data = Dataset.zip((all_data, dataset))\n",
    "    y_ds = list_to_dataSet(y, tf.int32)\n",
    "    final_set = Dataset.zip((all_data, y_ds))\n",
    "    if not isValidation:\n",
    "        final_set = final_set.shuffle(buffer_size=1024).batch(batch_size)\n",
    "    else:\n",
    "        final_set = final_set.batch(batch_size)\n",
    "    return final_set\n",
    "\n",
    "def composite_list_to_dataset(x, batch_size = 1):\n",
    "    all_data = None\n",
    "    for data in x:\n",
    "        dataset = list_to_dataSet(data, tf.float32)\n",
    "        if all_data == None:\n",
    "            all_data = dataset\n",
    "        else:\n",
    "            all_data = Dataset.zip((all_data, dataset))\n",
    "    return all_data.batch(batch_size)\n",
    "\n",
    "\n",
    "def get_test_attr(tk_train, test_token_features):\n",
    "    test_attr_pages = [[node['attribute_text'].lower() for node in page] for page in test_token_features]\n",
    "    test_sequences = [tk_train.texts_to_sequences(page) for page in test_attr_pages]\n",
    "    test_attr_data = [pad_sequences(test_page, maxlen=256, padding='post') for test_page in test_sequences]\n",
    "    test_attr_data = [np.array(test_page, dtype='float32') for test_page in test_attr_data]\n",
    "    return test_attr_data\n",
    "\n",
    "def prepare_input_ids(page_tokens, max_len):\n",
    "    pages_class = []\n",
    "    pages_query = []\n",
    "    pages_text = []\n",
    "#     print(len(page_tokens))\n",
    "    for page in page_tokens:\n",
    "        class_page = []\n",
    "        query_page = []\n",
    "        text_page = []\n",
    "        for node in page:\n",
    "            #class\n",
    "            class_ids = class_tokenizer.tokenize(node['class'])\n",
    "            class_ids = class_ids + [0] * (max_len-len(class_ids))\n",
    "            class_page.append(class_ids[:max_len])\n",
    "            #query\n",
    "            query_ids = query_tokenizer.tokenize(node['query'])\n",
    "            query_ids = query_ids + [0] * (max_len-len(query_ids))\n",
    "            query_page.append(query_ids[:max_len])\n",
    "            #text\n",
    "            text_ids = text_tokenizer.tokenize(node['text'])\n",
    "            text_ids = text_ids + [0] * (max_len-len(text_ids))\n",
    "            text_page.append(text_ids[:max_len])\n",
    "        pages_class.append(np.array(class_page))\n",
    "        pages_query.append(np.array(query_page))\n",
    "        pages_text.append(np.array(text_page))\n",
    "    return pages_class, pages_query, pages_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_features, tag_features = get_token_tag_features_from_chunks(chunks_x)\n",
    "# train_tag_feature_token_list = extract_tokens_from_token_features(token_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_attr = []\n",
    "attr_pages = [[node['attribute_text'].lower() for node in page] for page in token_features]\n",
    "_ = [fit_attr.extend([node['attribute_text'].lower() for node in page]) for page in token_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "tk.fit_on_texts(fit_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------Skip part start--------------------------\n",
    "# construct a new vocabulary\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "char_dict = {}\n",
    "for i, char in enumerate(alphabet):\n",
    "    char_dict[char] = i + 1\n",
    "\n",
    "# Use char_dict to replace the tk.word_index\n",
    "tk.word_index = char_dict.copy()\n",
    "# Add 'UNK' to the vocabulary\n",
    "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
    "# -----------------------Skip part end----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string to index\n",
    "train_sequences = [tk.texts_to_sequences(page) for page in attr_pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attr_data = [pad_sequences(train_page, maxlen=256, padding='post') for train_page in train_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attr_data = [np.array(train_page, dtype='float32') for train_page in train_attr_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tk.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = []\n",
    "embedding_weights.append(np.zeros(vocab_size))\n",
    "\n",
    "for char, i in tk.word_index.items():\n",
    "    onehot = np.zeros(vocab_size)\n",
    "    onehot[i-1] = 1\n",
    "    embedding_weights.append(onehot)\n",
    "embedding_weights = np.array(embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_embedding_size = 69\n",
    "input_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_embedding_layer = Embedding(vocab_size+1, \n",
    "                                 attr_embedding_size,\n",
    "                                 input_length=input_size,\n",
    "                                 weights = [embedding_weights]\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_feature_list = list(token_features[0][0].keys())\n",
    "\n",
    "def pages_to_word_vector_from_keylist(word_vector_method, token_features, word_to_vec_list):\n",
    "    print(f\"Transform key {word_to_vec_list} to word_vector ... \")\n",
    "    pages_vector = []\n",
    "    for idx, page in enumerate(token_features):\n",
    "        page_vectors = []\n",
    "        for node in page:\n",
    "            full_vector_list = []\n",
    "            for k,v in node.items():\n",
    "                if k in word_to_vec_list:\n",
    "                    full_vector_list.append(word_to_vector(v, word_vector_method))\n",
    "            full_vector = np.concatenate(full_vector_list, axis=0)\n",
    "            page_vectors.append(full_vector)\n",
    "        pages_vector.append(np.array(page_vectors))\n",
    "    print(\"Finish transforming to word_vector\")\n",
    "    return pages_vector\n",
    "\n",
    "def sparse_representation_with_map(tag, data_map):\n",
    "    rt_vec = [0] * len(data_map)\n",
    "    for idx, map_tag in enumerate(data_map):\n",
    "        if tag == map_tag[0]:\n",
    "            rt_vec[idx] = 1\n",
    "            break\n",
    "    return rt_vec\n",
    "\n",
    "def get_ptags_vector(token_features, data_map):\n",
    "    pages_ptag = []\n",
    "    for page in token_features:\n",
    "        ptag_page = []\n",
    "        for node in page:\n",
    "            p_tag = node['parent-tag']\n",
    "            ptag_page.append(sparse_representation_with_map(p_tag, data_map))\n",
    "        pages_ptag.append(np.array(ptag_page))\n",
    "    return pages_ptag\n",
    "\n",
    "top_parent_tags = {}\n",
    "for page in token_features:\n",
    "    for node in page:\n",
    "        p_tag = node['parent-tag']\n",
    "        if p_tag not in top_parent_tags:\n",
    "            top_parent_tags[p_tag] = 1\n",
    "        else:\n",
    "            top_parent_tags[p_tag] += 1\n",
    "            \n",
    "# Create datamap for ptag\n",
    "sorted_parent_tags = sorted(top_parent_tags.items(),key=lambda x:x[1],reverse=True)\n",
    "data_map_for_ptag = sorted_parent_tags[:30]\n",
    "\n",
    "\n",
    "\n",
    "ptags_vector = get_ptags_vector(token_features, data_map_for_ptag)\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TagTokenizer:\n",
    "    def __init__(self, myDict = None):\n",
    "        rt_dict = {}\n",
    "        rt_dict['[PAD]'] = 0\n",
    "        rt_dict['[UNK]'] = 1\n",
    "        i = 2\n",
    "        if myDict is not None:\n",
    "            for k,v in myDict.items():\n",
    "                rt_dict[k] = i\n",
    "                i+=1\n",
    "        self.map = rt_dict\n",
    "        \n",
    "    def tokenize(self, word):\n",
    "        if type(word) == type([]):\n",
    "            token_list = []\n",
    "            for _word in word:\n",
    "                if _word not in self.map:\n",
    "                    token_list.append(self.map['[UNK]'])\n",
    "                else:\n",
    "                    token_list.append(self.map[_word])\n",
    "            return token_list\n",
    "        else:\n",
    "            if word not in self.map:\n",
    "                return self.map['[UNK]']\n",
    "            else:\n",
    "                return self.map[word]\n",
    "    def get_size(self):\n",
    "        return len(self.map)\n",
    "\n",
    "top_thousand_class = {}\n",
    "top_thousand_query = {}\n",
    "text_map = {}\n",
    "for page in token_features:\n",
    "    for node in page:\n",
    "        for _class in node['class']:\n",
    "            if _class in top_thousand_class:\n",
    "                top_thousand_class[_class]+=1\n",
    "            else:\n",
    "                top_thousand_class[_class]=1\n",
    "        for _query in node['query']:\n",
    "            if _query in top_thousand_query:\n",
    "                top_thousand_query[_query]+=1\n",
    "            else:\n",
    "                top_thousand_query[_query]=1\n",
    "        for _text in node['text']:\n",
    "            if _text not in text_map:\n",
    "                text_map[_text] = 1\n",
    "\n",
    "class_tokenizer = TagTokenizer(top_thousand_class)\n",
    "query_tokenizer = TagTokenizer(top_thousand_query)\n",
    "text_tokenizer = TagTokenizer(text_map)\n",
    "\n",
    "\n",
    "ft_full_tokens_emb = np.load('embedding/train/LaserEmb.npy', allow_pickle=True)\n",
    "train_tag_info_list = tag_features #features which only have tag true/false information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_class, pages_query, pages_text = prepare_input_ids(token_features, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attr_x = ft_full_tokens_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ptag = ptags_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag_x = tag_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag_x = [ np.concatenate([tag_info,ptags], axis = 1) if len(tag_info)!=0 else np.array([]) for tag_info, ptags in zip(train_tag_x, train_ptag)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_composite_with_token = [train_attr_x, train_attr_data, pages_class, pages_query, train_tag_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"O\", \"PREV\", \"PAGE\", \"NEXT\"]\n",
    "tag2idx = { label:idx for idx,label in enumerate(labels)}\n",
    "idx2tag = { idx:label for idx,label in enumerate(labels)}\n",
    "num_tags = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = [np.array([tag2idx.get(l) for l in lab]) for lab in chunks_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 1024)\n",
      "(303, 256)\n",
      "(303, 256)\n",
      "(303, 256)\n",
      "(303, 38)\n"
     ]
    }
   ],
   "source": [
    "for inputs in train_composite_with_token:\n",
    "    print(inputs[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = [[256, 7, 3],\n",
    "               [256, 7, 3],\n",
    "               [256, 3, -1],\n",
    "               [256, 3, -1],\n",
    "               [256, 3, -1],\n",
    "               [256, 3, 3]\n",
    "              ]\n",
    "dropout_p = 0.25\n",
    "fully_connected_layers = [512, 512]\n",
    "ft_shape = (None, 1024)\n",
    "tag_info_shape = (None, 38)\n",
    "tag_emb_shape = (None, 256)\n",
    "HIDDEN_UNITS = 300\n",
    "embedding_size = 32\n",
    "\n",
    "\n",
    "NUM_CLASS = num_tags\n",
    "embbed_output_shape = embedding_size\n",
    "page_embbed_shape = (-1, embbed_output_shape)\n",
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "input_ft_embedding = Input(shape=(ft_shape), name=\"input_ft_embeddings\")\n",
    "input_tag_information = Input(shape=(tag_info_shape), name=\"input_tag_information\")\n",
    "input_attribute = Input(shape=(tag_emb_shape), name=\"input_attr\")\n",
    "input_class = Input(shape=(tag_emb_shape), name=\"input_class\")\n",
    "input_query = Input(shape=(tag_emb_shape), name=\"input_query\")\n",
    "\n",
    "# Char-CNN Attribute start\n",
    "\n",
    "\n",
    "class_emb = Embedding(input_dim = class_tokenizer.get_size(), output_dim = embbed_output_shape, input_length=max_page_seq, mask_zero = True)(input_class)\n",
    "class_emb = AveragePooling2D((256, 1), data_format = 'channels_first')(class_emb)\n",
    "class_emb = Reshape(page_embbed_shape, name=\"class_emb_out\")(class_emb)\n",
    "## input_query\n",
    "query_emb = Embedding(input_dim = query_tokenizer.get_size(), output_dim = embbed_output_shape, input_length=max_page_seq, mask_zero = True)(input_query)\n",
    "query_emb = AveragePooling2D((256, 1), data_format = 'channels_first')(query_emb)\n",
    "query_emb = Reshape(page_embbed_shape, name=\"query_emb_out\")(query_emb)\n",
    "\n",
    "attr_emb = Embedding(vocab_size+1,attr_embedding_size,input_length=input_size,weights = [embedding_weights])(input_attribute)\n",
    "for filter_num, filter_size, pooling_size in conv_layers:\n",
    "    attr_emb = Conv1D(filter_num, filter_size, input_shape = attr_emb.shape[2:])(attr_emb)\n",
    "    attr_emb = Activation('relu')(attr_emb)\n",
    "    if pooling_size != -1:\n",
    "        attr_emb = MaxPooling2D(pool_size=(1, pooling_size))(attr_emb)  # Final shape=(None, 34, 256)\n",
    "attr_shape = attr_emb.get_shape().as_list()\n",
    "attr_emb = Reshape((-1, attr_shape[2] * attr_shape[3]))(attr_emb)\n",
    "attr_emb_merged = attr_emb\n",
    "attr_emb_merged = Concatenate()([attr_emb, class_emb, query_emb])\n",
    "for dense_size in fully_connected_layers:\n",
    "    attr_emb_merged = Dense(dense_size, activation='relu')(attr_emb_merged)  # dense_size == 1024\n",
    "\n",
    "# Char-CNN Attribute end\n",
    "ft_FFN = Dense(units = 512, activation = 'relu', name=\"ft_FFN_01\")(input_ft_embedding)\n",
    "ft_FFN = Dense(units = 256, activation = 'relu', name=\"ft_FFN_02\")(ft_FFN)\n",
    "ft_FFN = Dense(units = 128, activation = 'relu', name=\"ft_FFN_out\")(ft_FFN)\n",
    "\n",
    "merged = Concatenate()([ft_FFN, attr_emb_merged, input_tag_information])\n",
    "model = Bidirectional(LSTM(units = HIDDEN_UNITS//2, return_sequences=True))(merged)\n",
    "\n",
    "crf=CRF(NUM_CLASS, name='crf_layer')\n",
    "out =crf(model)\n",
    "model = Model([input_ft_embedding, input_attribute, input_class, input_query, input_tag_information], out)\n",
    "\n",
    "loss_fn = crf.get_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "def calculate_pages_metric(y_true_pages, y_predict_pages):\n",
    "    pages_f1 = []\n",
    "    nexts_f1 = []\n",
    "    avg_f1 = []\n",
    "    for y_true, y_predict in zip(y_true_pages, y_predict_pages):\n",
    "        if len(y_true) == 0:\n",
    "            break\n",
    "        report = classification_report(y_true, y_predict,output_dict=True)\n",
    "#         print(report)\n",
    "        PAGE = report['2']['f1-score']\n",
    "        NEXT = report['3']['f1-score']\n",
    "        pages_f1.append(PAGE)\n",
    "        nexts_f1.append(NEXT)\n",
    "        avg_f1.append((PAGE+NEXT)/2)\n",
    "    return pages_f1, nexts_f1, avg_f1\n",
    "def calculate_page_metric(y_true, y_predict):    \n",
    "    report = classification_report(y_true, y_predict,labels=[0,2,3],output_dict=True)\n",
    "    OTHER = report['0']['f1-score']\n",
    "    PAGE = report['2']['f1-score']\n",
    "    NEXT = report['3']['f1-score']\n",
    "    if 2 in y_true and 3 in y_true:\n",
    "        AVG = (PAGE+NEXT)/2\n",
    "    elif 2 in y_true and 3 not in y_true:\n",
    "        AVG = PAGE\n",
    "    elif 2 not in y_true and 3 in y_true:\n",
    "        AVG = NEXT\n",
    "    else:\n",
    "        AVG = OTHER\n",
    "    return AVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_epoch(epochs, model, optimizer, train_dataset, val_dataset, best_model_method = 'f1-score'):\n",
    "    import time\n",
    "    \n",
    "    epochs = epochs\n",
    "    best_weights = None\n",
    "    best_f1_weights = None\n",
    "    best = np.Inf\n",
    "    best_loss_history = None\n",
    "    best_f1 = 0\n",
    "    best_f1_history = None\n",
    "    avg_epoch_losses = []\n",
    "    avg_epoch_f1s = []\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(x_batch_train, training=True)\n",
    "                loss_value = loss_fn(y_batch_train, logits)\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "            # Log every 50 batches.\n",
    "#             if step % 50 == 0:\n",
    "#                 print(\n",
    "#                     \"Training loss (for one batch) at step %d: %.4f\"\n",
    "#                     % (step, float(loss_value))\n",
    "#                 )\n",
    "#                 print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "\n",
    "\n",
    "        # Run a validation loop at the end of each epoch.\n",
    "        val_losses = []\n",
    "        val_f1s = []\n",
    "        for x_batch_val, y_batch_val in val_dataset:\n",
    "            val_logits = model(x_batch_val, training=False)\n",
    "            val_loss_value = loss_fn(y_batch_val, val_logits)\n",
    "            val_avg_f1 = calculate_page_metric(y_batch_val.numpy()[0], val_logits.numpy()[0])\n",
    "            val_losses.append(val_loss_value)\n",
    "            val_f1s.append(val_avg_f1)\n",
    "        average_val_loss = np.average(val_losses)\n",
    "        average_val_f1 = np.average(val_f1s)\n",
    "        avg_epoch_losses.append(average_val_loss)\n",
    "        avg_epoch_f1s.append(average_val_f1)\n",
    "        if average_val_loss < best:\n",
    "            best_weights = model.get_weights()\n",
    "            best = average_val_loss\n",
    "            best_loss_history = [val_losses, val_f1s]\n",
    "        if average_val_f1 > best_f1:\n",
    "            best_f1_weights = model.get_weights()\n",
    "            best_f1 = average_val_f1\n",
    "            best_f1_history = [val_losses, val_f1s]\n",
    "        print(\"Validation loss: %.4f\" % (float(average_val_loss),))\n",
    "        print(\"Validation F1: %.4f\" % (float(average_val_f1),))\n",
    "        print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "    print(f\"Best loss: {best}, Best F1: {best_f1}\")\n",
    "    print(f\"Training finish, load best weights. {best_model_method}\")\n",
    "    \n",
    "    if best_model_method == 'loss':\n",
    "        model.set_weights(best_weights)\n",
    "    elif best_model_method == 'f1-score':\n",
    "        model.set_weights(best_f1_weights)\n",
    "    avg_epoch_result = {\"epoch_losses\": avg_epoch_losses, \"epoch_f1s\": avg_epoch_f1s}\n",
    "    return model, avg_epoch_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_testing(test_X_raw, test_y_raw): #ft-bert -no chunks\n",
    "    chunks_test_x, chunks_test_y = test_X_raw, test_y_raw\n",
    "    chunks_test_x, chunks_test_y = filter_empty(chunks_test_x, chunks_test_y)\n",
    "    test_token_features, test_tag_features = get_token_tag_features_from_chunks(chunks_test_x)\n",
    "    \n",
    "    test_ptags_vector = get_ptags_vector(test_token_features, data_map_for_ptag)\n",
    "    test_ft_emb = pages_to_word_vector_from_keylist('Laser', test_token_features, ['text-exact'])\n",
    "    test_attr = get_test_attr(tk, test_token_features)\n",
    "    test_pages_class, test_pages_query, _ = prepare_input_ids(test_token_features, 256)\n",
    "    test_tag_info_list = test_tag_features\n",
    "\n",
    "    ## X_test_input\n",
    "    test_tag_x = [ np.concatenate([tag_info,ptags], axis = 1) if len(tag_info)!=0 else np.array([]) for tag_info, ptags in zip(test_tag_info_list, test_ptags_vector)]\n",
    "    test_composite_input = [test_ft_emb, test_attr, test_pages_class, test_pages_query, test_tag_x]\n",
    "    \n",
    "    ## y_test_input\n",
    "    y_test = [[tag2idx.get(l) for l in lab] for lab in chunks_test_y]\n",
    "    y_test = [[idx2tag.get(lab) for lab in page] for page in y_test]\n",
    "    y_test = np.asarray(y_test)\n",
    "    \n",
    "    return test_composite_input, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_from_batch(model, x, y, evaluate_labels):\n",
    "    print(\"Start predicting test data ...\")\n",
    "    test_page_dataset = composite_list_to_dataset(x)\n",
    "    predicted_y = []\n",
    "    for pageIdx, batch_x_test in enumerate(test_page_dataset):\n",
    "        if len(y[pageIdx]) == 0:\n",
    "            batch_predict_y = np.array([])\n",
    "        else:\n",
    "            batch_predict_y = model(batch_x_test)[0].numpy()\n",
    "        if len(batch_predict_y.shape) != 1:\n",
    "            tmp = list()\n",
    "            for lab in batch_predict_y:\n",
    "                lab = lab.tolist()\n",
    "                tmp.append(lab.index(max(lab)))\n",
    "            batch_predict_y = tmp\n",
    "        predicted_y.append(batch_predict_y)\n",
    "    print(\"Start evaluating test data ...\")\n",
    "    predict_y = np.asarray([[idx2tag.get(lab) for lab in page] for page in predicted_y])\n",
    "    report = flat_classification_report(y, predict_y, labels=evaluate_labels, digits=3,output_dict=True)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, target = \"all\"):\n",
    "    TEST_MODEL = model\n",
    "#     test_languages = storage.get_all_test_languages()\n",
    "    test_languages = ['en','de','ru','zh','ja','ko']\n",
    "    if target != \"all\":\n",
    "        test_languages = [target]\n",
    "    reports = {}\n",
    "    for language in test_languages:\n",
    "        print(\"Testing language: \", language)\n",
    "        test_urls = [rec['Page URL'] for rec in storage.iter_test_records_by_language(language=language)]\n",
    "        test_X_raw, test_y = storage.get_test_Xy_by_language(language=language)\n",
    "        print(\"pages: {}  domains: {}\".format(len(test_urls), len({get_domain(url) for url in test_urls})))\n",
    "        _test_x, _test_y = prepare_for_testing(test_X_raw, test_y)\n",
    "        report = evaluate_from_batch(TEST_MODEL, _test_x, _test_y, ['PAGE','NEXT'])\n",
    "        print(pd.DataFrame(report))\n",
    "        reports[language] = report\n",
    "        print(\"===================================\")\n",
    "    return reports\n",
    "\n",
    "def calculate_macro_avg(reports):\n",
    "    avg_macro = 0\n",
    "    for lan, report in reports.items():\n",
    "        avg_macro+=report['macro avg']['f1-score']\n",
    "    return avg_macro/len(reports)\n",
    "\n",
    "def calculate_micro_avg(reports):\n",
    "    avg_micro = 0\n",
    "    for lan, report in reports.items():\n",
    "        avg_micro+=report['micro avg']['f1-score']\n",
    "    return avg_micro/len(reports)\n",
    "\n",
    "def calculate_all_score(reports):\n",
    "    avg_page = 0\n",
    "    avg_next = 0\n",
    "    avg_macro = 0\n",
    "    for lan, report in reports.items():\n",
    "        avg_page+=report['micro avg']['f1-score']\n",
    "        avg_next+=report['micro avg']['f1-score']\n",
    "        avg_micro+=report['micro avg']['f1-score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val = composite_splite_to_train_val(train_composite_with_token, train_y, 20)\n",
    "train_dataset = data_list_to_dataset(x_train, y_train, isValidation=False)\n",
    "val_dataset = data_list_to_dataset(x_val, y_val, isValidation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ready for traininig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, avg_epoch_result = train_on_epoch(train_epoch, model, optimizer, train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = evaluate_model(model, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_score = calculate_micro_avg(reports)\n",
    "macro_score = calculate_macro_avg(reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Micro F1: \", micro_score)\n",
    "print(\"Macro F1: \", macro_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================================================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
