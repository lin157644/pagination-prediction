{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "import re\n",
    "import sys\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "from urllib.parse import urlparse, urlsplit, parse_qs, parse_qsl\n",
    "\n",
    "import numpy as np\n",
    "import parsel\n",
    "from sklearn_crfsuite.metrics import flat_classification_report, sequence_accuracy_score\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "from autopager.storage import Storage\n",
    "from autopager.htmlutils import (get_link_text, get_text_around_selector_list,\n",
    "                                 get_link_href, get_selector_root)\n",
    "from autopager.utils import (\n",
    "    get_domain, normalize_whitespaces, normalize, ngrams, tokenize, ngrams_wb, replace_digits\n",
    ")\n",
    "from autopager.model import _num_tokens_feature, _elem_attr\n",
    "from autopager import AUTOPAGER_LIMITS\n",
    "from autopager.parserutils import (TagParser, MyHTMLParser, draw_scaled_page, position_check, compare_tag, get_first_tag)\n",
    "parser = MyHTMLParser()\n",
    "tagParser = TagParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import random\n",
    "tf.random.set_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current test file:  ['en', 'zh', 'ko', 'ja', 'de', 'ru']\n"
     ]
    }
   ],
   "source": [
    "storage = Storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpus)!=0:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPUs visible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contain position: True\n",
      "Finish: Get Page 1 (Encoding: UTF-8)records ... (len: 303)\n",
      "Finish: Get Page 2 (Encoding: UTF-8)records ... (len: 243)\n",
      "Finish: Get Page 3 (Encoding: UTF-8)records ... (len: 119)\n",
      "Finish: Get Page 4 (Encoding: UTF-8)records ... (len: 944)\n",
      "Finish: Get Page 5 (Encoding: UTF-8)records ... (len: 93)\n",
      "Finish: Get Page 6 (Encoding: UTF-8)records ... (len: 994)\n",
      "Finish: Get Page 7 (Encoding: UTF-8)records ... (len: 1014)\n",
      "Finish: Get Page 8 (Encoding: UTF-8)records ... (len: 7)\n",
      "Finish: Get Page 21 (Encoding: UTF-8)records ... (len: 158)\n",
      "Finish: Get Page 22 (Encoding: UTF-8)records ... (len: 171)\n",
      "Finish: Get Page 23 (Encoding: UTF-8)records ... (len: 181)\n",
      "Finish: Get Page 24 (Encoding: UTF-8)records ... (len: 10)\n",
      "Finish: Get Page 25 (Encoding: UTF-8)records ... (len: 165)\n",
      "Finish: Get Page 26 (Encoding: UTF-8)records ... (len: 147)\n",
      "Finish: Get Page 28 (Encoding: UTF-8)records ... (len: 268)\n",
      "Finish: Get Page 33 (Encoding: UTF-8)records ... (len: 108)\n",
      "Finish: Get Page 34 (Encoding: UTF-8)records ... (len: 109)\n",
      "Finish: Get Page 35 (Encoding: UTF-8)records ... (len: 84)\n",
      "Finish: Get Page 36 (Encoding: UTF-8)records ... (len: 718)\n",
      "Finish: Get Page 37 (Encoding: UTF-8)records ... (len: 723)\n",
      "Finish: Get Page 38 (Encoding: UTF-8)records ... (len: 703)\n",
      "Finish: Get Page 46 (Encoding: UTF-8)records ... (len: 82)\n",
      "Finish: Get Page 47 (Encoding: UTF-8)records ... (len: 88)\n",
      "Finish: Get Page 48 (Encoding: UTF-8)records ... (len: 34)\n",
      "Finish: Get Page 49 (Encoding: UTF-8)records ... (len: 62)\n",
      "Finish: Get Page 50 (Encoding: UTF-8)records ... (len: 15)\n",
      "Finish: Get Page 56 (Encoding: cp1252)records ... (len: 520)\n",
      "Finish: Get Page 57 (Encoding: cp1252)records ... (len: 463)\n",
      "Finish: Get Page 62 (Encoding: UTF-8)records ... (len: 55)\n",
      "Finish: Get Page 63 (Encoding: UTF-8)records ... (len: 84)\n",
      "Finish: Get Page 64 (Encoding: UTF-8)records ... (len: 95)\n",
      "Finish: Get Page 68 (Encoding: UTF-8)records ... (len: 37)\n",
      "Finish: Get Page 69 (Encoding: UTF-8)records ... (len: 312)\n",
      "Finish: Get Page 71 (Encoding: UTF-8)records ... (len: 104)\n",
      "Finish: Get Page 72 (Encoding: UTF-8)records ... (len: 92)\n",
      "Finish: Get Page 74 (Encoding: UTF-8)records ... (len: 171)\n",
      "Finish: Get Page 75 (Encoding: UTF-8)records ... (len: 386)\n",
      "Finish: Get Page 76 (Encoding: UTF-8)records ... (len: 319)\n",
      "Finish: Get Page 77 (Encoding: UTF-8)records ... (len: 114)\n",
      "Finish: Get Page 78 (Encoding: UTF-8)records ... (len: 118)\n",
      "Finish: Get Page 79 (Encoding: UTF-8)records ... (len: 98)\n",
      "Finish: Get Page 84 (Encoding: UTF-8)records ... (len: 94)\n",
      "Finish: Get Page 85 (Encoding: UTF-8)records ... (len: 98)\n",
      "Finish: Get Page 91 (Encoding: UTF-8)records ... (len: 2389)\n",
      "Finish: Get Page 92 (Encoding: UTF-8)records ... (len: 2379)\n",
      "Finish: Get Page 93 (Encoding: UTF-8)records ... (len: 160)\n",
      "Finish: Get Page 94 (Encoding: UTF-8)records ... (len: 74)\n",
      "Finish: Get Page 95 (Encoding: UTF-8)records ... (len: 143)\n",
      "Finish: Get Page 96 (Encoding: UTF-8)records ... (len: 133)\n",
      "Finish: Get Page 97 (Encoding: UTF-8)records ... (len: 163)\n",
      "Finish: Get Page 98 (Encoding: UTF-8)records ... (len: 378)\n",
      "Finish: Get Page 99 (Encoding: UTF-8)records ... (len: 120)\n",
      "Finish: Get Page 100 (Encoding: UTF-8)records ... (len: 124)\n",
      "Finish: Get Page 101 (Encoding: UTF-8)records ... (len: 122)\n",
      "Finish: Get Page 108 (Encoding: UTF-8)records ... (len: 233)\n",
      "Finish: Get Page 109 (Encoding: cp1252)records ... (len: 155)\n",
      "Finish: Get Page 110 (Encoding: cp1252)records ... (len: 161)\n",
      "Finish: Get Page 111 (Encoding: cp1252)records ... (len: 131)\n",
      "Finish: Get Page 112 (Encoding: cp1252)records ... (len: 131)\n",
      "Finish: Get Page 113 (Encoding: UTF-8)records ... (len: 72)\n",
      "Finish: Get Page 114 (Encoding: UTF-8)records ... (len: 70)\n",
      "Finish: Get Page 115 (Encoding: UTF-8)records ... (len: 126)\n",
      "Finish: Get Page 116 (Encoding: UTF-8)records ... (len: 90)\n",
      "Finish: Get Page 117 (Encoding: UTF-8)records ... (len: 89)\n",
      "Finish: Get Page 118 (Encoding: UTF-8)records ... (len: 79)\n",
      "Finish: Get Page 119 (Encoding: UTF-8)records ... (len: 133)\n",
      "Finish: Get Page 120 (Encoding: UTF-8)records ... (len: 89)\n",
      "Finish: Get Page 121 (Encoding: UTF-8)records ... (len: 81)\n",
      "Finish: Get Page 125 (Encoding: UTF-8)records ... (len: 16)\n",
      "Finish: Get Page 127 (Encoding: UTF-8)records ... (len: 154)\n",
      "Finish: Get Page 128 (Encoding: UTF-8)records ... (len: 51)\n",
      "Finish: Get Page 129 (Encoding: UTF-8)records ... (len: 88)\n",
      "Finish: Get Page 130 (Encoding: UTF-8)records ... (len: 88)\n",
      "Finish: Get Page 132 (Encoding: UTF-8)records ... (len: 306)\n",
      "Finish: Get Page 133 (Encoding: UTF-8)records ... (len: 309)\n",
      "Finish: Get Page 134 (Encoding: UTF-8)records ... (len: 103)\n",
      "Finish: Get Page 135 (Encoding: UTF-8)records ... (len: 88)\n",
      "Finish: Get Page 136 (Encoding: UTF-8)records ... (len: 159)\n",
      "Finish: Get Page 137 (Encoding: UTF-8)records ... (len: 35)\n",
      "Finish: Get Page 138 (Encoding: UTF-8)records ... (len: 112)\n",
      "Finish: Get Page 139 (Encoding: UTF-8)records ... (len: 117)\n",
      "Finish: Get Page 140 (Encoding: UTF-8)records ... (len: 142)\n",
      "Finish: Get Page 141 (Encoding: UTF-8)records ... (len: 116)\n",
      "Finish: Get Page 143 (Encoding: cp1252)records ... (len: 84)\n",
      "Finish: Get Page 144 (Encoding: cp1252)records ... (len: 134)\n",
      "Finish: Get Page 145 (Encoding: cp1252)records ... (len: 139)\n",
      "Finish: Get Page 146 (Encoding: cp1252)records ... (len: 95)\n",
      "Finish: Get Page 147 (Encoding: cp1252)records ... (len: 130)\n",
      "Finish: Get Page 148 (Encoding: UTF-8)records ... (len: 478)\n",
      "Finish: Get Page 149 (Encoding: UTF-8)records ... (len: 365)\n",
      "Finish: Get Page 150 (Encoding: UTF-8)records ... (len: 368)\n",
      "Finish: Get Page 151 (Encoding: UTF-8)records ... (len: 369)\n",
      "Finish: Get Page 152 (Encoding: cp1252)records ... (len: 294)\n",
      "Finish: Get Page 153 (Encoding: UTF-8)records ... (len: 271)\n",
      "Finish: Get Page 162 (Encoding: UTF-8)records ... (len: 308)\n",
      "Finish: Get Page 163 (Encoding: UTF-8)records ... (len: 298)\n",
      "Finish: Get Page 164 (Encoding: UTF-8)records ... (len: 285)\n",
      "Finish: Get Page 165 (Encoding: UTF-8)records ... (len: 221)\n",
      "Finish: Get Page 203 (Encoding: UTF-8)records ... (len: 361)\n",
      "Finish: Get Page 204 (Encoding: UTF-8)records ... (len: 356)\n",
      "Finish: Get Page 205 (Encoding: UTF-8)records ... (len: 360)\n",
      "Finish: Get Page 206 (Encoding: UTF-8)records ... (len: 331)\n",
      "Finish: Get Page 207 (Encoding: UTF-8)records ... (len: 498)\n",
      "Finish: Get Page 208 (Encoding: UTF-8)records ... (len: 499)\n",
      "Finish: Get Page 209 (Encoding: UTF-8)records ... (len: 497)\n",
      "Finish: Get Page 211 (Encoding: UTF-8)records ... (len: 145)\n",
      "Finish: Get Page 212 (Encoding: UTF-8)records ... (len: 147)\n",
      "Finish: Get Page 213 (Encoding: UTF-8)records ... (len: 111)\n",
      "Finish: Get Page 218 (Encoding: UTF-8)records ... (len: 83)\n",
      "Finish: Get Page 219 (Encoding: UTF-8)records ... (len: 84)\n",
      "Finish: Get Page 220 (Encoding: UTF-8)records ... (len: 72)\n",
      "Finish: Get Page 221 (Encoding: UTF-8)records ... (len: 207)\n",
      "Finish: Get Page 222 (Encoding: UTF-8)records ... (len: 202)\n",
      "Finish: Get Page 223 (Encoding: UTF-8)records ... (len: 103)\n",
      "Finish: Get Page 224 (Encoding: cp1252)records ... (len: 59)\n",
      "Finish: Get Page 225 (Encoding: UTF-8)records ... (len: 58)\n",
      "Finish: Get Page 226 (Encoding: UTF-8)records ... (len: 346)\n",
      "Finish: Get Page 227 (Encoding: UTF-8)records ... (len: 281)\n",
      "Finish: Get Page 228 (Encoding: UTF-8)records ... (len: 164)\n",
      "Finish: Get Page 232 (Encoding: cp1252)records ... (len: 74)\n",
      "Finish: Get Page 233 (Encoding: UTF-8)records ... (len: 61)\n",
      "Finish: Get Page 234 (Encoding: UTF-8)records ... (len: 258)\n",
      "Finish: Get Page 237 (Encoding: UTF-8)records ... (len: 286)\n",
      "Finish: Get Page 238 (Encoding: UTF-8)records ... (len: 190)\n",
      "Finish: Get Page 239 (Encoding: cp1252)records ... (len: 256)\n",
      "Finish: Get Page 240 (Encoding: cp1252)records ... (len: 148)\n",
      "Finish: Get Page 241 (Encoding: UTF-8)records ... (len: 98)\n",
      "Finish: Get Page 242 (Encoding: UTF-8)records ... (len: 164)\n",
      "Finish: Get Page 243 (Encoding: UTF-8)records ... (len: 169)\n",
      "Finish: Get Page 244 (Encoding: UTF-8)records ... (len: 139)\n",
      "Finish: Get Page 245 (Encoding: UTF-8)records ... (len: 131)\n",
      "Finish: Get Page 246 (Encoding: cp1252)records ... (len: 528)\n",
      "Finish: Get Page 247 (Encoding: UTF-8)records ... (len: 217)\n",
      "Finish: Get Page 249 (Encoding: UTF-8)records ... (len: 61)\n",
      "Finish: Get Page 261 (Encoding: UTF-8)records ... (len: 306)\n",
      "Finish: Get Page 262 (Encoding: UTF-8)records ... (len: 129)\n",
      "Finish: Get Page 263 (Encoding: UTF-8)records ... (len: 171)\n",
      "Finish: Get Page 264 (Encoding: UTF-8)records ... (len: 53)\n",
      "Finish: Get Page 265 (Encoding: UTF-8)records ... (len: 199)\n",
      "Finish: Get Page 266 (Encoding: UTF-8)records ... (len: 86)\n",
      "Finish: Get Page 267 (Encoding: UTF-8)records ... (len: 131)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish: Get Page 284 (Encoding: cp1252)records ... (len: 130)\n",
      "Finish: Get Page 287 (Encoding: UTF-8)records ... (len: 82)\n",
      "Finish: Get Page 288 (Encoding: UTF-8)records ... (len: 140)\n",
      "Finish: Get Page 289 (Encoding: UTF-8)records ... (len: 44)\n",
      "Finish: Get Page 293 (Encoding: UTF-8)records ... (len: 74)\n",
      "Finish: Get Page 294 (Encoding: UTF-8)records ... (len: 63)\n",
      "Finish: Get Page 295 (Encoding: UTF-8)records ... (len: 65)\n",
      "Finish: Get Page 296 (Encoding: UTF-8)records ... (len: 20)\n",
      "Finish: Get Page 299 (Encoding: UTF-8)records ... (len: 361)\n",
      "Finish: Get Page 300 (Encoding: UTF-8)records ... (len: 361)\n",
      "Finish: Get Page 301 (Encoding: UTF-8)records ... (len: 364)\n",
      "Finish: Get Page 302 (Encoding: UTF-8)records ... (len: 170)\n",
      "Finish: Get Page 303 (Encoding: UTF-8)records ... (len: 154)\n",
      "Finish: Get Page 304 (Encoding: cp1252)records ... (len: 117)\n",
      "Finish: Get Page 305 (Encoding: UTF-8)records ... (len: 1987)\n",
      "Finish: Get Page 312 (Encoding: cp1252)records ... (len: 136)\n",
      "Finish: Get Page 313 (Encoding: UTF-8)records ... (len: 383)\n",
      "Finish: Get Page 314 (Encoding: UTF-8)records ... (len: 317)\n",
      "Finish: Get Page 315 (Encoding: cp1252)records ... (len: 314)\n",
      "Finish: Get Page 316 (Encoding: cp1252)records ... (len: 357)\n",
      "Finish: Get Page 317 (Encoding: cp1252)records ... (len: 370)\n",
      "Finish: Get Page 323 (Encoding: UTF-8)records ... (len: 389)\n",
      "Finish: Get Page 324 (Encoding: UTF-8)records ... (len: 330)\n",
      "pages: 164  domains: 55\n",
      "CPU times: user 7.19 s, sys: 131 ms, total: 7.32 s\n",
      "Wall time: 7.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "urls = [rec['Page URL'] for rec in storage.iter_records(language='en',contain_button = True, file_type='T')]\n",
    "X_raw, y, page_positions = storage.get_Xy(language='en',contain_button = True,  contain_position=True,file_type='T', scaled_page='normal')\n",
    "print(\"pages: {}  domains: {}\".format(len(urls), len({get_domain(url) for url in urls})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_page_seq = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slice data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_data(x, y, p):\n",
    "    new_tmp_x_array = []\n",
    "    new_tmp_y_array = []\n",
    "    new_tmp_p_array = []\n",
    "    for tmp_x, tmp_y, tmp_p in zip(x, y, p):\n",
    "        new_tmp_x_array.extend(chunks(tmp_x, max_page_seq))\n",
    "        new_tmp_y_array.extend(chunks(tmp_y, max_page_seq))\n",
    "        new_tmp_p_array.extend(chunks(tmp_p, max_page_seq))\n",
    "    return new_tmp_x_array, new_tmp_y_array, new_tmp_p_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_x, chunks_y, chunk_positions = get_chunks_data(X_raw, y, page_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained Fastext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "from FastTextModel import FastTextModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dimension:  100\n"
     ]
    }
   ],
   "source": [
    "ft = FastTextModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    ans = dot(a, b)/(norm(a)*norm(b))\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80560714"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(ft.getWordVector(\"last\"), ft.getWordVector(\"next\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 24 µs, total: 24 µs\n",
      "Wall time: 32.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# XXX: these functions should be copy-pasted from autopager/model.py\n",
    "\n",
    "def _as_list(generator, limit=None):\n",
    "    \"\"\"\n",
    "    >>> _as_list(ngrams_wb(\"text\", 2, 2), 0)\n",
    "    []\n",
    "    >>> _as_list(ngrams_wb(\"text\", 2, 2), 2)\n",
    "    ['te', 'ex']\n",
    "    >>> _as_list(ngrams_wb(\"text\", 2, 2))\n",
    "    ['te', 'ex', 'xt']\n",
    "    \"\"\"\n",
    "    return list(generator if limit is None else islice(generator, 0, limit))\n",
    "\n",
    "def feat_to_tokens(feat, tokenizer):\n",
    "    if type(feat) == type([]):\n",
    "        feat = ' '.join(feat)\n",
    "    tokens = tokenizer.tokenize(feat)\n",
    "    return tokens\n",
    "\n",
    "def num_token_feature_to_class(number):\n",
    "    if number == '=0':\n",
    "        return [1, 0, 0, 0]\n",
    "    elif number == '=1':\n",
    "        return [0, 1, 0, 0]\n",
    "    elif number == '=2':\n",
    "        return [0, 0, 1, 0]\n",
    "    else:\n",
    "        return [0, 0, 0, 1]\n",
    "\n",
    "def link_to_features(link):\n",
    "    text = normalize(get_link_text(link))\n",
    "    href = get_link_href(link)\n",
    "    if href is None:\n",
    "        href = \"\"\n",
    "    p = urlsplit(href)\n",
    "    parent = link.xpath('..').extract()\n",
    "    parent = get_first_tag(parser, parent[0])\n",
    "    query_parsed = parse_qsl(p.query) #parse query string from path\n",
    "    query_param_names = [k.lower() for k, v in query_parsed]\n",
    "    query_param_names_ngrams = _as_list(ngrams_wb(\n",
    "        \" \".join([normalize(name) for name in query_param_names]), 3, 5, True\n",
    "    ))\n",
    "\n",
    "    # Classes of link itself and all its children.\n",
    "    # It is common to have e.g. span elements with fontawesome\n",
    "    # arrow icon classes inside <a> links.\n",
    "    self_and_children_classes = ' '.join(link.xpath(\".//@class\").extract())\n",
    "    parent_classes = ' '.join(link.xpath('../@class').extract())\n",
    "    css_classes = normalize(parent_classes + ' ' + self_and_children_classes)\n",
    "    \n",
    "    token_feature = {\n",
    "        'text-exact': replace_digits(text.strip()[:100].strip()),\n",
    "        'query': query_param_names,\n",
    "        'parent-tag': parent,\n",
    "        'class': css_classes.split()[:AUTOPAGER_LIMITS.max_css_features],\n",
    "    }\n",
    "    tag_feature = {\n",
    "        'isdigit': 1 if text.isdigit() is True else 0,\n",
    "        'isalpha': 1 if text.isalpha() is True else 0,\n",
    "        'has-href': 0 if href is \"\" else 1,\n",
    "        'path-has-page': 1 if 'page' in p.path.lower() else 0,\n",
    "        'path-has-pageXX': 1 if re.search(r'[/-](?:p|page\\w?)/?\\d+', p.path.lower()) is not None else 0,\n",
    "        'path-has-number': 1 if any(part.isdigit() for part in p.path.split('/')) else 0,\n",
    "        'href-has-year': 1 if re.search('20\\d\\d', href) is not None else 0,\n",
    "        'class-has-disabled': 1 if 'disabled' in css_classes else 0,\n",
    "#         'num-tokens': num_token_feature_to_class(_num_tokens_feature(text)),\n",
    "    }\n",
    "    non_token_feature = []\n",
    "    for k,v in tag_feature.items():\n",
    "        if type(v) == type([]):\n",
    "            non_token_feature.extend(v)\n",
    "        else:\n",
    "            non_token_feature.append(v)\n",
    "\n",
    "    return [token_feature, non_token_feature]\n",
    "\n",
    "\n",
    "def page_to_features(xseq):\n",
    "    feat_list = [link_to_features(a) for a in xseq]\n",
    "    around = get_text_around_selector_list(xseq, max_length=15)\n",
    "    \n",
    "    return feat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_tag_features_from_chunks(chunks):\n",
    "    token_features = []\n",
    "    tag_features = []\n",
    "    for idx, page in enumerate(chunks):\n",
    "        try:\n",
    "            feat_list = page_to_features(page)\n",
    "            token_features.append([node[0] for node in feat_list])\n",
    "            tag_features.append([node[1] for node in feat_list])\n",
    "        except:\n",
    "            raise Exception(f\"Error occured on {idx}\")\n",
    "    return token_features, tag_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_vector(ft, word_list):\n",
    "    if type(word_list) == type([]):\n",
    "        if len(word_list) == 0:\n",
    "            return np.zeros(ft.getModel().get_dimension())\n",
    "        else:\n",
    "            vectors_array = []\n",
    "            for word in word_list:\n",
    "                vector = ft.getWordVector(word)\n",
    "                vectors_array.append(vector)\n",
    "            mean_vector = np.mean(vectors_array, axis = 0)\n",
    "            return mean_vector\n",
    "    else:\n",
    "        return ft.getWordVector(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pages_to_word_vector(ft, token_features):\n",
    "    pages_vector = []\n",
    "    for page in token_features:\n",
    "        page_vectors = []\n",
    "        for node in page:\n",
    "            classes = word_to_vector(ft, node['class'])\n",
    "            query = word_to_vector(ft, node['query'])\n",
    "            p_tag = word_to_vector(ft, node['parent-tag'])\n",
    "            full_vector = np.concatenate([classes, query, p_tag], axis = 0)\n",
    "            page_vectors.append(full_vector)\n",
    "        pages_vector.append(np.array(page_vectors))\n",
    "    return pages_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_features, tag_features = get_token_tag_features_from_chunks(chunks_x)\n",
    "# train_tag_feature_token_list = extract_tokens_from_token_features(token_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_feature_list = list(token_features[0][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pages_to_word_vector_from_keylist(ft, token_features, word_to_vec_list = token_feature_list):\n",
    "    print(f\"Transform key {word_to_vec_list} to word_vector ... \")\n",
    "    pages_vector = []\n",
    "    for page in token_features:\n",
    "        page_vectors = []\n",
    "        for node in page:\n",
    "            full_vector_list = []\n",
    "            for k,v in node.items():\n",
    "                if k in word_to_vec_list:\n",
    "                    full_vector_list.append(word_to_vector(ft, v))\n",
    "            full_vector = np.concatenate(full_vector_list, axis=0)\n",
    "            page_vectors.append(full_vector)\n",
    "        pages_vector.append(np.array(page_vectors))\n",
    "    return pages_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_parent_tags = {}\n",
    "for page in token_features:\n",
    "    for node in page:\n",
    "        p_tag = node['parent-tag']\n",
    "        if p_tag not in top_parent_tags:\n",
    "            top_parent_tags[p_tag] = 1\n",
    "        else:\n",
    "            top_parent_tags[p_tag] += 1\n",
    "sorted_parent_tags = sorted(top_parent_tags.items(),key=lambda x:x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_map_for_ptag = sorted_parent_tags[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_representation_with_map(tag, data_map = data_map_for_ptag):\n",
    "    rt_vec = [0] * len(data_map)\n",
    "    for idx, map_tag in enumerate(data_map):\n",
    "        if tag == map_tag[0]:\n",
    "            rt_vec[idx] = 1\n",
    "            break\n",
    "    return rt_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ptags_vector(token_features):\n",
    "    pages_ptag = []\n",
    "    for page in token_features:\n",
    "        ptag_page = []\n",
    "        for node in page:\n",
    "            p_tag = node['parent-tag']\n",
    "            ptag_page.append(sparse_representation_with_map(p_tag))\n",
    "        pages_ptag.append(ptag_page)\n",
    "    return pages_ptag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptags_vector = get_ptags_vector(token_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TagTokenizer:\n",
    "    def __init__(self, myDict = None):\n",
    "        rt_dict = {}\n",
    "        rt_dict['[PAD]'] = 0\n",
    "        rt_dict['[UNK]'] = 1\n",
    "        i = 2\n",
    "        if myDict is not None:\n",
    "            for item in myDict:\n",
    "                rt_dict[item[0]] = i\n",
    "                i+=1\n",
    "        self.map = rt_dict\n",
    "        \n",
    "    def tokenize(self, word):\n",
    "        if type(word) == type([]):\n",
    "            token_list = []\n",
    "            for _word in word:\n",
    "                if _word not in self.map:\n",
    "                    token_list.append(self.map['[UNK]'])\n",
    "                else:\n",
    "                    token_list.append(self.map[_word])\n",
    "            return token_list\n",
    "        else:\n",
    "            if word not in self.map:\n",
    "                return self.map['[UNK]']\n",
    "            else:\n",
    "                return self.map[word]\n",
    "    def get_size(self):\n",
    "        return len(self.map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_thousand_class = {}\n",
    "top_thousand_query = {}\n",
    "for page in token_features:\n",
    "    for node in page:\n",
    "        for _class in node['class']:\n",
    "            if _class in top_thousand_class:\n",
    "                top_thousand_class[_class]+=1\n",
    "            else:\n",
    "                top_thousand_class[_class]=1\n",
    "        for _query in node['query']:\n",
    "            if _query in top_thousand_query:\n",
    "                top_thousand_query[_query]+=1\n",
    "            else:\n",
    "                top_thousand_query[_query]=1\n",
    "sorted_class_map = sorted(top_thousand_class.items(),key=lambda x:x[1],reverse=True)\n",
    "sorted_query_map = sorted(top_thousand_query.items(),key=lambda x:x[1],reverse=True)\n",
    "\n",
    "class_tokenizer = TagTokenizer(sorted_class_map)\n",
    "query_tokenizer = TagTokenizer(sorted_query_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform key ['text-exact'] to word_vector ... \n"
     ]
    }
   ],
   "source": [
    "# Use ft to encode all token_features\n",
    "ft_full_tokens_emb = pages_to_word_vector_from_keylist(ft, token_features, ['text-exact'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 100)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_full_tokens_emb[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag_info_list = tag_features #features which only have tag true/false information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding to fixed size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_ids(page_tokens, max_len):\n",
    "    pages_class = []\n",
    "    pages_query = []\n",
    "#     print(len(page_tokens))\n",
    "    for page in page_tokens:\n",
    "        class_page = []\n",
    "        query_page = []\n",
    "        for node in page:\n",
    "            #class\n",
    "            class_ids = class_tokenizer.tokenize(node['class'])\n",
    "            class_ids = class_ids + [0] * (max_len-len(class_ids))\n",
    "            class_page.append(class_ids[:max_len])\n",
    "            #query\n",
    "            query_ids = query_tokenizer.tokenize(node['query'])\n",
    "            query_ids = query_ids + [0] * (max_len-len(query_ids))\n",
    "            query_page.append(query_ids[:max_len])\n",
    "        pages_class.append(class_page)\n",
    "        pages_query.append(query_page)\n",
    "    return pages_class, pages_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_pad_to_npdata(embedding):\n",
    "    dataset = Dataset.from_generator(lambda: iter(embedding), tf.float32)\n",
    "    dataset = dataset.padded_batch(1, padded_shapes= (max_page_seq, len(embedding[0][0])), padding_values=-1.,drop_remainder=False)\n",
    "    after_pad = np.array([ data[0] for data in list(dataset.as_numpy_iterator())])\n",
    "    return after_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_pad_to_npdata(embedding):\n",
    "    dataset = Dataset.from_generator(lambda: iter(embedding), tf.int32)\n",
    "    dataset = dataset.padded_batch(1, padded_shapes= (max_page_seq, len(embedding[0][0])), padding_values=0,drop_remainder=False)\n",
    "    after_pad = np.array([ data[0] for data in list(dataset.as_numpy_iterator())])\n",
    "    return after_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_class, pages_query = prepare_input_ids(token_features, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Shape:\n",
      "train_class: (183, 512, 256)\n",
      "train_query: (183, 512, 256)\n"
     ]
    }
   ],
   "source": [
    "train_class = token_pad_to_npdata(pages_class)\n",
    "train_query = token_pad_to_npdata(pages_query)\n",
    "print(\"Current Shape:\")\n",
    "print(f\"train_class: {train_class.shape}\")\n",
    "print(f\"train_query: {train_query.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ptag = token_pad_to_npdata(ptags_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag_x = feature_pad_to_npdata(tag_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attr_x = feature_pad_to_npdata(ft_full_tokens_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concate Tag Embedding + Tag information\n",
    "train_tag_x = np.concatenate([train_tag_x, train_ptag], axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"O\", \"PREV\", \"PAGE\", \"NEXT\", \"[PAD]\"]\n",
    "tag2idx = { label:idx for idx,label in enumerate(labels)}\n",
    "idx2tag = { idx:label for idx,label in enumerate(labels)}\n",
    "num_tags = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pad_sequences([[tag2idx.get(l) for l in lab] for lab in chunks_y], maxlen=max_page_seq, padding=\"post\", truncating=\"post\", value=tag2idx[\"[PAD]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Shape:\n",
      "train_tag_x: (183, 512, 38)\n",
      "train_ft_x: (183, 512, 100)\n",
      "train_y: (183, 512)\n"
     ]
    }
   ],
   "source": [
    "print(\"Current Shape:\")\n",
    "print(f\"train_tag_x: {train_tag_x.shape}\")\n",
    "print(f\"train_ft_x: {train_attr_x.shape}\")\n",
    "print(f\"train_y: {train_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_composite_with_token = [train_attr_x, train_tag_x, train_class, train_query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(183, 512, 100)\n",
      "(183, 512, 38)\n",
      "(183, 512, 256)\n",
      "(183, 512, 256)\n"
     ]
    }
   ],
   "source": [
    "for inputs in train_composite_with_token:\n",
    "    print(inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build BERT-BiLSTM-CRF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_addons.layers.crf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (Dense, Input, Bidirectional, LSTM, Embedding, Masking, Concatenate,\n",
    "                                    AveragePooling2D, MaxPooling2D, Reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestWeightCallback(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.best_weights = None\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.best = np.Inf\n",
    "        self.best_epoch = np.Inf\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"val_loss\")\n",
    "        epoch = epoch + 1\n",
    "        if np.less(current, self.best):\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            self.best = current\n",
    "            self.best_epoch = epoch\n",
    "    def on_train_end(self, logs=None):\n",
    "        print(f\"Training Finish, Best epoch: {self.best_epoch}, Best Val_loss: {self.best}\")\n",
    "        self.model.set_weights(self.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME_STAMP: 512\n",
      "HIDDEN_UNITS: 200\n",
      "DROPOUT_RATE: 0.1\n",
      "NUM_CLASS: 5\n"
     ]
    }
   ],
   "source": [
    "TIME_STAMPS = max_page_seq\n",
    "HIDDEN_UNITS = 200\n",
    "DROPOUT_RATE = 0.1\n",
    "# NUM_CLASS = 5\n",
    "NUM_CLASS = num_tags\n",
    "print(f\"TIME_STAMP: {TIME_STAMPS}\")\n",
    "print(f\"HIDDEN_UNITS: {HIDDEN_UNITS}\")\n",
    "print(f\"DROPOUT_RATE: {DROPOUT_RATE}\")\n",
    "print(f\"NUM_CLASS: {NUM_CLASS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwCallback = BestWeightCallback()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Test avg pooling (ROW)\n",
    "input_class = Input(shape=(tag_emb_shape), name=\"input_class\")\n",
    "\n",
    "class_emb = Embedding(input_dim = class_tokenizer.get_size(), output_dim = embbed_output_shape, input_length=max_page_seq, mask_zero = True)(input_class)\n",
    "class_emb = AveragePooling2D(pool_size, data_format = 'channels_first')(class_emb)\n",
    "class_emb = Reshape(page_embbed_shape, name=\"class_emb_out\")(class_emb)\n",
    "\n",
    "model = Model([input_class], class_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For custom embedding\n",
    "ft_shape = train_attr_x[0].shape\n",
    "tag_info_shape = train_tag_x[0].shape\n",
    "tag_emb_shape = train_class[0].shape\n",
    "embbed_output_shape = 32\n",
    "page_embbed_shape = (train_class[0].shape[0], embbed_output_shape)\n",
    "pool_size = (train_class[0].shape[1], 1)\n",
    "def get_custom_emb_model(use_crf = True):\n",
    "    input_ft_embedding = Input(shape=(ft_shape), name=\"input_ft_embeddings\")\n",
    "    input_tag_information = Input(shape=(tag_info_shape), name=\"input_tag_information\")\n",
    "    input_class = Input(shape=(tag_emb_shape), name=\"input_class\")\n",
    "    input_query = Input(shape=(tag_emb_shape), name=\"input_query\")\n",
    "\n",
    "    #Embedding layers\n",
    "    ## input_class\n",
    "    class_emb = Embedding(input_dim = class_tokenizer.get_size(), output_dim = embbed_output_shape, input_length=max_page_seq, mask_zero = True)(input_class)\n",
    "    class_emb = AveragePooling2D(pool_size, data_format = 'channels_first')(class_emb)\n",
    "#     class_emb = MaxPooling2D(pool_size, data_format = 'channels_first')(class_emb)\n",
    "    class_emb = Reshape(page_embbed_shape, name=\"class_emb_out\")(class_emb)\n",
    "    ## input_query\n",
    "    query_emb = Embedding(input_dim = query_tokenizer.get_size(), output_dim = embbed_output_shape, input_length=max_page_seq, mask_zero = True)(input_query)\n",
    "    query_emb = AveragePooling2D(pool_size, data_format = 'channels_first')(query_emb)\n",
    "#     query_emb = MaxPooling2D(pool_size, data_format = 'channels_first')(query_emb)\n",
    "    query_emb = Reshape(page_embbed_shape, name=\"query_emb_out\")(query_emb)\n",
    "\n",
    "    input_tags = Concatenate()([class_emb, query_emb])\n",
    "#     input_tags = Concatenate()([class_emb, query_emb, input_tag_information])\n",
    "    input_tags_FFN = Dense(units = 256, activation = 'relu')(input_tags)\n",
    "    input_tags_FFN = Dense(units = 128, activation = 'relu', name=\"input_tag_FFN_out\")(input_tags_FFN)\n",
    "\n",
    "    ft_FFN = Dense(units = 256, activation = 'relu', name=\"ft_FFN_01\")(input_ft_embedding)\n",
    "    ft_FFN = Dense(units = 128, activation = 'relu', name=\"ft_FFN_out\")(ft_FFN)\n",
    "\n",
    "\n",
    "    merged = Concatenate()([ft_FFN, input_tags_FFN, input_tag_information])\n",
    "    model = Bidirectional(LSTM(units = HIDDEN_UNITS//2, return_sequences=True))(merged)\n",
    "    if use_crf:\n",
    "        crf=CRF(NUM_CLASS, name='crf_layer')\n",
    "        out =crf(model)\n",
    "    else:\n",
    "        out = Dense(units = NUM_CLASS, activation='softmax')(model)\n",
    "    model = Model([input_ft_embedding, input_tag_information, input_class, input_query], out)\n",
    "    if use_crf:\n",
    "        model.compile('adam',loss={'crf_layer': crf.get_loss})\n",
    "    else:\n",
    "        model.compile('adam',loss=tf.keras.losses.SparseCategoricalCrossentropy())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_custom_emb_model(use_crf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_class (InputLayer)        [(None, 512, 256)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_query (InputLayer)        [(None, 512, 256)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 512, 256, 32) 122400      input_class[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 512, 256, 32) 9632        input_query[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 512, 1, 32)   0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 512, 1, 32)   0           embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "class_emb_out (Reshape)         (None, 512, 32)      0           average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "query_emb_out (Reshape)         (None, 512, 32)      0           average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "input_ft_embeddings (InputLayer [(None, 512, 100)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 512, 64)      0           class_emb_out[0][0]              \n",
      "                                                                 query_emb_out[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ft_FFN_01 (Dense)               (None, 512, 256)     25856       input_ft_embeddings[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512, 256)     16640       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ft_FFN_out (Dense)              (None, 512, 128)     32896       ft_FFN_01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_tag_FFN_out (Dense)       (None, 512, 128)     32896       dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_tag_information (InputLay [(None, 512, 38)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 512, 294)     0           ft_FFN_out[0][0]                 \n",
      "                                                                 input_tag_FFN_out[0][0]          \n",
      "                                                                 input_tag_information[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 512, 200)     316000      concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "crf_layer (CRF)                 (None, 512)          1040        bidirectional_4[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 557,360\n",
      "Trainable params: 557,360\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#For custom embedding\n",
    "ft_shape = train_attr_x[0].shape\n",
    "tag_info_shape = train_tag_x[0].shape\n",
    "tag_emb_shape = train_class[0].shape\n",
    "embbed_output_shape = 32\n",
    "page_embbed_shape = (512, 256)\n",
    "pool_size = (1, embbed_output_shape)\n",
    "def get_custom_emb_model_colpooling(use_crf = True):\n",
    "    input_ft_embedding = Input(shape=(ft_shape), name=\"input_ft_embeddings\")\n",
    "    input_tag_information = Input(shape=(tag_info_shape), name=\"input_tag_information\")\n",
    "    input_class = Input(shape=(tag_emb_shape), name=\"input_class\")\n",
    "    input_query = Input(shape=(tag_emb_shape), name=\"input_query\")\n",
    "\n",
    "    #Embedding layers\n",
    "    ## input_class\n",
    "    class_emb = Embedding(input_dim = class_tokenizer.get_size(), output_dim = embbed_output_shape, input_length=max_page_seq, mask_zero = True)(input_class)\n",
    "    class_emb = AveragePooling2D(pool_size, data_format = 'channels_first')(class_emb)\n",
    "    class_emb = Reshape(page_embbed_shape, name=\"class_emb_out\")(class_emb)\n",
    "    ## input_query\n",
    "    query_emb = Embedding(input_dim = query_tokenizer.get_size(), output_dim = embbed_output_shape, input_length=max_page_seq, mask_zero = True)(input_query)\n",
    "    query_emb = AveragePooling2D(pool_size, data_format = 'channels_first')(query_emb)\n",
    "    query_emb = Reshape(page_embbed_shape, name=\"query_emb_out\")(query_emb)\n",
    "\n",
    "    input_tags = Concatenate()([class_emb, query_emb])\n",
    "#     input_tags = Concatenate()([class_emb, query_emb, input_tag_information])\n",
    "    input_tags_FFN = Dense(units = 256, activation = 'relu')(input_tags)\n",
    "    input_tags_FFN = Dense(units = 128, activation = 'relu', name=\"input_tag_FFN_out\")(input_tags_FFN)\n",
    "\n",
    "    ft_FFN = Dense(units = 256, activation = 'relu', name=\"ft_FFN_01\")(input_ft_embedding)\n",
    "    ft_FFN = Dense(units = 128, activation = 'relu', name=\"ft_FFN_out\")(ft_FFN)\n",
    "\n",
    "\n",
    "    merged = Concatenate()([ft_FFN, input_tags_FFN, input_tag_information])\n",
    "    model = Bidirectional(LSTM(units = HIDDEN_UNITS//2, return_sequences=True))(merged)\n",
    "    if use_crf:\n",
    "        crf=CRF(NUM_CLASS, name='crf_layer')\n",
    "        out =crf(model)\n",
    "    else:\n",
    "        out = Dense(units = NUM_CLASS, activation='softmax')(model)\n",
    "    model = Model([input_ft_embedding, input_tag_information, input_class, input_query], out)\n",
    "    if use_crf:\n",
    "        model.compile('adam',loss={'crf_layer': crf.get_loss})\n",
    "    else:\n",
    "        model.compile('adam',loss=tf.keras.losses.SparseCategoricalCrossentropy())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = get_custom_emb_model_colpooling(use_crf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 3s 984ms/step - loss: 603.2602 - val_loss: 439.5186\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 2s 808ms/step - loss: 252.2511 - val_loss: 253.4986\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 2s 802ms/step - loss: 134.6169 - val_loss: 101.8919\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 2s 802ms/step - loss: 58.4380 - val_loss: 60.1065\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 2s 822ms/step - loss: 43.0310 - val_loss: 57.0982\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 2s 813ms/step - loss: 41.8883 - val_loss: 52.5880\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 2s 801ms/step - loss: 38.1009 - val_loss: 44.1428\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 2s 808ms/step - loss: 31.6315 - val_loss: 36.6052\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 2s 823ms/step - loss: 26.6555 - val_loss: 34.6982\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 2s 812ms/step - loss: 24.0023 - val_loss: 38.2233\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 2s 816ms/step - loss: 23.3092 - val_loss: 36.6595\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 2s 805ms/step - loss: 20.1945 - val_loss: 30.6613\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 2s 806ms/step - loss: 17.7020 - val_loss: 25.9240\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 2s 822ms/step - loss: 16.4451 - val_loss: 23.6548\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 2s 793ms/step - loss: 15.7669 - val_loss: 23.0630\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 2s 794ms/step - loss: 15.1989 - val_loss: 23.7077\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 2s 811ms/step - loss: 14.7803 - val_loss: 24.4100\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 2s 789ms/step - loss: 14.4888 - val_loss: 24.5638\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 2s 798ms/step - loss: 14.0730 - val_loss: 23.3614\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 2s 804ms/step - loss: 13.6680 - val_loss: 22.6019\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 2s 811ms/step - loss: 13.2912 - val_loss: 21.7959\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 2s 829ms/step - loss: 12.9787 - val_loss: 21.6072\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 2s 801ms/step - loss: 12.7077 - val_loss: 21.7656\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 2s 812ms/step - loss: 12.4486 - val_loss: 22.2368\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 2s 827ms/step - loss: 12.1799 - val_loss: 22.5745\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 2s 805ms/step - loss: 11.9465 - val_loss: 22.5564\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 2s 822ms/step - loss: 11.7227 - val_loss: 22.4275\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 2s 810ms/step - loss: 11.5009 - val_loss: 21.8066\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 2s 792ms/step - loss: 11.2897 - val_loss: 21.4580\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 2s 806ms/step - loss: 11.0728 - val_loss: 21.8897\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 2s 810ms/step - loss: 10.8492 - val_loss: 21.9850\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 2s 795ms/step - loss: 10.6568 - val_loss: 21.6907\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 2s 811ms/step - loss: 10.4332 - val_loss: 21.9169\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 2s 797ms/step - loss: 10.2674 - val_loss: 22.5731\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 2s 831ms/step - loss: 10.0852 - val_loss: 21.7704\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 2s 812ms/step - loss: 9.8884 - val_loss: 21.8423\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 3s 843ms/step - loss: 9.6746 - val_loss: 21.3749\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 3s 962ms/step - loss: 9.5071 - val_loss: 21.2214\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 3s 957ms/step - loss: 9.3300 - val_loss: 20.8877\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 9.2053 - val_loss: 20.9137\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 3s 989ms/step - loss: 9.0828 - val_loss: 22.2196\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 8.9158 - val_loss: 20.7492\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 8.7573 - val_loss: 20.1481\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 8.6195 - val_loss: 21.6189\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 8.4940 - val_loss: 21.1612\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 8.3041 - val_loss: 19.9608\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 8.1998 - val_loss: 19.8056\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 8.0261 - val_loss: 20.9586\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 8.0041 - val_loss: 20.5431\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 7.8479 - val_loss: 18.4348\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 7.7863 - val_loss: 20.2036\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 7.5584 - val_loss: 22.7180\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 7.5814 - val_loss: 21.2024\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 7.3469 - val_loss: 19.0018\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 7.3542 - val_loss: 20.0653\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 7.1282 - val_loss: 23.2404\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 7.2858 - val_loss: 21.7660\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 6.9462 - val_loss: 20.2177\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 6.8143 - val_loss: 20.1240\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 6.7500 - val_loss: 20.7983\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 6.7449 - val_loss: 20.3917\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 6.5879 - val_loss: 19.1344\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 6.4458 - val_loss: 21.2878\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 6.3844 - val_loss: 21.1046\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 6.2480 - val_loss: 19.2483\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 6.1657 - val_loss: 20.5384\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 6.1253 - val_loss: 20.9933\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 5.9846 - val_loss: 19.3676\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 5.7478 - val_loss: 20.2588\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 5.7013 - val_loss: 20.7702\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 5.5059 - val_loss: 20.8026\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 5.4028 - val_loss: 22.5624\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 5.3502 - val_loss: 21.8560\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 5.2575 - val_loss: 21.3889\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 5.1188 - val_loss: 21.6074\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 5.0480 - val_loss: 21.3412\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 4.8877 - val_loss: 22.8059\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 4.9226 - val_loss: 22.9499\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 4.8324 - val_loss: 23.2710\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 4.7271 - val_loss: 22.1788\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 4.6105 - val_loss: 23.6983\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 4.5441 - val_loss: 23.4736\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 3s 1s/step - loss: 4.3856 - val_loss: 23.3377\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 4.2712 - val_loss: 22.0974\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 4.2302 - val_loss: 21.1252\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 4.2136 - val_loss: 23.7476\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 4.3218 - val_loss: 20.3932\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 3s 1s/step - loss: 4.2448 - val_loss: 23.9632\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 57s 19s/step - loss: 4.1060 - val_loss: 23.6546\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 2s 796ms/step - loss: 3.9700 - val_loss: 22.0642\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 2s 808ms/step - loss: 3.9874 - val_loss: 23.6333\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 2s 805ms/step - loss: 3.8990 - val_loss: 22.4345\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 2s 796ms/step - loss: 3.7597 - val_loss: 24.5762\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 2s 823ms/step - loss: 3.7446 - val_loss: 23.2073\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 2s 810ms/step - loss: 3.6147 - val_loss: 25.3795\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 2s 795ms/step - loss: 3.4879 - val_loss: 24.5003\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 2s 794ms/step - loss: 3.4924 - val_loss: 26.8148\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 2s 799ms/step - loss: 3.4638 - val_loss: 23.5858\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 2s 804ms/step - loss: 3.4544 - val_loss: 24.3196\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 2s 811ms/step - loss: 3.3072 - val_loss: 22.8150\n",
      "Training Finish, Best epoch: 50, Best Val_loss: 18.434776306152344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdd23b3c668>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_composite_with_token, train_y, batch_size=64, epochs=100, validation_split=0.1, verbose=1, callbacks=[bwCallback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_distribution_to_label(predict_y):\n",
    "    if len(predict_y.shape) != 3:\n",
    "        return predict_y\n",
    "    label_y = list()\n",
    "    for page in predict_y:\n",
    "        tmp = list()\n",
    "        for lab in page:\n",
    "            lab = lab.tolist()\n",
    "            tmp.append(lab.index(max(lab)))\n",
    "        label_y.append(tmp)\n",
    "    return label_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_data_wo_position(x, y):\n",
    "    new_tmp_x_array = []\n",
    "    new_tmp_y_array = []\n",
    "    for tmp_x, tmp_y in zip(x, y):\n",
    "        new_tmp_x_array.extend(chunks(tmp_x, max_page_seq))\n",
    "        new_tmp_y_array.extend(chunks(tmp_y, max_page_seq))\n",
    "    return new_tmp_x_array, new_tmp_y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_testing(test_X_raw, test_y_raw): #ft-bert\n",
    "    chunks_test_x, chunks_test_y = get_chunks_data_wo_position(test_X_raw, test_y_raw)\n",
    "    test_token_features, test_tag_features = get_token_tag_features_from_chunks(chunks_test_x)\n",
    "    \n",
    "    test_ptags_vector = get_ptags_vector(test_token_features)\n",
    "    test_ft_emb = pages_to_word_vector_from_keylist(ft, test_token_features, ['text-exact'])\n",
    "    test_tag_info_list = test_tag_features\n",
    "    ## Tokens prepare\n",
    "    test_pages_class, test_pages_query = prepare_input_ids(test_token_features, max_len)\n",
    "    test_class = token_pad_to_npdata(test_pages_class)\n",
    "    test_query = token_pad_to_npdata(test_pages_query)\n",
    "    test_ptags = token_pad_to_npdata(test_ptags_vector)\n",
    "    ## X_test_input\n",
    "    test_ft_emb_x = feature_pad_to_npdata(test_ft_emb)\n",
    "    test_tag_x = feature_pad_to_npdata(test_tag_info_list)\n",
    "    test_tag_x = np.concatenate([test_tag_x, test_ptags], axis = 2)\n",
    "    test_composite_input = [test_ft_emb_x, test_tag_x, test_class, test_query]\n",
    "    \n",
    "    ## y_test_input\n",
    "    y_test = pad_sequences([[tag2idx.get(l) for l in lab] for lab in chunks_test_y], maxlen=max_page_seq, padding=\"post\", truncating=\"post\", value=tag2idx[\"[PAD]\"])\n",
    "    y_test = [[idx2tag.get(lab) for lab in page] for page in y_test]\n",
    "    y_test = np.asarray(y_test)\n",
    "    \n",
    "    return test_composite_input, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready to testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_predict_and_evaluate(models, x_test, y_test, evaluate_labels):\n",
    "    for idx, model in enumerate(models):\n",
    "        print(f\"Start predict model {idx}\")\n",
    "#         print(model.summary())\n",
    "        print(\"--------------------------\")\n",
    "        predict_y = model.predict(x_test)\n",
    "        predict_y = label_distribution_to_label(predict_y)\n",
    "        predict_y = np.asarray([[idx2tag.get(lab) for lab in page] for page in predict_y])\n",
    "        print(flat_classification_report(y_test, predict_y, labels=evaluate_labels, digits=3))\n",
    "        print(\"--------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_languages = storage.get_all_test_languages()\n",
    "# test_languages = [\"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing language:  en\n",
      "pages: 49  domains: 34\n",
      "Transform key ['text-exact'] to word_vector ... \n",
      "Start predict model 0\n",
      "--------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PAGE      0.844     0.857     0.850       126\n",
      "        NEXT      0.000     0.000     0.000        29\n",
      "\n",
      "   micro avg      0.844     0.697     0.763       155\n",
      "   macro avg      0.422     0.429     0.425       155\n",
      "weighted avg      0.686     0.697     0.691       155\n",
      "\n",
      "--------------------------\n",
      "===================================\n",
      "Testing language:  zh\n",
      "pages: 44  domains: 19\n",
      "Transform key ['text-exact'] to word_vector ... \n",
      "Start predict model 0\n",
      "--------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PAGE      0.851     0.596     0.701       277\n",
      "        NEXT      1.000     0.083     0.154        24\n",
      "\n",
      "   micro avg      0.852     0.555     0.672       301\n",
      "   macro avg      0.925     0.340     0.427       301\n",
      "weighted avg      0.862     0.555     0.657       301\n",
      "\n",
      "--------------------------\n",
      "===================================\n",
      "Testing language:  ko\n",
      "pages: 24  domains: 13\n",
      "Transform key ['text-exact'] to word_vector ... \n",
      "Start predict model 0\n",
      "--------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PAGE      0.616     0.883     0.726        60\n",
      "        NEXT      0.000     0.000     0.000         5\n",
      "\n",
      "   micro avg      0.616     0.815     0.702        65\n",
      "   macro avg      0.308     0.442     0.363        65\n",
      "weighted avg      0.569     0.815     0.670        65\n",
      "\n",
      "--------------------------\n",
      "===================================\n",
      "Testing language:  ja\n",
      "pages: 23  domains: 9\n",
      "Transform key ['text-exact'] to word_vector ... \n",
      "Start predict model 0\n",
      "--------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PAGE      0.561     0.469     0.511        49\n",
      "        NEXT      0.000     0.000     0.000        11\n",
      "\n",
      "   micro avg      0.561     0.383     0.455        60\n",
      "   macro avg      0.280     0.235     0.256        60\n",
      "weighted avg      0.458     0.383     0.417        60\n",
      "\n",
      "--------------------------\n",
      "===================================\n",
      "Testing language:  de\n",
      "pages: 20  domains: 7\n",
      "Transform key ['text-exact'] to word_vector ... \n",
      "Start predict model 0\n",
      "--------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PAGE      0.919     0.734     0.816       139\n",
      "        NEXT      0.000     0.000     0.000        22\n",
      "\n",
      "   micro avg      0.919     0.634     0.750       161\n",
      "   macro avg      0.459     0.367     0.408       161\n",
      "weighted avg      0.793     0.634     0.704       161\n",
      "\n",
      "--------------------------\n",
      "===================================\n",
      "Testing language:  ru\n",
      "pages: 21  domains: 14\n",
      "Transform key ['text-exact'] to word_vector ... \n",
      "Start predict model 0\n",
      "--------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PAGE      0.771     0.730     0.750        37\n",
      "        NEXT      0.000     0.000     0.000         7\n",
      "\n",
      "   micro avg      0.771     0.614     0.684        44\n",
      "   macro avg      0.386     0.365     0.375        44\n",
      "weighted avg      0.649     0.614     0.631        44\n",
      "\n",
      "--------------------------\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "TEST_MODELS = [model]\n",
    "test_languages = storage.get_all_test_languages()\n",
    "for language in test_languages:\n",
    "    print(\"Testing language: \", language)\n",
    "    test_urls = [rec['Page URL'] for rec in storage.iter_test_records_by_language(language=language)]\n",
    "    test_X_raw, test_y = storage.get_test_Xy_by_language(language=language)\n",
    "    print(\"pages: {}  domains: {}\".format(len(test_urls), len({get_domain(url) for url in test_urls})))\n",
    "    _test_x, _test_y = prepare_for_testing(test_X_raw, test_y)\n",
    "    recursive_predict_and_evaluate(TEST_MODELS, _test_x, _test_y, ['PAGE','NEXT'])\n",
    "    print(\"===================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
