{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRNSMRework"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Log level: Default:0 (ALL), 1 (INFO), 2 (WARNING), 3 (ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\"\n",
    "import logging\n",
    "import random\n",
    "import sys\n",
    "from urllib.parse import parse_qs, parse_qsl, urlparse, urlsplit, unquote\n",
    "from itertools import islice\n",
    "from typing import Any\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import parsel\n",
    "from sklearn_crfsuite.metrics import (flat_classification_report,\n",
    "                                      sequence_accuracy_score)\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Import autopager\n",
    "sys.path.insert(0, '..')\n",
    "from autopager import AUTOPAGER_LIMITS\n",
    "from autopager.htmlutils import (get_link_href, get_link_text,\n",
    "                                 get_selector_root,\n",
    "                                 get_text_around_selector_list)\n",
    "from autopager.model import _elem_attr, _num_tokens_feature\n",
    "from autopager.parserutils import (MyHTMLParser, TagParser, compare_tag,\n",
    "                                   draw_scaled_page, get_first_tag,\n",
    "                                   position_check)\n",
    "from autopager.storage import Storage\n",
    "from autopager.utils import (get_domain, ngrams, ngrams_wb, normalize,\n",
    "                             normalize_whitespaces, replace_digits, tokenize)\n",
    "\n",
    "parser = MyHTMLParser()\n",
    "tagParser = TagParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow_addons.layers.crf import CRF\n",
    "import tensorflow_text as tf_text\n",
    "from keras import backend as K\n",
    "# mixed_precision = tf.keras.mixed_precision\n",
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "# mixed_precision.set_global_policy(policy)\n",
    "\n",
    "layers = tf.keras.layers\n",
    "pad_sequences = tf.keras.preprocessing.sequence.pad_sequences\n",
    "Model = tf.keras.Model\n",
    "Dataset = tf.data.Dataset\n",
    "Dense = tf.keras.layers.Dense\n",
    "Input = tf.keras.layers.Input\n",
    "Bidirectional = tf.keras.layers.Bidirectional\n",
    "LSTM = tf.keras.layers.LSTM\n",
    "Embedding = tf.keras.layers.Embedding\n",
    "Masking = tf.keras.layers.Masking\n",
    "Concatenate = tf.keras.layers.Concatenate\n",
    "AveragePooling2D = tf.keras.layers.AveragePooling2D\n",
    "MaxPooling2D = tf.keras.layers.MaxPooling2D\n",
    "MaxPooling3D = tf.keras.layers.MaxPooling3D\n",
    "Conv2D = tf.keras.layers.Conv2D\n",
    "Conv1D = tf.keras.layers.Conv1D\n",
    "Reshape = tf.keras.layers.Reshape\n",
    "Attention = tf.keras.layers.Attention\n",
    "GlobalAveragePooling1D = tf.keras.layers.GlobalAveragePooling1D\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PAGE_SEQ = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use argparse\n",
    "USED_GPU = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow\n",
    "tf.__version__\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if len(gpus)!=0:\n",
    "    for device in gpus:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[USED_GPU], 'GPU')\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"Tensorflow: No GPUs visible\")\n",
    "    \n",
    "# Pytorch\n",
    "import torch\n",
    "\n",
    "if(torch.cuda.is_available() and torch.cuda.device_count() > USED_GPU):\n",
    "    torch.cuda.set_device(USED_GPU)\n",
    "    torch.cuda.current_device()\n",
    "    torch.cuda.device(USED_GPU)\n",
    "else:\n",
    "    print(\"PyTorch: No GPUs visible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current test file:  ['en', 'zh', 'ko', 'ja', 'de', 'ru', 'test', 'event']\n"
     ]
    }
   ],
   "source": [
    "storage = Storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pages: 164  domains: 55\n"
     ]
    }
   ],
   "source": [
    "urls = [rec['Page URL'] for rec in storage.iter_records(language='en',contain_button = True, file_type='T')]\n",
    "# X_raw: <Selector xpath='.//a|.//button' data='<a href=\"https://www.oneplus.com\"><im...'>,\n",
    "# The a and button that is not yet extracted\n",
    "X_raw: list[parsel.selector.SelectorList]\n",
    "# y: ['O',  'PAGE', 'O', 'PAGE', 'PAGE', 'PAGE', 'PAGE', 'PAGE', 'O', 'PAGE', 'NEXT', 'O']\n",
    "y: list[str]\n",
    "X_raw, y, page_positions = storage.get_Xy(language='en', contain_button = True,  contain_position=True,file_type='T', scaled_page='normal')\n",
    "print(\"pages: {}  domains: {}\".format(len(urls), len({get_domain(url) for url in urls})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token feature and Tag feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX: these functions should be copy-pasted from autopager/model.py\n",
    "\n",
    "def _as_list(generator, limit=None) -> list:\n",
    "    \"\"\"\n",
    "    >>> _as_list(ngrams_wb(\"text\", 2, 2), 0)\n",
    "    []\n",
    "    >>> _as_list(ngrams_wb(\"text\", 2, 2), 2)\n",
    "    ['te', 'ex']\n",
    "    >>> _as_list(ngrams_wb(\"text\", 2, 2))\n",
    "    ['te', 'ex', 'xt']\n",
    "    \"\"\"\n",
    "    return list(generator if limit is None else islice(generator, 0, limit))\n",
    "\n",
    "def feat_to_tokens(feat, tokenizer):\n",
    "    if type(feat) == type([]):\n",
    "        feat = ' '.join(feat)\n",
    "    tokens = tokenizer.tokenize(feat)\n",
    "    return tokens\n",
    "\n",
    "def num_token_feature_to_class(number):\n",
    "    if number == '=0':\n",
    "        return [1, 0, 0, 0]\n",
    "    elif number == '=1':\n",
    "        return [0, 1, 0, 0]\n",
    "    elif number == '=2':\n",
    "        return [0, 0, 1, 0]\n",
    "    else:\n",
    "        return [0, 0, 0, 1]\n",
    "\n",
    "def link_to_features(link: parsel.Selector):\n",
    "    # Get text contecnt of the link otherwise alt or img.\n",
    "    # Normalize multiple white space to one and to lowercase.\n",
    "    text = normalize(get_link_text(link))\n",
    "    href = get_link_href(link)\n",
    "    if href is None:\n",
    "        href = \"\"\n",
    "    p = urlsplit(href)\n",
    "    parent = link.xpath('..').extract()\n",
    "    # Retrive the line of first tag opening\n",
    "    parent = get_first_tag(parser, parent[0])\n",
    "    query_parsed = parse_qsl(p.query) #parse query string from path\n",
    "    query_param_names = [k.lower() for k, v in query_parsed]\n",
    "    query_param_names_ngrams = _as_list(ngrams_wb(\n",
    "        \" \".join([normalize(name) for name in query_param_names]), 3, 5, True\n",
    "    ))\n",
    "\n",
    "    # Classes of link itself and all its children.\n",
    "    # It is common to have e.g. span elements with fontawesome\n",
    "    # arrow icon classes inside <a> links.\n",
    "    self_and_children_classes = ' '.join(link.xpath(\".//@class\").extract())\n",
    "    parent_classes = ' '.join(link.xpath('../@class').extract())\n",
    "    css_classes = normalize(parent_classes + ' ' + self_and_children_classes)\n",
    "    \n",
    "    token_feature = {\n",
    "        'text-exact': replace_digits(text.strip()[:100].strip()),\n",
    "        # <scheme>://<netloc>/<path>?<query>#<fragment>\n",
    "        'url': href,\n",
    "        'parent-tag': parent,\n",
    "        'class':_as_list(ngrams_wb(css_classes, 4, 5),\n",
    "                          AUTOPAGER_LIMITS.max_css_features),\n",
    "        'text': _as_list(ngrams_wb(replace_digits(text), 2, 5),\n",
    "                         AUTOPAGER_LIMITS.max_text_features),\n",
    "    }\n",
    "    tag_feature = {\n",
    "        'isdigit': 1 if text.isdigit() is True else 0,\n",
    "        'isalpha': 1 if text.isalpha() is True else 0,\n",
    "        'has-href': 0 if href == \"\" else 1,\n",
    "        'path-has-page': 1 if 'page' in p.path.lower() else 0,\n",
    "        'path-has-pageXX': 1 if re.search(r'[/-](?:p|page\\w?)/?\\d+', p.path.lower()) is not None else 0,\n",
    "        'path-has-number': 1 if any(part.isdigit() for part in p.path.split('/')) else 0,\n",
    "        'href-has-year': 1 if re.search('20\\d\\d', href) is not None else 0,\n",
    "        'class-has-disabled': 1 if 'disabled' in css_classes else 0,\n",
    "    }\n",
    "    non_token_feature = []\n",
    "    for k,v in tag_feature.items():\n",
    "        if type(v) == type([]):\n",
    "            non_token_feature.extend(v)\n",
    "        else:\n",
    "            non_token_feature.append(v)\n",
    "\n",
    "    # print(token_feature)\n",
    "\n",
    "    return [token_feature, non_token_feature]\n",
    "\n",
    "\n",
    "def page_to_features(xseq):\n",
    "    feat_list = [link_to_features(a) for a in xseq]\n",
    "    around = get_text_around_selector_list(xseq, max_length=15)\n",
    "    # Append sibling's text-exact to each node's text-full.\n",
    "    for feat, (before, after) in zip(feat_list, around):\n",
    "        feat[0]['text-full'] = normalize(before) + ',' + feat[0]['text-exact'] + ',' + normalize(after)\n",
    "    \n",
    "    return feat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_tag_features_from_chunks(chunks):\n",
    "    token_features = []\n",
    "    tag_features = []\n",
    "    for idx, page in enumerate(chunks):\n",
    "        try:\n",
    "            feat_list = page_to_features(page)\n",
    "            token_features.append([node[0] for node in feat_list])\n",
    "            tag_features.append(np.array([node[1] for node in feat_list]))\n",
    "        except:\n",
    "            raise Exception(f\"Error occured on {idx}\")\n",
    "    return token_features, tag_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_features: list[list[dict[str, Any]]]\n",
    "token_features, tag_features = get_token_tag_features_from_chunks(X_raw)\n",
    "# ['text-exact', 'query', 'parent-tag', 'class', 'text', 'text-full']\n",
    "token_feature_titles: list[str] = list(token_features[0][0].keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare parent tag feature\n",
    "From top 30 parent tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_representation_with_map(tag, data_map):\n",
    "    # Vector length is the number of tags in the map(30).\n",
    "    rt_vec = [0] * len(data_map)\n",
    "    for idx, map_tag in enumerate(data_map):\n",
    "        # ('tag_name', count)\n",
    "        if tag == map_tag[0]:\n",
    "            rt_vec[idx] = 1\n",
    "            break\n",
    "    return rt_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ptags_vector(token_features, data_map_for_ptag):\n",
    "    pages_ptag = []\n",
    "    for page in token_features:\n",
    "        ptag_page = []\n",
    "        for node in page:\n",
    "            p_tag = node['parent-tag']\n",
    "            ptag_page.append(sparse_representation_with_map(p_tag, data_map_for_ptag))\n",
    "        pages_ptag.append(np.array(ptag_page))\n",
    "    return pages_ptag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_parent_tags = {}\n",
    "\n",
    "for page in token_features:\n",
    "    for node in page:\n",
    "        p_tag = node['parent-tag']\n",
    "        if p_tag not in top_parent_tags:\n",
    "            top_parent_tags[p_tag] = 1\n",
    "        else:\n",
    "            top_parent_tags[p_tag] += 1\n",
    "sorted_parent_tags = sorted(top_parent_tags.items(),key=lambda x:x[1],reverse=True)\n",
    "\n",
    "data_map_for_ptag = sorted_parent_tags[:30]\n",
    "#Get parent tag vector\n",
    "ptags_vector = get_ptags_vector(token_features, data_map_for_ptag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Class, Query tokens by tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TagTokenizer:\n",
    "    def __init__(self, tag_name_count=None):\n",
    "        rt_dict = {}\n",
    "        rt_dict[\"[PAD]\"] = 0\n",
    "        rt_dict[\"[UNK]\"] = 1\n",
    "        if tag_name_count is not None:\n",
    "            for k in tag_name_count.keys():\n",
    "                rt_dict[k] = len(rt_dict)\n",
    "        self.map = rt_dict\n",
    "\n",
    "    def tokenize(self, word):\n",
    "        if isinstance(word, list):\n",
    "            token_list = []\n",
    "            for _word in word:\n",
    "                if _word not in self.map:\n",
    "                    token_list.append(self.map[\"[UNK]\"])\n",
    "                else:\n",
    "                    token_list.append(self.map[_word])\n",
    "            return token_list\n",
    "        else:\n",
    "            if word not in self.map:\n",
    "                return self.map[\"[UNK]\"]\n",
    "            else:\n",
    "                return self.map[word]\n",
    "\n",
    "    def get_size(self):\n",
    "        return len(self.map)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare class and query features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_thousand_class = {}\n",
    "\n",
    "for page in token_features:\n",
    "    for node in page:\n",
    "        for class_name in node['class']:\n",
    "            top_thousand_class[class_name] = top_thousand_class.get(class_name, 0) + 1\n",
    "          \n",
    "class_tokenizer = TagTokenizer(top_thousand_class)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get pre-trained sentence(text content) embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Pre-trained Laser model and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from LaserSentenceModel import LaserSentenceModel\n",
    "laser = LaserSentenceModel()\n",
    "laser.getSentenceVector('hello').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_vector(word_list, word_vector_method = None):\n",
    "    if word_vector_method is None:\n",
    "        print(\"Need to specified a method.\")\n",
    "        return\n",
    "    elif word_vector_method == 'FastText':\n",
    "        from FastTextModel import FastTextModel\n",
    "        ft = FastTextModel()\n",
    "        if type(word_list) == type([]):\n",
    "            if len(word_list) == 0:\n",
    "                return np.zeros(ft.getModel().get_dimension())\n",
    "            else:\n",
    "                vectors_array = []\n",
    "                for word in word_list:\n",
    "                    vector = ft.getWordVector(word)\n",
    "                    vectors_array.append(vector)\n",
    "                mean_vector = np.mean(vectors_array, axis = 0)\n",
    "                return mean_vector\n",
    "        else:\n",
    "            return ft.getWordVector(word_list)\n",
    "    elif word_vector_method == 'Laser':\n",
    "        return laser.getSentenceVector(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pages_to_word_vector_from_keylist(word_vector_method, token_features, word_to_vec_list):\n",
    "    print(f\"Transform key {word_to_vec_list} to word_vector ... useing {word_vector_method}\")\n",
    "    pages_vector = []\n",
    "    p = IntProgress(max=len(token_features))\n",
    "    p.description = '(Init)'\n",
    "    p.value = 0\n",
    "    display(p)\n",
    "    for idx, page in enumerate(token_features):\n",
    "        p.description = f\"Task: {idx+1}\"\n",
    "        p.value = idx+1\n",
    "        page_vectors = []\n",
    "        for node in page:\n",
    "            full_vector_list = []\n",
    "            for k,v in node.items():\n",
    "                if k in word_to_vec_list:\n",
    "                    # if word_to_vec_list is 'text-full'\n",
    "                    # v is str, returns laser.embed_sentences(sents, lang=self.lang)[0]\n",
    "                    full_vector_list.append(word_to_vector(v, word_vector_method))\n",
    "            full_vector = np.concatenate(full_vector_list, axis=0)\n",
    "            page_vectors.append(full_vector)\n",
    "        pages_vector.append(np.array(page_vectors))\n",
    "    p.description = '(Done)'\n",
    "    return pages_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('embedding/train/LaserEmb_full.npy'):\n",
    "    laser_full_tokens_emb = np.load('embedding/train/LaserEmb_full.npy', allow_pickle=True)\n",
    "else:\n",
    "    laser_full_tokens_emb = pages_to_word_vector_from_keylist('Laser', token_features, ['text-full'])\n",
    "    np.save('embedding/train/LaserEmb_full.npy', laser_full_tokens_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding to fixed size and prepare for training inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_ids(page_tokens, max_len):\n",
    "    pages_class = []\n",
    "    for page in page_tokens:\n",
    "        class_page = []\n",
    "        for node in page:\n",
    "            #class\n",
    "            class_ids = class_tokenizer.tokenize(node['class'])\n",
    "            class_ids = class_ids + [0] * (max_len-len(class_ids))\n",
    "            class_page.append(class_ids[:max_len])\n",
    "        pages_class.append(np.array(class_page))\n",
    "    return pages_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag_info_list = tag_features #features which only have tag true/false information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 256\n",
    "train_attr_x = laser_full_tokens_emb\n",
    "train_ptag: list = ptags_vector\n",
    "pages_class: list[np.ndarray] = prepare_input_ids(token_features, max_len=max_len)\n",
    "pages_url: list[np.ndarray] = [np.array([node['url'] for node in page], dtype=object) for page in token_features]\n",
    "train_tag_x = tag_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_composite_with_token = [train_attr_x, train_ptag, pages_class, pages_url, train_tag_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"O\", \"PREV\", \"PAGE\", \"NEXT\"]\n",
    "tag2idx = {label: idx for idx, label in enumerate(labels)}\n",
    "idx2tag = {idx: label for idx, label in enumerate(labels)}\n",
    "num_tags = len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = [np.array([tag2idx.get(l) for l in lab]) for lab in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 1024)\n",
      "(303, 30)\n",
      "(303, 256)\n",
      "(303,)\n",
      "(303, 8)\n"
     ]
    }
   ],
   "source": [
    "for inputs in train_composite_with_token:\n",
    "    print(inputs[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit word and char tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 序列長度\n",
    "MAX_URL_CHAR_LEN = 1024\n",
    "MAX_URL_WORD_LEN = 128\n",
    "\n",
    "# 種類\n",
    "MAX_URL_CHAR_TOKEN = 256\n",
    "MAX_URL_WORD_TOKEN = 20000\n",
    "\n",
    "\n",
    "def word_spliter(url: tf.Tensor):\n",
    "    # May be .lower()?\n",
    "    return tf_text.regex_split(url, r\"\\/|&|\\?|#|\\.|://|=|-|[\\ ]\")\n",
    "\n",
    "\n",
    "def char_spliter(url: tf.Tensor):\n",
    "    return tf.strings.unicode_split(url, \"UTF-8\")\n",
    "    # return tf.strings.bytes_split(url)\n",
    "\n",
    "\n",
    "char_vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    standardize=\"lower\",  # Do not remove punchuation!!\n",
    "    split=char_spliter,\n",
    "    max_tokens=MAX_URL_CHAR_TOKEN,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=MAX_URL_CHAR_LEN,\n",
    ")\n",
    "\n",
    "word_vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    standardize=\"lower\",  # Do not remove punchuation!!\n",
    "    split=word_spliter,\n",
    "    max_tokens=MAX_URL_WORD_TOKEN,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=MAX_URL_WORD_LEN,\n",
    ")\n",
    "\n",
    "# Smaller batch?\n",
    "url_dataset = Dataset.from_tensor_slices([unquote(node['url']) for page in token_features for node in page])\n",
    "char_vectorize_layer.adapt(url_dataset.batch(64))\n",
    "word_vectorize_layer.adapt(url_dataset.batch(64))\n",
    "\n",
    "# pages_url_char = [[char_vectorize_layer(node['url']) for node in page] for page in token_features[:1]]\n",
    "# pages_url_word: list[np.ndarray] = [np.array([word_vectorize_layer(node['url']) for node in page], dtype=str) for page in token_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_emb_model(use_crf=True, embedding_size=32, hidden_size=300):\n",
    "    HIDDEN_UNITS = hidden_size\n",
    "    NUM_CLASS = num_tags\n",
    "    EMBEDDING_SIZE = embedding_size\n",
    "    CLS_EMB_SIZE = 256\n",
    "    CONV_FILTERS = 64 # Recommand 256\n",
    "    CLASS_CONV_FILTERS = 8\n",
    "\n",
    "    # Since the tensor may be ragged, we use None to represent the dimention of sequence length\n",
    "    laser_emb_shape = (None, 1024)  # Laser embedding\n",
    "    tag_info_shape = (None, 8)  # tag_freatures from autopager\n",
    "    class_emb_shape = (None, 256)  # Class and Query embedding Shape\n",
    "    ptag_emb_shape = (None, 30)  # Parent tag embedding shape\n",
    "    page_embbed_shape = (-1, EMBEDDING_SIZE)\n",
    "\n",
    "    filter_sizes = [3, 4, 5, 6]\n",
    "\n",
    "    input_ft_embedding = Input(\n",
    "        shape=laser_emb_shape, name=\"input_ft_embeddings\"\n",
    "    )\n",
    "    input_ptag_emb = Input(shape=(ptag_emb_shape), name=\"input_ptag_embedding\")\n",
    "    input_class = Input(shape=class_emb_shape, name=\"input_class\")\n",
    "    \n",
    "    ##\n",
    "    input_url = Input(shape=(None,), dtype=tf.string, name=\"input_url\")\n",
    "    input_url = Reshape([-1, 1])(input_url)\n",
    "    ##\n",
    "    \n",
    "    input_tag_information = Input(\n",
    "        shape=(tag_info_shape), name=\"input_tag_information\"\n",
    "    )\n",
    "\n",
    "    ### CLASS EMBEDDING ###\n",
    "    # TODO: Find proper Conv2D size and Embedding size\n",
    "    class_emb = input_class\n",
    "    class_emb = K.expand_dims(class_emb, axis=-1)\n",
    "    # [batch_size, seq_len, 256, 1]\n",
    "    class_emb = Conv1D(\n",
    "        filters=CLASS_CONV_FILTERS,\n",
    "        kernel_size=[CLS_EMB_SIZE],\n",
    "        strides=1,\n",
    "        padding=\"valid\",\n",
    "        data_format=\"channels_last\",\n",
    "        activation=\"relu\",\n",
    "        name='Conv_class'\n",
    "    )(class_emb)\n",
    "    class_emb = Reshape((-1, CLASS_CONV_FILTERS), name=\"class_emb_out\")(class_emb)\n",
    "\n",
    "    ##### URL EMBEDDING #####\n",
    "    ### CHAR CONVOLUTION LAYER ###\n",
    "    # url_char_emb = Reshape(target_shape=(MAX_URL_CHAR_LEN,))(input_url)\n",
    "    # print(K.int_shape(input_url))\n",
    "    url_char_emb = char_vectorize_layer(input_url)\n",
    "    # print(K.int_shape(url_char_emb))\n",
    "    # url_char_emb = K.squeeze(url_char_emb, axis = -1)\n",
    "    url_char_emb = Embedding(\n",
    "        input_dim=MAX_URL_CHAR_TOKEN,\n",
    "        output_dim=EMBEDDING_SIZE,\n",
    "        input_length=MAX_URL_CHAR_LEN,\n",
    "        mask_zero=True,\n",
    "    )(url_char_emb)\n",
    "    # print(K.int_shape(url_char_emb))\n",
    "    url_char_emb = K.expand_dims(url_char_emb, axis=-1)\n",
    "    \n",
    "    pooled_char_x = []\n",
    "    for i, filter_size in enumerate(filter_sizes):\n",
    "        # print(\"Before conv\", K.int_shape(url_char_emb))\n",
    "        url_char_emb_conved = Conv2D(\n",
    "            filters=CONV_FILTERS,\n",
    "            kernel_size=[filter_size, EMBEDDING_SIZE],\n",
    "            strides=(1, 1),\n",
    "            padding=\"valid\",\n",
    "            data_format=\"channels_last\",\n",
    "            activation=\"relu\",\n",
    "            name=f'Conv_char_{i}'\n",
    "        )(url_char_emb)\n",
    "        # print(\"Before pool\", K.int_shape(url_char_emb_conved))\n",
    "        url_char_emb_pooled = MaxPooling3D(\n",
    "            pool_size=(1, MAX_URL_CHAR_LEN - filter_size + 1, 1),\n",
    "            strides=(1, 1, 1),\n",
    "            padding=\"valid\",\n",
    "            data_format=\"channels_last\",\n",
    "            name=f'MaxPooling_char_{i}'\n",
    "        )(url_char_emb_conved)\n",
    "        # print(\"After pool\", K.int_shape(url_char_emb_pooled))\n",
    "        pooled_char_x.append(url_char_emb_pooled)\n",
    "\n",
    "    num_filters_total = CONV_FILTERS * len(filter_sizes)\n",
    "    # print(K.int_shape(pooled_char_x[0]))\n",
    "    \n",
    "    url_char_emb = Concatenate(axis=2, name='Concat_pooled_char')(pooled_char_x)\n",
    "    url_char_emb = Reshape(target_shape=(-1, num_filters_total))(url_char_emb)\n",
    "\n",
    "    char_output = Dense(units=512, activation=\"relu\")(\n",
    "        url_char_emb\n",
    "    )  # (num_filters_total, 512)\n",
    "\n",
    "    #### WORD CONVOLUTION LAYER ###\n",
    "    url_word_emb = word_vectorize_layer(input_url)\n",
    "    # print(K.int_shape(url_word_emb))\n",
    "    # url_char_emb = K.squeeze(url_word_emb, axis = -1)\n",
    "    url_word_emb = Embedding(\n",
    "        input_dim=MAX_URL_WORD_TOKEN,\n",
    "        output_dim=EMBEDDING_SIZE,\n",
    "        input_length=MAX_URL_WORD_LEN,\n",
    "        mask_zero=True,\n",
    "    )(url_word_emb)\n",
    "    url_word_emb = K.expand_dims(url_word_emb, axis=-1)\n",
    "    pooled_word_x = []\n",
    "    for i, filter_size in enumerate(filter_sizes):\n",
    "        # [filter_height, filter_width, in_channels, out_channels]\n",
    "        # filter_shape = [filter_size, embedding_size, 1, 256]\n",
    "        url_word_emb_conved = Conv2D(\n",
    "            filters=CONV_FILTERS,\n",
    "            kernel_size=[filter_size, EMBEDDING_SIZE],\n",
    "            padding=\"valid\",\n",
    "            data_format=\"channels_last\",\n",
    "            activation=\"relu\",\n",
    "            name=f'Conv_word_{i}'\n",
    "        )(url_word_emb)\n",
    "        url_word_emb_pooled = MaxPooling3D(\n",
    "            pool_size=(1, MAX_URL_WORD_LEN - filter_size + 1, 1),\n",
    "            strides=(1, 1, 1),\n",
    "            padding=\"valid\",\n",
    "            data_format=\"channels_last\",\n",
    "            name=f'MaxPooling_word_{i}'\n",
    "        )(url_word_emb_conved)\n",
    "        pooled_word_x.append(url_word_emb_pooled)\n",
    "    \n",
    "    num_filters_total = CONV_FILTERS * len(filter_sizes)\n",
    "    url_word_emb = Concatenate(axis=2, name='Concat_pooled_word')(pooled_word_x)\n",
    "    url_word_emb = Reshape(target_shape=(-1, num_filters_total))(url_word_emb)\n",
    "    # url_word_emb = Dropout(.2)(url_word_emb)\n",
    "    word_output = Dense(units=512, activation=\"relu\")(\n",
    "        url_word_emb\n",
    "    )  # (num_filters_total)\n",
    "\n",
    "    ############################### CONCAT WORD AND CHAR BRANCH ############################\n",
    "    conv_output = Concatenate(axis=2, name='Concat_word_char')([word_output, char_output])\n",
    "    conv_output = Dense(units=512, activation=\"relu\")(conv_output)\n",
    "    conv_output = Dense(units=256, activation=\"relu\")(conv_output)\n",
    "    url_emb = Dense(units=128, activation=\"relu\")(conv_output)\n",
    "\n",
    "    # ft_FFN = Dense(units=512, activation=\"relu\", name=\"ft_FFN_01\")(\n",
    "    #     input_ft_embedding\n",
    "    # )\n",
    "    # ft_FFN = Dense(units=256, activation=\"relu\", name=\"ft_FFN_02\")(ft_FFN)\n",
    "    # ft_FFN = Dense(units=128, activation=\"relu\", name=\"ft_FFN_out\")(ft_FFN)\n",
    "\n",
    "    # FFN for ptag\n",
    "    # ptag_FFN = Dense(units = 128, activation = 'relu', name=\"ptag_FFN_01\")(input_ptag_vector)\n",
    "    # ptag_FFN = Dense(units = 64, activation = 'relu', name=\"ptag_FFN_out\")(ptag_FFN)\n",
    "\n",
    "    ############################### LSTM+CRF ############################\n",
    "    merged = Concatenate(name='Concat_all_freatures')(\n",
    "        [\n",
    "            input_ft_embedding,\n",
    "            input_ptag_emb,\n",
    "            class_emb,\n",
    "            url_emb,\n",
    "            input_tag_information,\n",
    "        ]\n",
    "        \n",
    "    )\n",
    "    model = Bidirectional(LSTM(units=HIDDEN_UNITS // 2, return_sequences=True))(\n",
    "        merged\n",
    "    )\n",
    "\n",
    "    crf = CRF(NUM_CLASS, name=\"crf_layer\")\n",
    "    out = crf(model)\n",
    "    loss_fn = crf.get_loss\n",
    "    # [train_attr_x, train_ptag, pages_class, pages_url, train_tag_x]\n",
    "    model = Model(\n",
    "        [\n",
    "            input_ft_embedding,\n",
    "            input_ptag_emb,\n",
    "            input_class,\n",
    "            input_url,\n",
    "            input_tag_information,\n",
    "        ],\n",
    "        out,\n",
    "    )\n",
    "\n",
    "    return model, loss_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train/val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def composite_train_test_split(composite_x, y, number):\n",
    "    x_train = [data[:-number] for data in composite_x]\n",
    "    y_train = y[:-number]\n",
    "    x_val = [data[-number:] for data in composite_x]\n",
    "    y_val = y[-number:]\n",
    "    return x_train, y_train, x_val, y_val\n",
    "\n",
    "def list_to_dataSet(data, dataType):\n",
    "    # return tf.convert_to_tensor(data)\n",
    "    dataset = Dataset.from_generator(lambda: iter(data), dataType)\n",
    "    return dataset\n",
    "\n",
    "def data_list_to_dataset(x, y, isValidation = False, batch_size = 1):\n",
    "    all_data = Dataset.zip(tuple([list_to_dataSet(feature_type, feature_type[0].dtype) for feature_type in x]))\n",
    "    y_ds = list_to_dataSet(y, tf.int32)\n",
    "    final_set = Dataset.zip((all_data, y_ds))\n",
    "    if not isValidation:\n",
    "        final_set = final_set.shuffle(buffer_size=1024).batch(batch_size)\n",
    "    else:\n",
    "        final_set = final_set.batch(batch_size)\n",
    "    return final_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val = composite_train_test_split(train_composite_with_token, train_y, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def composite_list_to_dataset(x, batch_size = 1):\n",
    "    \"\"\"\n",
    "    x: [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]\n",
    "    output: [[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]]\n",
    "    batch_size = 2\n",
    "    output: [[[1, 5, 9], [2, 6, 10]], [[3, 7, 11], [4, 8, 12]]\n",
    "    \"\"\"\n",
    "    all_data = Dataset.zip(tuple([list_to_dataSet(data, tf.float32) for data in x]))\n",
    "    return all_data.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data_list_to_dataset(x_train, y_train, isValidation=False)\n",
    "val_dataset = data_list_to_dataset(x_val, y_val, isValidation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train[3][0].shape)\n",
    "it = iter(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "(1, 82)\n"
     ]
    }
   ],
   "source": [
    "print(type(next(it)))\n",
    "print(next(it)[0][3].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Custom Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate training/val f1-score\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def calculate_pages_metric(y_true_pages, y_predict_pages):\n",
    "    pages_f1 = []\n",
    "    nexts_f1 = []\n",
    "    avg_f1 = []\n",
    "    for y_true, y_predict in zip(y_true_pages, y_predict_pages):\n",
    "        if len(y_true) == 0:\n",
    "            break\n",
    "        report = classification_report(\n",
    "            y_true, y_predict, output_dict=True, zero_division=1\n",
    "        )\n",
    "        #         print(report)\n",
    "        PAGE = report[\"2\"][\"f1-score\"]\n",
    "        NEXT = report[\"3\"][\"f1-score\"]\n",
    "        pages_f1.append(PAGE)\n",
    "        nexts_f1.append(NEXT)\n",
    "        avg_f1.append((PAGE + NEXT) / 2)\n",
    "    return pages_f1, nexts_f1, avg_f1\n",
    "\n",
    "\n",
    "def calculate_page_metric(y_true, y_predict):\n",
    "    # 下面以對zero_division做處理\n",
    "    report = classification_report(\n",
    "        y_true, y_predict, labels=[0, 2, 3], output_dict=True, zero_division=0\n",
    "    )\n",
    "    OTHER = report[\"0\"][\"f1-score\"]\n",
    "    PAGE = report[\"2\"][\"f1-score\"]\n",
    "    NEXT = report[\"3\"][\"f1-score\"]\n",
    "    if 2 in y_true and 3 in y_true:\n",
    "        AVG = (PAGE + NEXT) / 2\n",
    "    elif 2 in y_true and 3 not in y_true:\n",
    "        AVG = PAGE\n",
    "    elif 2 not in y_true and 3 in y_true:\n",
    "        AVG = NEXT\n",
    "    else:\n",
    "        AVG = OTHER\n",
    "    return AVG\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for 1 data predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for (batch_x, batch_y) in train_dataset.take(1):\n",
    "#     batch_predict_y = model(batch_x).numpy()\n",
    "#     batch_true_y = batch_y.numpy()\n",
    "#     print(batch_true_y)\n",
    "#     print(batch_predict_y)\n",
    "#     print(calculate_page_metric(batch_true_y[0], batch_predict_y[0]))\n",
    "#     print(classification_report(batch_true_y[0], batch_predict_y[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_epoch(epochs, model, optimizer, train_dataset, val_dataset, best_model_method = 'f1-score'):\n",
    "    import time\n",
    "    \n",
    "    epochs = epochs\n",
    "    best_weights = None\n",
    "    best_f1_weights = None\n",
    "    best = np.Inf\n",
    "    best_loss_history = None\n",
    "    best_f1 = 0\n",
    "    best_f1_history = None\n",
    "    avg_epoch_losses = []\n",
    "    avg_epoch_f1s = []\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(x_batch_train, training=True)\n",
    "                loss_value = loss_fn(y_batch_train, logits)\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        \n",
    "        # TODO: Tensorboard\n",
    "        \n",
    "        # Run a validation loop at the end of each epoch.\n",
    "        val_losses = []\n",
    "        val_f1s = []\n",
    "        for x_batch_val, y_batch_val in val_dataset:\n",
    "            val_logits = model(x_batch_val, training=False)\n",
    "            val_loss_value = loss_fn(y_batch_val, val_logits)\n",
    "            val_avg_f1 = calculate_page_metric(y_batch_val.numpy()[0], val_logits.numpy()[0])\n",
    "            val_losses.append(val_loss_value)\n",
    "            val_f1s.append(val_avg_f1)\n",
    "        average_val_loss = np.average(val_losses)\n",
    "        average_val_f1 = np.average(val_f1s)\n",
    "        avg_epoch_losses.append(average_val_loss)\n",
    "        avg_epoch_f1s.append(average_val_f1)\n",
    "        if average_val_loss < best:\n",
    "            best_weights = model.get_weights()\n",
    "            best = average_val_loss\n",
    "            best_loss_history = [val_losses, val_f1s]\n",
    "        if average_val_f1 > best_f1:\n",
    "            best_f1_weights = model.get_weights()\n",
    "            best_f1 = average_val_f1\n",
    "            best_f1_history = [val_losses, val_f1s]\n",
    "        print(\"Validation loss: %.4f\" % (float(average_val_loss),))\n",
    "        print(\"Validation F1: %.4f\" % (float(average_val_f1),))\n",
    "        print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "    print(f\"Best loss: {best}, Best F1: {best_f1}\")\n",
    "    print(f\"Training finish, load best weights. {best_model_method}\")\n",
    "    \n",
    "    if best_model_method == 'loss':\n",
    "        model.set_weights(best_weights)\n",
    "    elif best_model_method == 'f1-score':\n",
    "        model.set_weights(best_f1_weights)\n",
    "    avg_epoch_result = {\"epoch_losses\": avg_epoch_losses, \"epoch_f1s\": avg_epoch_f1s}\n",
    "    return model, avg_epoch_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_curve(epochs, model, optimizer, train_dataset, val_dataset):\n",
    "    import time\n",
    "    \n",
    "    epochs = epochs\n",
    "    best_f1_weights = None\n",
    "    best_f1 = 0\n",
    "    best_f1_history = None\n",
    "    best_train = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "        start_time = time.time()\n",
    "        train_f1s = []\n",
    "        # Iterate over the batches of the dataset.\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(x_batch_train, training=True)\n",
    "                loss_value = loss_fn(y_batch_train, logits)\n",
    "                train_avg_f1 = calculate_page_metric(y_batch_train.numpy()[0], logits.numpy()[0])\n",
    "                train_f1s.append(train_avg_f1)\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        average_train_f1 = np.average(train_f1s)\n",
    "        \n",
    "        # Run a validation loop at the end of each epoch.\n",
    "        val_losses = []\n",
    "        val_f1s = []\n",
    "        for x_batch_val, y_batch_val in val_dataset:\n",
    "            val_logits = model(x_batch_val, training=False)\n",
    "            val_loss_value = loss_fn(y_batch_val, val_logits)\n",
    "            val_avg_f1 = calculate_page_metric(y_batch_val.numpy()[0], val_logits.numpy()[0])\n",
    "            val_losses.append(val_loss_value)\n",
    "            val_f1s.append(val_avg_f1)\n",
    "\n",
    "        average_val_f1 = np.average(val_f1s)\n",
    "\n",
    "        if average_val_f1 > best_f1:\n",
    "            best_f1 = average_val_f1\n",
    "            best_train = average_train_f1\n",
    "            \n",
    "    print(f\"Best train f1: {best_train}, Best val f1: {best_f1}\")\n",
    "    \n",
    "    return best_train, best_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer distribution to corresponding label\n",
    "def label_distribution_to_label(predict_y):\n",
    "    if len(predict_y.shape) != 3:\n",
    "        return predict_y\n",
    "    label_y = list()\n",
    "    for page in predict_y:\n",
    "        tmp = list()\n",
    "        for lab in page:\n",
    "            lab = lab.tolist()\n",
    "            tmp.append(lab.index(max(lab)))\n",
    "        label_y.append(tmp)\n",
    "    return label_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for testing inputs\n",
    "def prepare_for_testing(test_X_raw, test_y_raw): #ft-bert -no chunks\n",
    "    test_token_features, test_tag_features = get_token_tag_features_from_chunks(test_X_raw)\n",
    "    \n",
    "    top_parent_tags = {}\n",
    "    for page in test_token_features:\n",
    "        for node in page:\n",
    "            p_tag = node['parent-tag']\n",
    "            if p_tag not in top_parent_tags:\n",
    "                top_parent_tags[p_tag] = 1\n",
    "            else:\n",
    "                top_parent_tags[p_tag] += 1\n",
    "    sorted_parent_tags = sorted(top_parent_tags.items(),key=lambda x:x[1],reverse=True)\n",
    "    \n",
    "    test_ptags_vector = get_ptags_vector(test_token_features, sorted_parent_tags[:30])\n",
    "    \n",
    "    if os.path.isfile('embedding/train/LaserEmb_full.npy'):\n",
    "        test_ft_emb = np.load('embedding/train/LaserEmb_full.npy', allow_pickle=True)\n",
    "    else:\n",
    "        test_ft_emb = pages_to_word_vector_from_keylist('Laser', token_features, ['text-full']) #token_feature_titles?\n",
    "        np.save('embedding/train/LaserEmb_full.npy', laser_full_tokens_emb)\n",
    "    \n",
    "    test_tag_info_list = test_tag_features\n",
    "    ## Tokens prepare\n",
    "    test_pages_class, test_pages_query = prepare_input_ids(test_token_features, max_len)\n",
    "    # test_pages_class, test_pages_query, test_pages_text = prepare_input_ids(test_token_features, max_len)\n",
    "    ## X_test_input\n",
    "    test_composite_input = [test_ft_emb, test_ptags_vector, test_pages_class, test_pages_query, test_tag_info_list]\n",
    "    \n",
    "    ## y_test_input\n",
    "    y_test = np.asarray(test_y_raw)\n",
    "    \n",
    "    \n",
    "    return test_composite_input, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_level_score(y_pred, y_true):\n",
    "    reports = flat_classification_report(\n",
    "        y_true, y_pred, labels=[\"PAGE\", \"NEXT\"], digits=3, output_dict=True\n",
    "    )\n",
    "\n",
    "    page_prec = reports[\"PAGE\"][\"precision\"]\n",
    "    page_rec = reports[\"PAGE\"][\"recall\"]\n",
    "    page_f1 = reports[\"PAGE\"][\"f1-score\"]\n",
    "    next_prec = reports[\"NEXT\"][\"precision\"]\n",
    "    next_rec = reports[\"NEXT\"][\"recall\"]\n",
    "    next_f1 = reports[\"NEXT\"][\"f1-score\"]\n",
    "\n",
    "    record = {\n",
    "        \"page_prec\": page_prec,\n",
    "        \"page_rec\": page_rec,\n",
    "        \"page_f1\": page_f1,\n",
    "        \"next_prec\": next_prec,\n",
    "        \"next_rec\": next_rec,\n",
    "        \"next_f1\": next_f1,\n",
    "    }\n",
    "    print(\"Finish page \")\n",
    "    return record\n",
    "\n",
    "\n",
    "def page_level_score(y_pred, y_true):\n",
    "    page_prec = 0\n",
    "    page_rec = 0\n",
    "    page_f1 = 0\n",
    "    next_prec = 0\n",
    "    next_rec = 0\n",
    "    next_f1 = 0\n",
    "    macro_f1 = 0\n",
    "    size = 0\n",
    "    for idx, (page_pred, page_true) in enumerate(zip(y_pred, y_true)):\n",
    "        # 沒算到 False positive?\n",
    "        if (\n",
    "            \"NEXT\" not in page_true\n",
    "            and \"PAGE\" not in page_true\n",
    "            and \"PREV\" not in page_true\n",
    "        ):\n",
    "            # True positive = 0\n",
    "            continue\n",
    "        else:\n",
    "            size += 1\n",
    "        reports = classification_report(\n",
    "            page_true,\n",
    "            page_pred,\n",
    "            labels=[\"PAGE\", \"NEXT\"],\n",
    "            digits=3,\n",
    "            output_dict=True,\n",
    "            zero_division=0,\n",
    "        )\n",
    "        page_prec += reports[\"PAGE\"][\"precision\"]\n",
    "        page_rec += reports[\"PAGE\"][\"recall\"]\n",
    "        page_f1 += reports[\"PAGE\"][\"f1-score\"]\n",
    "        next_prec += reports[\"NEXT\"][\"precision\"]\n",
    "        next_rec += reports[\"NEXT\"][\"recall\"]\n",
    "        next_f1 += reports[\"NEXT\"][\"f1-score\"]\n",
    "    record = {\n",
    "        \"page_prec\": page_prec / size,\n",
    "        \"page_rec\": page_rec / size,\n",
    "        \"page_f1\": page_f1 / size,\n",
    "        \"next_prec\": next_prec / size,\n",
    "        \"next_rec\": next_rec / size,\n",
    "        \"next_f1\": next_f1 / size,\n",
    "    }\n",
    "    print(\"Finish page \")\n",
    "    return record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_from_batch(model, x, y, evaluate_labels, multiTask=False):\n",
    "    print(\"Start predicting test data ...\")\n",
    "    test_page_dataset = composite_list_to_dataset(x)\n",
    "    predicted_y = []\n",
    "    for pageIdx, batch_x_test in enumerate(test_page_dataset):\n",
    "        if len(y[pageIdx]) == 0:\n",
    "            batch_predict_y = np.array([])\n",
    "        else:\n",
    "            if multiTask:\n",
    "                batch_predict_y = model(batch_x_test)[0][0].numpy()\n",
    "            else:\n",
    "                batch_predict_y = model(batch_x_test)[0].numpy()\n",
    "\n",
    "        if len(batch_predict_y.shape) != 1:\n",
    "            tmp = list()\n",
    "            for lab in batch_predict_y:\n",
    "                lab = lab.tolist()\n",
    "                tmp.append(lab.index(max(lab)))\n",
    "            batch_predict_y = tmp\n",
    "        predicted_y.append(batch_predict_y)\n",
    "    print(\"Start evaluating test data ...\")\n",
    "    predict_y = np.asarray(\n",
    "        [[idx2tag.get(lab) for lab in page] for page in predicted_y]\n",
    "    )\n",
    "\n",
    "    print(\"Node level classification report:\")\n",
    "    micro_report = flat_classification_report(\n",
    "        y, predict_y, digits=3, labels=[\"PAGE\", \"NEXT\"]\n",
    "    )\n",
    "    print(micro_report)\n",
    "    micro_report_dict = flat_classification_report(\n",
    "        y, predict_y, digits=3, labels=[\"PAGE\", \"NEXT\"], output_dict=True\n",
    "    )\n",
    "    print(\n",
    "        \"Page level macro (For caculation methology please refer to the docs)\"\n",
    "    )\n",
    "    macro_report = page_level_score(predict_y, y)\n",
    "    print(macro_report)\n",
    "    \n",
    "    # with open('predict_y.txt', 'w') as f:\n",
    "    #     f.write(str(predict_y))\n",
    "    # with open('y.txt', 'w') as f:\n",
    "    #     f.write(str(y))\n",
    "\n",
    "    return (\n",
    "        0.5 * (micro_report_dict[\"PAGE\"][\"f1-score\"] + macro_report[\"next_f1\"])\n",
    "        + 0.5 * (micro_report_dict[\"PAGE\"][\"f1-score\"] + macro_report[\"next_f1\"])\n",
    "    ) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, target=\"all\"):\n",
    "    # test_languages = storage.get_all_test_languages()\n",
    "    test_languages = [\"en\", \"de\", \"ru\", \"zh\", \"ja\", \"ko\"]\n",
    "    if target != \"all\":\n",
    "        test_languages = [target]\n",
    "\n",
    "    reports: dict = {}\n",
    "    score: int = 0\n",
    "    has_test_data = False\n",
    "\n",
    "    for language in test_languages:\n",
    "        print(\"Testing language: \", language)\n",
    "        test_urls = [\n",
    "            rec[\"Page URL\"]\n",
    "            for rec in storage.iter_test_records_by_language(language=language)\n",
    "        ]\n",
    "        test_X_raw, test_y = storage.get_test_Xy_by_language(language=language)\n",
    "        print(\n",
    "            \"pages: {}  domains: {}\".format(\n",
    "                len(test_urls), len({get_domain(url) for url in test_urls})\n",
    "            )\n",
    "        )\n",
    "        _test_x, _test_y = prepare_for_testing(test_X_raw, test_y)\n",
    "        score = evaluate_from_batch(model, _test_x, _test_y, [\"PAGE\", \"NEXT\"])\n",
    "        print(\"===================================\")\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_macro_avg(reports):\n",
    "    avg_macro = 0\n",
    "    for lan, report in reports.items():\n",
    "        avg_macro+=report['macro avg']['f1-score']\n",
    "    return avg_macro/len(reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page/Node level evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pages: 100  domains: 53\n"
     ]
    }
   ],
   "source": [
    "def get_test_data(type=None, scaled_page='normal'):\n",
    "    if type is None:\n",
    "        print(\"Please assign type of test_data\")\n",
    "        return (None, None, None)\n",
    "    test_X_one = []\n",
    "    test_X_two = []\n",
    "    test_y_one = []\n",
    "    test_y_two = []\n",
    "    test_page_positions_one = []\n",
    "    test_page_positions_two = []\n",
    "    if type != 'EVENT_SOURCE':\n",
    "        storage.test_file = 'NORMAL'\n",
    "        test_urls = [rec['Page URL'] for rec in storage.iter_test_records(exclude_en=None)]\n",
    "        test_X_one, test_y_one, test_page_positions_one = storage.get_test_Xy(validate=False, contain_position=True,scaled_page=scaled_page,exclude_en=None)\n",
    "        print(\"pages: {}  domains: {}\".format(len(test_urls), len({get_domain(url) for url in test_urls})))\n",
    "        if type == 'NORMAL':\n",
    "            return test_X_one, test_y_one, test_page_positions_one\n",
    "    if type != 'NORMAL':\n",
    "        storage.test_file = 'EVENT_SOURCE'\n",
    "        test_urls = [rec['Page URL'] for rec in storage.iter_test_records(exclude_en=None)]\n",
    "        test_X_two, test_y_two, test_page_positions_two = storage.get_test_Xy(validate=False, contain_position=True,scaled_page=scaled_page,exclude_en=None)\n",
    "        print(\"pages: {}  domains: {}\".format(len(test_urls), len({get_domain(url) for url in test_urls})))\n",
    "        if type == 'EVENT_SOURCE':\n",
    "            return test_X_two, test_y_two, test_page_positions_two\n",
    "    test_X_raw = test_X_one + test_X_two\n",
    "    test_y = test_y_one + test_y_two\n",
    "    test_positions = test_page_positions_one + test_page_positions_two\n",
    "    return test_X_raw, test_y, test_positions\n",
    "\n",
    "def evaluate_test(test_X_raw, test_y, model):\n",
    "    storage.test_file = 'EVENT_SOURCE'\n",
    "    _test_x, _test_y = prepare_for_testing(test_X_raw, test_y)\n",
    "    evaluate_from_batch(model, _test_x, _test_y, ['PAGE','NEXT'])\n",
    "\n",
    "    # count_predict(test_y_pred, test_y)\n",
    "    # page_classifier(test_y_pred, test_y)\n",
    "    return\n",
    "\n",
    "test_X_raw, test_y, test_page_positions = get_test_data('EVENT_SOURCE')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, loss_fn = get_custom_emb_model(use_crf=True, embedding_size = 32, hidden_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f48926e68b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f48926e68b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer 'Conv_char_3' (type Conv2D).\n\n{{function_node __wrapped__Conv2D_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[2379,64,1019,1] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2D]\n\nCall arguments received by layer 'Conv_char_3' (type Conv2D):\n  • inputs=tf.Tensor(shape=(1, 2379, 1024, 32, 1), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, avg_epoch_result \u001b[39m=\u001b[39m train_on_epoch(\u001b[39m2\u001b[39;49m, model, optimizer, train_dataset, val_dataset)\n",
      "Cell \u001b[0;32mIn[40], line 20\u001b[0m, in \u001b[0;36mtrain_on_epoch\u001b[0;34m(epochs, model, optimizer, train_dataset, val_dataset, best_model_method)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m step, (x_batch_train, y_batch_train) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataset):\n\u001b[1;32m     19\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[0;32m---> 20\u001b[0m         logits \u001b[39m=\u001b[39m model(x_batch_train, training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     21\u001b[0m         loss_value \u001b[39m=\u001b[39m loss_fn(y_batch_train, logits)\n\u001b[1;32m     22\u001b[0m     grads \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(loss_value, model\u001b[39m.\u001b[39mtrainable_weights)\n",
      "File \u001b[0;32m~/miniconda3/envs/prnsm/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/prnsm/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:7262\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7261\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 7262\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer 'Conv_char_3' (type Conv2D).\n\n{{function_node __wrapped__Conv2D_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[2379,64,1019,1] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2D]\n\nCall arguments received by layer 'Conv_char_3' (type Conv2D):\n  • inputs=tf.Tensor(shape=(1, 2379, 1024, 32, 1), dtype=float32)"
     ]
    }
   ],
   "source": [
    "model, avg_epoch_result = train_on_epoch(2, model, optimizer, train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = evaluate_model(model, target='event')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"saved_model/0416\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_X_raw, test_y = storage.get_test_Xy_by_language(language=\"event\")\n",
    "# _test_x, _test_y = prepare_for_testing(test_X_raw, test_y)\n",
    "# evaluate_from_batch(model, _test_x, _test_y, [\"PAGE\", \"NEXT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "time_stamp = datetime.now().strftime(\"%m%d-%H%M\")\n",
    "\n",
    "model.save(f'saved_model/{time_stamp}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
