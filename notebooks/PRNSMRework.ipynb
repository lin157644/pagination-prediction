{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRNSMRework"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Log level: Default:0 (ALL), 1 (INFO), 2 (WARNING), 3 (ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\"\n",
    "import logging\n",
    "import random\n",
    "import sys\n",
    "from urllib.parse import parse_qs, parse_qsl, urlparse, urlsplit\n",
    "from itertools import islice\n",
    "from typing import Any\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import parsel\n",
    "from sklearn_crfsuite.metrics import (flat_classification_report,\n",
    "                                      sequence_accuracy_score)\n",
    "\n",
    "# Import autopager\n",
    "sys.path.insert(0, '..')\n",
    "from autopager import AUTOPAGER_LIMITS\n",
    "from autopager.htmlutils import (get_link_href, get_link_text,\n",
    "                                 get_selector_root,\n",
    "                                 get_text_around_selector_list)\n",
    "from autopager.model import _elem_attr, _num_tokens_feature\n",
    "from autopager.parserutils import (MyHTMLParser, TagParser, compare_tag,\n",
    "                                   draw_scaled_page, get_first_tag,\n",
    "                                   position_check)\n",
    "from autopager.storage import Storage\n",
    "from autopager.utils import (get_domain, ngrams, ngrams_wb, normalize,\n",
    "                             normalize_whitespaces, replace_digits, tokenize)\n",
    "\n",
    "parser = MyHTMLParser()\n",
    "tagParser = TagParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow_addons.layers.crf import CRF\n",
    "\n",
    "layers = tf.keras.layers\n",
    "pad_sequences = tf.keras.preprocessing.sequence.pad_sequences\n",
    "Model = tf.keras.Model\n",
    "Dataset = tf.data.Dataset\n",
    "\n",
    "Dense = tf.keras.layers.Dense\n",
    "Input = tf.keras.layers.Input\n",
    "Bidirectional = tf.keras.layers.Bidirectional\n",
    "LSTM = tf.keras.layers.LSTM\n",
    "Embedding = tf.keras.layers.Embedding\n",
    "Masking = tf.keras.layers.Masking\n",
    "Concatenate = tf.keras.layers.Concatenate\n",
    "AveragePooling2D = tf.keras.layers.AveragePooling2D\n",
    "MaxPooling2D = tf.keras.layers.MaxPooling2D\n",
    "Reshape = tf.keras.layers.Reshape\n",
    "Attention = tf.keras.layers.Attention\n",
    "GlobalAveragePooling1D = tf.keras.layers.GlobalAveragePooling1D\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PAGE_SEQ = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use argparse\n",
    "USED_GPU = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow\n",
    "tf.__version__\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if len(gpus)!=0:\n",
    "    for device in gpus:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[USED_GPU], 'GPU')\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"Tensorflow: No GPUs visible\")\n",
    "    \n",
    "# Pytorch\n",
    "import torch\n",
    "\n",
    "if(torch.cuda.is_available() and torch.cuda.device_count() > USED_GPU):\n",
    "    torch.cuda.set_device(USED_GPU)\n",
    "    torch.cuda.current_device()\n",
    "    torch.cuda.device(USED_GPU)\n",
    "else:\n",
    "    print(\"PyTorch: No GPUs visible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current test file:  ['en', 'zh', 'ko', 'ja', 'de', 'ru', 'test', 'event']\n"
     ]
    }
   ],
   "source": [
    "storage = Storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pages: 164  domains: 55\n"
     ]
    }
   ],
   "source": [
    "urls = [rec['Page URL'] for rec in storage.iter_records(language='en',contain_button = True, file_type='T')]\n",
    "# X_raw: <Selector xpath='.//a|.//button' data='<a href=\"https://www.oneplus.com\"><im...'>,\n",
    "# The a and button that is not yet extracted\n",
    "X_raw: list[parsel.selector.SelectorList]\n",
    "# y: ['O',  'PAGE', 'O', 'PAGE', 'PAGE', 'PAGE', 'PAGE', 'PAGE', 'O', 'PAGE', 'NEXT', 'O']\n",
    "y: list[str]\n",
    "X_raw, y, page_positions = storage.get_Xy(language='en', contain_button = True,  contain_position=True,file_type='T', scaled_page='normal')\n",
    "print(\"pages: {}  domains: {}\".format(len(urls), len({get_domain(url) for url in urls})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token feature and Tag feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX: these functions should be copy-pasted from autopager/model.py\n",
    "\n",
    "def _as_list(generator, limit=None) -> list:\n",
    "    \"\"\"\n",
    "    >>> _as_list(ngrams_wb(\"text\", 2, 2), 0)\n",
    "    []\n",
    "    >>> _as_list(ngrams_wb(\"text\", 2, 2), 2)\n",
    "    ['te', 'ex']\n",
    "    >>> _as_list(ngrams_wb(\"text\", 2, 2))\n",
    "    ['te', 'ex', 'xt']\n",
    "    \"\"\"\n",
    "    return list(generator if limit is None else islice(generator, 0, limit))\n",
    "\n",
    "def feat_to_tokens(feat, tokenizer):\n",
    "    if type(feat) == type([]):\n",
    "        feat = ' '.join(feat)\n",
    "    tokens = tokenizer.tokenize(feat)\n",
    "    return tokens\n",
    "\n",
    "def num_token_feature_to_class(number):\n",
    "    if number == '=0':\n",
    "        return [1, 0, 0, 0]\n",
    "    elif number == '=1':\n",
    "        return [0, 1, 0, 0]\n",
    "    elif number == '=2':\n",
    "        return [0, 0, 1, 0]\n",
    "    else:\n",
    "        return [0, 0, 0, 1]\n",
    "\n",
    "def link_to_features(link: parsel.Selector):\n",
    "    # Get text contecnt of the link otherwise alt or img.\n",
    "    # Normalize multiple white space to one and to lowercase.\n",
    "    text = normalize(get_link_text(link))\n",
    "    href = get_link_href(link)\n",
    "    if href is None:\n",
    "        href = \"\"\n",
    "    p = urlsplit(href)\n",
    "    parent = link.xpath('..').extract()\n",
    "    # Retrive the line of first tag opening\n",
    "    parent = get_first_tag(parser, parent[0])\n",
    "    query_parsed = parse_qsl(p.query) #parse query string from path\n",
    "    query_param_names = [k.lower() for k, v in query_parsed]\n",
    "    query_param_names_ngrams = _as_list(ngrams_wb(\n",
    "        \" \".join([normalize(name) for name in query_param_names]), 3, 5, True\n",
    "    ))\n",
    "\n",
    "    # Classes of link itself and all its children.\n",
    "    # It is common to have e.g. span elements with fontawesome\n",
    "    # arrow icon classes inside <a> links.\n",
    "    self_and_children_classes = ' '.join(link.xpath(\".//@class\").extract())\n",
    "    parent_classes = ' '.join(link.xpath('../@class').extract())\n",
    "    css_classes = normalize(parent_classes + ' ' + self_and_children_classes)\n",
    "    \n",
    "    token_feature = {\n",
    "        'text-exact': replace_digits(text.strip()[:100].strip()),\n",
    "        # <scheme>://<netloc>/<path>?<query>#<fragment>\n",
    "        'query': query_param_names_ngrams,\n",
    "        'parent-tag': parent,\n",
    "        'class':_as_list(ngrams_wb(css_classes, 4, 5),\n",
    "                          AUTOPAGER_LIMITS.max_css_features),\n",
    "        'text': _as_list(ngrams_wb(replace_digits(text), 2, 5),\n",
    "                         AUTOPAGER_LIMITS.max_text_features),\n",
    "    }\n",
    "    tag_feature = {\n",
    "        'isdigit': 1 if text.isdigit() is True else 0,\n",
    "        'isalpha': 1 if text.isalpha() is True else 0,\n",
    "        'has-href': 0 if href == \"\" else 1,\n",
    "        'path-has-page': 1 if 'page' in p.path.lower() else 0,\n",
    "        'path-has-pageXX': 1 if re.search(r'[/-](?:p|page\\w?)/?\\d+', p.path.lower()) is not None else 0,\n",
    "        'path-has-number': 1 if any(part.isdigit() for part in p.path.split('/')) else 0,\n",
    "        'href-has-year': 1 if re.search('20\\d\\d', href) is not None else 0,\n",
    "        'class-has-disabled': 1 if 'disabled' in css_classes else 0,\n",
    "#         'num-tokens': num_token_feature_to_class(_num_tokens_feature(text)),\n",
    "    }\n",
    "    non_token_feature = []\n",
    "    for k,v in tag_feature.items():\n",
    "        if type(v) == type([]):\n",
    "            non_token_feature.extend(v)\n",
    "        else:\n",
    "            non_token_feature.append(v)\n",
    "\n",
    "    # print(token_feature)\n",
    "\n",
    "    return [token_feature, non_token_feature]\n",
    "\n",
    "\n",
    "def page_to_features(xseq):\n",
    "    feat_list = [link_to_features(a) for a in xseq]\n",
    "    around = get_text_around_selector_list(xseq, max_length=15)\n",
    "    # Append sibling's text-exact to each node's text-full.\n",
    "    for feat, (before, after) in zip(feat_list, around):\n",
    "        feat[0]['text-full'] = normalize(before) + ',' + feat[0]['text-exact'] + ',' + normalize(after)\n",
    "    \n",
    "    return feat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_tag_features_from_chunks(chunks):\n",
    "    token_features = []\n",
    "    tag_features = []\n",
    "    for idx, page in enumerate(chunks):\n",
    "        try:\n",
    "            feat_list = page_to_features(page)\n",
    "            token_features.append([node[0] for node in feat_list])\n",
    "            tag_features.append(np.array([node[1] for node in feat_list]))\n",
    "        except:\n",
    "            raise Exception(f\"Error occured on {idx}\")\n",
    "    return token_features, tag_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_features: list[list[dict[str, Any]]]\n",
    "token_features, tag_features = get_token_tag_features_from_chunks(X_raw)\n",
    "# ['text-exact', 'query', 'parent-tag', 'class', 'text', 'text-full']\n",
    "token_feature_titles: list[str] = list(token_features[0][0].keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare parent tag feature\n",
    "From top 30 parent tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_representation_with_map(tag, data_map):\n",
    "    # Vector length is the number of tags in the map(30).\n",
    "    rt_vec = [0] * len(data_map)\n",
    "    for idx, map_tag in enumerate(data_map):\n",
    "        # ('tag_name', count)\n",
    "        if tag == map_tag[0]:\n",
    "            rt_vec[idx] = 1\n",
    "            break\n",
    "    return rt_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ptags_vector(token_features, data_map_for_ptag):\n",
    "    pages_ptag = []\n",
    "    for page in token_features:\n",
    "        ptag_page = []\n",
    "        for node in page:\n",
    "            p_tag = node['parent-tag']\n",
    "            ptag_page.append(sparse_representation_with_map(p_tag, data_map_for_ptag))\n",
    "        pages_ptag.append(np.array(ptag_page))\n",
    "    return pages_ptag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_parent_tags = {}\n",
    "\n",
    "for page in token_features:\n",
    "    for node in page:\n",
    "        p_tag = node['parent-tag']\n",
    "        if p_tag not in top_parent_tags:\n",
    "            top_parent_tags[p_tag] = 1\n",
    "        else:\n",
    "            top_parent_tags[p_tag] += 1\n",
    "sorted_parent_tags = sorted(top_parent_tags.items(),key=lambda x:x[1],reverse=True)\n",
    "\n",
    "data_map_for_ptag = sorted_parent_tags[:30]\n",
    "#Get parent tag vector\n",
    "ptags_vector = get_ptags_vector(token_features, data_map_for_ptag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Class, Query tokens by tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TagTokenizer:\n",
    "    def __init__(self, tag_name_count=None):\n",
    "        rt_dict = {}\n",
    "        rt_dict[\"[PAD]\"] = 0\n",
    "        rt_dict[\"[UNK]\"] = 1\n",
    "        if tag_name_count is not None:\n",
    "            for k in tag_name_count.keys():\n",
    "                rt_dict[k] = len(tag_name_count)\n",
    "        self.map = rt_dict\n",
    "\n",
    "    def tokenize(self, word):\n",
    "        if isinstance(word, list):\n",
    "            token_list = []\n",
    "            for _word in word:\n",
    "                if _word not in self.map:\n",
    "                    token_list.append(self.map[\"[UNK]\"])\n",
    "                else:\n",
    "                    token_list.append(self.map[_word])\n",
    "            return token_list\n",
    "        else:\n",
    "            if word not in self.map:\n",
    "                return self.map[\"[UNK]\"]\n",
    "            else:\n",
    "                return self.map[word]\n",
    "\n",
    "    def get_size(self):\n",
    "        return len(self.map)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare class and query features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_thousand_class = {}\n",
    "top_thousand_query = {}\n",
    "\n",
    "for page in token_features:\n",
    "    for node in page:\n",
    "        for class_name in node['class']:\n",
    "            top_thousand_class[class_name] = top_thousand_class.get(class_name, 0) + 1\n",
    "        for query_name in node['query']:\n",
    "            top_thousand_query[query_name] = top_thousand_query.get(query_name, 0) + 1\n",
    "          \n",
    "class_tokenizer = TagTokenizer(top_thousand_class)\n",
    "query_tokenizer = TagTokenizer(top_thousand_query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get pre-trained sentence(text content) embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Pre-trained Laser model and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from LaserSentenceModel import LaserSentenceModel\n",
    "laser = LaserSentenceModel()\n",
    "laser.getSentenceVector('hello').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_vector(word_list, word_vector_method = None):\n",
    "    if word_vector_method is None:\n",
    "        print(\"Need to specified a method.\")\n",
    "        return\n",
    "    elif word_vector_method == 'FastText':\n",
    "        from FastTextModel import FastTextModel\n",
    "        ft = FastTextModel()\n",
    "        if type(word_list) == type([]):\n",
    "            if len(word_list) == 0:\n",
    "                return np.zeros(ft.getModel().get_dimension())\n",
    "            else:\n",
    "                vectors_array = []\n",
    "                for word in word_list:\n",
    "                    vector = ft.getWordVector(word)\n",
    "                    vectors_array.append(vector)\n",
    "                mean_vector = np.mean(vectors_array, axis = 0)\n",
    "                return mean_vector\n",
    "        else:\n",
    "            return ft.getWordVector(word_list)\n",
    "    elif word_vector_method == 'Laser':\n",
    "        return laser.getSentenceVector(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pages_to_word_vector_from_keylist(word_vector_method, token_features, word_to_vec_list):\n",
    "    print(f\"Transform key {word_to_vec_list} to word_vector ... useing {word_vector_method}\")\n",
    "    pages_vector = []\n",
    "    p = IntProgress(max=len(token_features))\n",
    "    p.description = '(Init)'\n",
    "    p.value = 0\n",
    "    display(p)\n",
    "    for idx, page in enumerate(token_features):\n",
    "        p.description = f\"Task: {idx+1}\"\n",
    "        p.value = idx+1\n",
    "        page_vectors = []\n",
    "        for node in page:\n",
    "            full_vector_list = []\n",
    "            for k,v in node.items():\n",
    "                if k in word_to_vec_list:\n",
    "                    # if word_to_vec_list is 'text-full'\n",
    "                    # v is str, returns laser.embed_sentences(sents, lang=self.lang)[0]\n",
    "                    full_vector_list.append(word_to_vector(v, word_vector_method))\n",
    "            full_vector = np.concatenate(full_vector_list, axis=0)\n",
    "            page_vectors.append(full_vector)\n",
    "        pages_vector.append(np.array(page_vectors))\n",
    "    p.description = '(Done)'\n",
    "    return pages_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('embedding/train/LaserEmb_full.npy'):\n",
    "    laser_full_tokens_emb = np.load('embedding/train/LaserEmb_full.npy', allow_pickle=True)\n",
    "else:\n",
    "    laser_full_tokens_emb = pages_to_word_vector_from_keylist('Laser', token_features, ['text-full'])\n",
    "    np.save('embedding/train/LaserEmb_full.npy', laser_full_tokens_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding to fixed size and prepare for training inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_ids(page_tokens, max_len):\n",
    "    pages_class = []\n",
    "    pages_query = []\n",
    "    for page in page_tokens:\n",
    "        class_page = []\n",
    "        query_page = []\n",
    "        for node in page:\n",
    "            #class\n",
    "            class_ids = class_tokenizer.tokenize(node['class'])\n",
    "            class_ids = class_ids + [0] * (max_len-len(class_ids))\n",
    "            class_page.append(class_ids[:max_len])\n",
    "            #query\n",
    "            query_ids = query_tokenizer.tokenize(node['query'])\n",
    "            query_ids = query_ids + [0] * (max_len-len(query_ids))\n",
    "            query_page.append(query_ids[:max_len])\n",
    "        pages_class.append(np.array(class_page))\n",
    "        pages_query.append(np.array(query_page))\n",
    "    return pages_class, pages_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag_info_list = tag_features #features which only have tag true/false information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attr_x = laser_full_tokens_emb\n",
    "train_ptag: list = ptags_vector\n",
    "pages_class: list[np.ndarray]\n",
    "pages_query: list[np.ndarray]\n",
    "pages_class, pages_query = prepare_input_ids(token_features, max_len)\n",
    "train_tag_x = tag_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_composite_with_token = [train_attr_x, train_ptag, pages_class, pages_query, train_tag_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"O\", \"PREV\", \"PAGE\", \"NEXT\"]\n",
    "tag2idx = {label: idx for idx, label in enumerate(labels)}\n",
    "idx2tag = {idx: label for idx, label in enumerate(labels)}\n",
    "num_tags = len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = [np.array([tag2idx.get(l) for l in lab]) for lab in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 1024)\n",
      "(303, 30)\n",
      "(303, 256)\n",
      "(303, 256)\n",
      "(303, 8)\n"
     ]
    }
   ],
   "source": [
    "for inputs in train_composite_with_token:\n",
    "    print(inputs[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For custom embedding\n",
    "\n",
    "def get_custom_emb_model(use_crf = True, embedding_size = 32, hidden_size = 300):\n",
    "    ft_shape = (None, 1024) # Laser embedding\n",
    "    tag_info_shape = (None, 8) # tag_freatures from autopager\n",
    "    tag_emb_shape = (None, 256) # Class and Query embedding Shape\n",
    "    ptag_emb_shape = (None, 30) # Parent tag embedding shape\n",
    "    embbed_output_shape = embedding_size\n",
    "    page_embbed_shape = (-1, embbed_output_shape)\n",
    "    pool_size = (256, 1)\n",
    "    HIDDEN_UNITS = hidden_size\n",
    "    NUM_CLASS = num_tags\n",
    "    \n",
    "    input_ft_embedding = Input(shape=(ft_shape), name=\"input_ft_embeddings\")\n",
    "    input_tag_information = Input(shape=(tag_info_shape), name=\"input_tag_information\")\n",
    "    input_ptag_vector = Input(shape=(ptag_emb_shape), name=\"input_ptag\")\n",
    "    input_class = Input(shape=(tag_emb_shape), name=\"input_class\")\n",
    "    input_query = Input(shape=(tag_emb_shape), name=\"input_query\")\n",
    "\n",
    "    #Embedding layers\n",
    "    ## input_class\n",
    "    class_emb = Embedding(input_dim = class_tokenizer.get_size(), output_dim = embbed_output_shape, input_length=MAX_PAGE_SEQ, mask_zero = True)(input_class)\n",
    "    class_emb = MaxPooling2D(pool_size, data_format = 'channels_first')(class_emb)\n",
    "    class_emb = Reshape(page_embbed_shape, name=\"class_emb_out\")(class_emb)\n",
    "    ## input_query\n",
    "    query_emb = Embedding(input_dim = query_tokenizer.get_size(), output_dim = embbed_output_shape, input_length=MAX_PAGE_SEQ, mask_zero = True)(input_query)\n",
    "    query_emb = MaxPooling2D(pool_size, data_format = 'channels_first')(query_emb)\n",
    "    query_emb = Reshape(page_embbed_shape, name=\"query_emb_out\")(query_emb)\n",
    "\n",
    "    input_tags = Concatenate()([class_emb, query_emb])\n",
    "    input_tags_FFN = Dense(units = 2 * embbed_output_shape, activation = 'relu')(input_tags)\n",
    "    input_tags_FFN = Dense(units = embbed_output_shape, activation = 'relu', name=\"input_tag_FFN_out\")(input_tags_FFN)\n",
    "\n",
    "\n",
    "    ft_FFN = Dense(units = 512, activation = 'relu', name=\"ft_FFN_01\")(input_ft_embedding)\n",
    "    ft_FFN = Dense(units = 256, activation = 'relu', name=\"ft_FFN_02\")(ft_FFN)\n",
    "    ft_FFN = Dense(units = 128, activation = 'relu', name=\"ft_FFN_out\")(ft_FFN)\n",
    "    \n",
    "    # FFN for ptag\n",
    "    # ptag_FFN = Dense(units = 128, activation = 'relu', name=\"ptag_FFN_01\")(input_ptag_vector)\n",
    "    # ptag_FFN = Dense(units = 64, activation = 'relu', name=\"ptag_FFN_out\")(ptag_FFN)\n",
    "    \n",
    "    merged = Concatenate()([input_tags_FFN, input_ptag_vector, input_tag_information])\n",
    "    model = Bidirectional(LSTM(units = HIDDEN_UNITS//2, return_sequences=True))(merged)\n",
    "    \n",
    "    if use_crf:\n",
    "        crf=CRF(NUM_CLASS, name='crf_layer')\n",
    "        out =crf(model)\n",
    "        loss_fn = crf.get_loss\n",
    "    else:\n",
    "        out = Dense(units = NUM_CLASS, activation='softmax')(model)\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    \n",
    "    model = Model([input_ft_embedding, input_ptag_vector, input_class, input_query, input_tag_information], out)\n",
    "        \n",
    "    return model, loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train/val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_dataSet(data, dataType):\n",
    "    dataset = Dataset.from_generator(lambda: iter(data), dataType)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def composite_splite_to_train_val(composite_x, y, number):\n",
    "    x_train = [ data[:-number] for data in composite_x]\n",
    "    y_train = y[:-number]\n",
    "    x_val = [ data[-number:] for data in composite_x]\n",
    "    y_val = y[-number:]\n",
    "    return x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val = composite_splite_to_train_val(train_composite_with_token, train_y, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_list_to_dataset(x, y, isValidation = False, batch_size = 1):\n",
    "    all_data = Dataset.zip(tuple([list_to_dataSet(data, tf.float32) for data in x]))\n",
    "    y_ds = list_to_dataSet(y, tf.int32)\n",
    "    final_set = Dataset.zip((all_data, y_ds))\n",
    "    if not isValidation:\n",
    "        final_set = final_set.shuffle(buffer_size=1024).batch(batch_size)\n",
    "    else:\n",
    "        final_set = final_set.batch(batch_size)\n",
    "    return final_set\n",
    "\n",
    "def composite_list_to_dataset(x, batch_size = 1):\n",
    "    \"\"\"\n",
    "    x: [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]\n",
    "    output: [[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]]\n",
    "    batch_size = 2\n",
    "    output: [[[1, 5, 9], [2, 6, 10]], [[3, 7, 11], [4, 8, 12]]\n",
    "    \"\"\"\n",
    "    all_data = Dataset.zip(tuple([list_to_dataSet(data, tf.float32) for data in x]))\n",
    "    return all_data.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data_list_to_dataset(x_train, y_train, isValidation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = data_list_to_dataset(x_val, y_val, isValidation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Custom Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate training/val f1-score\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def calculate_pages_metric(y_true_pages, y_predict_pages):\n",
    "    pages_f1 = []\n",
    "    nexts_f1 = []\n",
    "    avg_f1 = []\n",
    "    for y_true, y_predict in zip(y_true_pages, y_predict_pages):\n",
    "        if len(y_true) == 0:\n",
    "            break\n",
    "        report = classification_report(\n",
    "            y_true, y_predict, output_dict=True, zero_division=1\n",
    "        )\n",
    "        #         print(report)\n",
    "        PAGE = report[\"2\"][\"f1-score\"]\n",
    "        NEXT = report[\"3\"][\"f1-score\"]\n",
    "        pages_f1.append(PAGE)\n",
    "        nexts_f1.append(NEXT)\n",
    "        avg_f1.append((PAGE + NEXT) / 2)\n",
    "    return pages_f1, nexts_f1, avg_f1\n",
    "\n",
    "\n",
    "def calculate_page_metric(y_true, y_predict):\n",
    "    # 下面以對zero_division做處理\n",
    "    report = classification_report(\n",
    "        y_true, y_predict, labels=[0, 2, 3], output_dict=True, zero_division=0\n",
    "    )\n",
    "    OTHER = report[\"0\"][\"f1-score\"]\n",
    "    PAGE = report[\"2\"][\"f1-score\"]\n",
    "    NEXT = report[\"3\"][\"f1-score\"]\n",
    "    if 2 in y_true and 3 in y_true:\n",
    "        AVG = (PAGE + NEXT) / 2\n",
    "    elif 2 in y_true and 3 not in y_true:\n",
    "        AVG = PAGE\n",
    "    elif 2 not in y_true and 3 in y_true:\n",
    "        AVG = NEXT\n",
    "    else:\n",
    "        AVG = OTHER\n",
    "    return AVG\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for 1 data predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for (batch_x, batch_y) in train_dataset.take(1):\n",
    "#     batch_predict_y = model(batch_x).numpy()\n",
    "#     batch_true_y = batch_y.numpy()\n",
    "#     print(batch_true_y)\n",
    "#     print(batch_predict_y)\n",
    "#     print(calculate_page_metric(batch_true_y[0], batch_predict_y[0]))\n",
    "#     print(classification_report(batch_true_y[0], batch_predict_y[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_epoch(epochs, model, optimizer, train_dataset, val_dataset, best_model_method = 'f1-score'):\n",
    "    import time\n",
    "    \n",
    "    epochs = epochs\n",
    "    best_weights = None\n",
    "    best_f1_weights = None\n",
    "    best = np.Inf\n",
    "    best_loss_history = None\n",
    "    best_f1 = 0\n",
    "    best_f1_history = None\n",
    "    avg_epoch_losses = []\n",
    "    avg_epoch_f1s = []\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(x_batch_train, training=True)\n",
    "                loss_value = loss_fn(y_batch_train, logits)\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        \n",
    "        # TODO: Tensorboard\n",
    "        \n",
    "        # Run a validation loop at the end of each epoch.\n",
    "        val_losses = []\n",
    "        val_f1s = []\n",
    "        for x_batch_val, y_batch_val in val_dataset:\n",
    "            val_logits = model(x_batch_val, training=False)\n",
    "            val_loss_value = loss_fn(y_batch_val, val_logits)\n",
    "            val_avg_f1 = calculate_page_metric(y_batch_val.numpy()[0], val_logits.numpy()[0])\n",
    "            val_losses.append(val_loss_value)\n",
    "            val_f1s.append(val_avg_f1)\n",
    "        average_val_loss = np.average(val_losses)\n",
    "        average_val_f1 = np.average(val_f1s)\n",
    "        avg_epoch_losses.append(average_val_loss)\n",
    "        avg_epoch_f1s.append(average_val_f1)\n",
    "        if average_val_loss < best:\n",
    "            best_weights = model.get_weights()\n",
    "            best = average_val_loss\n",
    "            best_loss_history = [val_losses, val_f1s]\n",
    "        if average_val_f1 > best_f1:\n",
    "            best_f1_weights = model.get_weights()\n",
    "            best_f1 = average_val_f1\n",
    "            best_f1_history = [val_losses, val_f1s]\n",
    "        print(\"Validation loss: %.4f\" % (float(average_val_loss),))\n",
    "        print(\"Validation F1: %.4f\" % (float(average_val_f1),))\n",
    "        print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "    print(f\"Best loss: {best}, Best F1: {best_f1}\")\n",
    "    print(f\"Training finish, load best weights. {best_model_method}\")\n",
    "    \n",
    "    if best_model_method == 'loss':\n",
    "        model.set_weights(best_weights)\n",
    "    elif best_model_method == 'f1-score':\n",
    "        model.set_weights(best_f1_weights)\n",
    "    avg_epoch_result = {\"epoch_losses\": avg_epoch_losses, \"epoch_f1s\": avg_epoch_f1s}\n",
    "    return model, avg_epoch_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_curve(epochs, model, optimizer, train_dataset, val_dataset):\n",
    "    import time\n",
    "    \n",
    "    epochs = epochs\n",
    "    best_f1_weights = None\n",
    "    best_f1 = 0\n",
    "    best_f1_history = None\n",
    "    best_train = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "        start_time = time.time()\n",
    "        train_f1s = []\n",
    "        # Iterate over the batches of the dataset.\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(x_batch_train, training=True)\n",
    "                loss_value = loss_fn(y_batch_train, logits)\n",
    "                train_avg_f1 = calculate_page_metric(y_batch_train.numpy()[0], logits.numpy()[0])\n",
    "                train_f1s.append(train_avg_f1)\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        average_train_f1 = np.average(train_f1s)\n",
    "        \n",
    "        # Run a validation loop at the end of each epoch.\n",
    "        val_losses = []\n",
    "        val_f1s = []\n",
    "        for x_batch_val, y_batch_val in val_dataset:\n",
    "            val_logits = model(x_batch_val, training=False)\n",
    "            val_loss_value = loss_fn(y_batch_val, val_logits)\n",
    "            val_avg_f1 = calculate_page_metric(y_batch_val.numpy()[0], val_logits.numpy()[0])\n",
    "            val_losses.append(val_loss_value)\n",
    "            val_f1s.append(val_avg_f1)\n",
    "\n",
    "        average_val_f1 = np.average(val_f1s)\n",
    "\n",
    "        if average_val_f1 > best_f1:\n",
    "            best_f1 = average_val_f1\n",
    "            best_train = average_train_f1\n",
    "            \n",
    "    print(f\"Best train f1: {best_train}, Best val f1: {best_f1}\")\n",
    "    \n",
    "    return best_train, best_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer distribution to corresponding label\n",
    "def label_distribution_to_label(predict_y):\n",
    "    if len(predict_y.shape) != 3:\n",
    "        return predict_y\n",
    "    label_y = list()\n",
    "    for page in predict_y:\n",
    "        tmp = list()\n",
    "        for lab in page:\n",
    "            lab = lab.tolist()\n",
    "            tmp.append(lab.index(max(lab)))\n",
    "        label_y.append(tmp)\n",
    "    return label_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for testing inputs\n",
    "def prepare_for_testing(test_X_raw, test_y_raw): #ft-bert -no chunks\n",
    "    test_token_features, test_tag_features = get_token_tag_features_from_chunks(test_X_raw)\n",
    "    \n",
    "    top_parent_tags = {}\n",
    "    for page in test_token_features:\n",
    "        for node in page:\n",
    "            p_tag = node['parent-tag']\n",
    "            if p_tag not in top_parent_tags:\n",
    "                top_parent_tags[p_tag] = 1\n",
    "            else:\n",
    "                top_parent_tags[p_tag] += 1\n",
    "    sorted_parent_tags = sorted(top_parent_tags.items(),key=lambda x:x[1],reverse=True)\n",
    "    \n",
    "    test_ptags_vector = get_ptags_vector(test_token_features, sorted_parent_tags[:30])\n",
    "    \n",
    "    if os.path.isfile('embedding/train/LaserEmb_full.npy'):\n",
    "        test_ft_emb = np.load('embedding/train/LaserEmb_full.npy', allow_pickle=True)\n",
    "    else:\n",
    "        test_ft_emb = pages_to_word_vector_from_keylist('Laser', token_features, ['text-full']) #token_feature_titles?\n",
    "        np.save('embedding/train/LaserEmb_full.npy', laser_full_tokens_emb)\n",
    "    \n",
    "    test_tag_info_list = test_tag_features\n",
    "    ## Tokens prepare\n",
    "    test_pages_class, test_pages_query = prepare_input_ids(test_token_features, max_len)\n",
    "    # test_pages_class, test_pages_query, test_pages_text = prepare_input_ids(test_token_features, max_len)\n",
    "    ## X_test_input\n",
    "    test_composite_input = [test_ft_emb, test_ptags_vector, test_pages_class, test_pages_query, test_tag_info_list]\n",
    "    \n",
    "    ## y_test_input\n",
    "    y_test = np.asarray(test_y_raw)\n",
    "    \n",
    "    \n",
    "    return test_composite_input, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_level_score(y_pred, y_true):\n",
    "    reports = flat_classification_report(\n",
    "        y_true, y_pred, labels=[\"PAGE\", \"NEXT\"], digits=3, output_dict=True\n",
    "    )\n",
    "\n",
    "    page_prec = reports[\"PAGE\"][\"precision\"]\n",
    "    page_rec = reports[\"PAGE\"][\"recall\"]\n",
    "    page_f1 = reports[\"PAGE\"][\"f1-score\"]\n",
    "    next_prec = reports[\"NEXT\"][\"precision\"]\n",
    "    next_rec = reports[\"NEXT\"][\"recall\"]\n",
    "    next_f1 = reports[\"NEXT\"][\"f1-score\"]\n",
    "\n",
    "    record = {\n",
    "        \"page_prec\": page_prec,\n",
    "        \"page_rec\": page_rec,\n",
    "        \"page_f1\": page_f1,\n",
    "        \"next_prec\": next_prec,\n",
    "        \"next_rec\": next_rec,\n",
    "        \"next_f1\": next_f1,\n",
    "    }\n",
    "    print(\"Finish page \")\n",
    "    return record\n",
    "\n",
    "\n",
    "def page_level_score(y_pred, y_true):\n",
    "    page_prec = 0\n",
    "    page_rec = 0\n",
    "    page_f1 = 0\n",
    "    next_prec = 0\n",
    "    next_rec = 0\n",
    "    next_f1 = 0\n",
    "    macro_f1 = 0\n",
    "    size = 0\n",
    "    for idx, (page_pred, page_true) in enumerate(zip(y_pred, y_true)):\n",
    "        # 沒算到 False positive?\n",
    "        if (\n",
    "            \"NEXT\" not in page_true\n",
    "            and \"PAGE\" not in page_true\n",
    "            and \"PREV\" not in page_true\n",
    "        ):\n",
    "            # True positive = 0\n",
    "            continue\n",
    "        else:\n",
    "            size += 1\n",
    "        reports = classification_report(\n",
    "            page_true,\n",
    "            page_pred,\n",
    "            labels=[\"PAGE\", \"NEXT\"],\n",
    "            digits=3,\n",
    "            output_dict=True,\n",
    "            zero_division=0,\n",
    "        )\n",
    "        page_prec += reports[\"PAGE\"][\"precision\"]\n",
    "        page_rec += reports[\"PAGE\"][\"recall\"]\n",
    "        page_f1 += reports[\"PAGE\"][\"f1-score\"]\n",
    "        next_prec += reports[\"NEXT\"][\"precision\"]\n",
    "        next_rec += reports[\"NEXT\"][\"recall\"]\n",
    "        next_f1 += reports[\"NEXT\"][\"f1-score\"]\n",
    "    record = {\n",
    "        \"page_prec\": page_prec / size,\n",
    "        \"page_rec\": page_rec / size,\n",
    "        \"page_f1\": page_f1 / size,\n",
    "        \"next_prec\": next_prec / size,\n",
    "        \"next_rec\": next_rec / size,\n",
    "        \"next_f1\": next_f1 / size,\n",
    "    }\n",
    "    print(\"Finish page \")\n",
    "    return record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_from_batch(model, x, y, evaluate_labels, multiTask=False):\n",
    "    print(\"Start predicting test data ...\")\n",
    "    test_page_dataset = composite_list_to_dataset(x)\n",
    "    predicted_y = []\n",
    "    for pageIdx, batch_x_test in enumerate(test_page_dataset):\n",
    "        if len(y[pageIdx]) == 0:\n",
    "            batch_predict_y = np.array([])\n",
    "        else:\n",
    "            if multiTask:\n",
    "                batch_predict_y = model(batch_x_test)[0][0].numpy()\n",
    "            else:\n",
    "                batch_predict_y = model(batch_x_test)[0].numpy()\n",
    "\n",
    "        if len(batch_predict_y.shape) != 1:\n",
    "            tmp = list()\n",
    "            for lab in batch_predict_y:\n",
    "                lab = lab.tolist()\n",
    "                tmp.append(lab.index(max(lab)))\n",
    "            batch_predict_y = tmp\n",
    "        predicted_y.append(batch_predict_y)\n",
    "    print(\"Start evaluating test data ...\")\n",
    "    predict_y = np.asarray(\n",
    "        [[idx2tag.get(lab) for lab in page] for page in predicted_y]\n",
    "    )\n",
    "\n",
    "    print(\"Node level classification report:\")\n",
    "    micro_report = flat_classification_report(\n",
    "        y, predict_y, digits=3, labels=[\"PAGE\", \"NEXT\"]\n",
    "    )\n",
    "    print(micro_report)\n",
    "    micro_report_dict = flat_classification_report(\n",
    "        y, predict_y, digits=3, labels=[\"PAGE\", \"NEXT\"], output_dict=True\n",
    "    )\n",
    "    print(\n",
    "        \"Page level macro (For caculation methology please refer to the docs)\"\n",
    "    )\n",
    "    macro_report = page_level_score(predict_y, y)\n",
    "    print(macro_report)\n",
    "    \n",
    "    # with open('predict_y.txt', 'w') as f:\n",
    "    #     f.write(str(predict_y))\n",
    "    # with open('y.txt', 'w') as f:\n",
    "    #     f.write(str(y))\n",
    "\n",
    "    return (\n",
    "        0.5 * (micro_report_dict[\"PAGE\"][\"f1-score\"] + macro_report[\"next_f1\"])\n",
    "        + 0.5 * (micro_report_dict[\"PAGE\"][\"f1-score\"] + macro_report[\"next_f1\"])\n",
    "    ) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, target=\"all\"):\n",
    "    # test_languages = storage.get_all_test_languages()\n",
    "    test_languages = [\"en\", \"de\", \"ru\", \"zh\", \"ja\", \"ko\"]\n",
    "    if target != \"all\":\n",
    "        test_languages = [target]\n",
    "\n",
    "    reports: dict = {}\n",
    "    score: int = 0\n",
    "    has_test_data = False\n",
    "\n",
    "    for language in test_languages:\n",
    "        print(\"Testing language: \", language)\n",
    "        test_urls = [\n",
    "            rec[\"Page URL\"]\n",
    "            for rec in storage.iter_test_records_by_language(language=language)\n",
    "        ]\n",
    "        test_X_raw, test_y = storage.get_test_Xy_by_language(language=language)\n",
    "        print(\n",
    "            \"pages: {}  domains: {}\".format(\n",
    "                len(test_urls), len({get_domain(url) for url in test_urls})\n",
    "            )\n",
    "        )\n",
    "        _test_x, _test_y = prepare_for_testing(test_X_raw, test_y)\n",
    "        score = evaluate_from_batch(model, _test_x, _test_y, [\"PAGE\", \"NEXT\"])\n",
    "        print(\"===================================\")\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_macro_avg(reports):\n",
    "    avg_macro = 0\n",
    "    for lan, report in reports.items():\n",
    "        avg_macro+=report['macro avg']['f1-score']\n",
    "    return avg_macro/len(reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page/Node level evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pages: 100  domains: 53\n"
     ]
    }
   ],
   "source": [
    "def get_test_data(type=None, scaled_page='normal'):\n",
    "    if type is None:\n",
    "        print(\"Please assign type of test_data\")\n",
    "        return (None, None, None)\n",
    "    test_X_one = []\n",
    "    test_X_two = []\n",
    "    test_y_one = []\n",
    "    test_y_two = []\n",
    "    test_page_positions_one = []\n",
    "    test_page_positions_two = []\n",
    "    if type != 'EVENT_SOURCE':\n",
    "        storage.test_file = 'NORMAL'\n",
    "        test_urls = [rec['Page URL'] for rec in storage.iter_test_records(exclude_en=None)]\n",
    "        test_X_one, test_y_one, test_page_positions_one = storage.get_test_Xy(validate=False, contain_position=True,scaled_page=scaled_page,exclude_en=None)\n",
    "        print(\"pages: {}  domains: {}\".format(len(test_urls), len({get_domain(url) for url in test_urls})))\n",
    "        if type == 'NORMAL':\n",
    "            return test_X_one, test_y_one, test_page_positions_one\n",
    "    if type != 'NORMAL':\n",
    "        storage.test_file = 'EVENT_SOURCE'\n",
    "        test_urls = [rec['Page URL'] for rec in storage.iter_test_records(exclude_en=None)]\n",
    "        test_X_two, test_y_two, test_page_positions_two = storage.get_test_Xy(validate=False, contain_position=True,scaled_page=scaled_page,exclude_en=None)\n",
    "        print(\"pages: {}  domains: {}\".format(len(test_urls), len({get_domain(url) for url in test_urls})))\n",
    "        if type == 'EVENT_SOURCE':\n",
    "            return test_X_two, test_y_two, test_page_positions_two\n",
    "    test_X_raw = test_X_one + test_X_two\n",
    "    test_y = test_y_one + test_y_two\n",
    "    test_positions = test_page_positions_one + test_page_positions_two\n",
    "    return test_X_raw, test_y, test_positions\n",
    "\n",
    "def evaluate_test(test_X_raw, test_y, model):\n",
    "    storage.test_file = 'EVENT_SOURCE'\n",
    "    _test_x, _test_y = prepare_for_testing(test_X_raw, test_y)\n",
    "    evaluate_from_batch(model, _test_x, _test_y, ['PAGE','NEXT'])\n",
    "\n",
    "    # count_predict(test_y_pred, test_y)\n",
    "    # page_classifier(test_y_pred, test_y)\n",
    "    return\n",
    "\n",
    "test_X_raw, test_y, test_page_positions = get_test_data('EVENT_SOURCE')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, loss_fn = get_custom_emb_model(use_crf=True, embedding_size = 32, hidden_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_class (InputLayer)       [(None, None, 256)]  0           []                               \n",
      "                                                                                                  \n",
      " input_query (InputLayer)       [(None, None, 256)]  0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, None, 256, 3  672864      ['input_class[0][0]']            \n",
      "                                2)                                                                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 256, 3  92672       ['input_query[0][0]']            \n",
      "                                2)                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, None, 1, 32)  0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, None, 1, 32)  0          ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " class_emb_out (Reshape)        (None, None, 32)     0           ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " query_emb_out (Reshape)        (None, None, 32)     0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, None, 64)     0           ['class_emb_out[0][0]',          \n",
      "                                                                  'query_emb_out[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 64)     4160        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " input_tag_FFN_out (Dense)      (None, None, 32)     2080        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " input_ptag (InputLayer)        [(None, None, 30)]   0           []                               \n",
      "                                                                                                  \n",
      " input_tag_information (InputLa  [(None, None, 8)]   0           []                               \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, None, 70)     0           ['input_tag_FFN_out[0][0]',      \n",
      "                                                                  'input_ptag[0][0]',             \n",
      "                                                                  'input_tag_information[0][0]']  \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, None, 300)    265200      ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " input_ft_embeddings (InputLaye  [(None, None, 1024)  0          []                               \n",
      " r)                             ]                                                                 \n",
      "                                                                                                  \n",
      " crf_layer (CRF)                (None, None)         1228        ['bidirectional[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,038,204\n",
      "Trainable params: 1,038,204\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7fe563c5cf70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7fe563c5cf70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Validation loss: 36.3054\n",
      "Validation F1: 0.2999\n",
      "Time taken: 287.07s\n",
      "\n",
      "Start of epoch 1\n",
      "Validation loss: 27.4091\n",
      "Validation F1: 0.4483\n",
      "Time taken: 237.78s\n",
      "Best loss: 27.40911293029785, Best F1: 0.4482953725252556\n",
      "Training finish, load best weights. f1-score\n"
     ]
    }
   ],
   "source": [
    "model, avg_epoch_result = train_on_epoch(25, model, optimizer, train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = evaluate_model(model, target='event')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"saved_model/0416\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_X_raw, test_y = storage.get_test_Xy_by_language(language=\"event\")\n",
    "# _test_x, _test_y = prepare_for_testing(test_X_raw, test_y)\n",
    "# evaluate_from_batch(model, _test_x, _test_y, [\"PAGE\", \"NEXT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/0416/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/0416/assets\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "time_stamp = datetime.now().strftime(\"%m%d-%H%M\")\n",
    "\n",
    "model.save(f'saved_model/{time_stamp}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
